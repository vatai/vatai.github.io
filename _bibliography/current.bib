@article{2015zeeman,
  title = {Zeeman Truncation in {{NMR}}. {{I}}. {{The}} Role of Operator Commutation},
  year = {2015},
  doi = {10.1002/cmr.a.21319},
  urldate = {2023-03-01},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/zeeman_truncation_in_nmr.pdf;/home/vatai/Sync/zotero-data/storage/Q8J2PS35/cmr.a.html}
}

@misc{2020stack,
  title = {The {{Stack}}},
  year = {2020},
  month = nov,
  journal = {BigCode},
  urldate = {2022-11-04},
  abstract = {As part of the BigCode project, we are releasing and maintaining The Stack, a 3.1 TB dataset of permissively licensed source code in 30 programming languages. One of our goals in this project is to give the people who wrote this source code a choice as to whether or not it should be used to develop and evaluate LLMs, as we acknowledge that not all developers may wish to have their data used for that purpose.},
  howpublished = {https://www.bigcode-project.org/docs/about/the-stack/},
  langid = {american},
  keywords = {dataset},
  file = {/home/vatai/Sync/zotero-data/storage/7RXGDMHY/the-stack.html}
}

@misc{2022coding,
  title = {Coding {{Made AI}}---{{Now How Will AI Unmake Coding}}?},
  year = {2022},
  month = sep,
  journal = {IEEE Spectrum},
  urldate = {2022-10-17},
  abstract = {Are coders doomed? That question has been bouncing around computer programming communities ever since OpenAI's large language model, GPT-3, surprised everyone with its ability to create html websites from simple written instructions.},
  chapter = {Computing},
  howpublished = {https://spectrum.ieee.org/ai-code-generation-language-models},
  langid = {english},
  keywords = {light},
  file = {/home/vatai/Sync/zotero-data/storage/N6QEX7WB/ai-code-generation-language-models.html}
}

@misc{2024bring,
  title = {Bring {{Your Own Codegen To TVM}} --- Tvm 0.13.Dev0 Documentation},
  year = {2024},
  urldate = {2023-05-24},
  howpublished = {https://tvm.apache.org/docs/dev/how\_to/relay\_bring\_your\_own\_codegen.html\#}
}

@misc{accardi2014electron,
  title = {Electron {{Ion Collider}}: {{The Next QCD Frontier}} - {{Understanding}} the Glue That Binds Us All},
  shorttitle = {Electron {{Ion Collider}}},
  author = {Accardi, A. and Albacete, J. L. and Anselmino, M. and Armesto, N. and Aschenauer, E. C. and Bacchetta, A. and Boer, D. and Brooks, W. K. and Burton, T. and Chang, N.-B. and Deng, W.-T. and Deshpande, A. and Diehl, M. and Dumitru, A. and Dupr{\'e}, R. and Ent, R. and Fazio, S. and Gao, H. and Guzey, V. and Hakobyan, H. and Hao, Y. and Hasch, D. and Holt, R. and Horn, T. and Huang, M. and Hutton, A. and Hyde, C. and {Jalilian-Marian}, J. and Klein, S. and Kopeliovich, B. and Kovchegov, Y. and Kumar, K. and Kumeri{\v c}ki, K. and Lamont, M. A. C. and Lappi, T. and Lee, J.-H. and Lee, Y. and Levin, E. M. and Lin, F.-L. and Litvinenko, V. and Ludlam, T. W. and Marquet, C. and Meziani, Z.-E. and McKeown, R. and Metz, A. and Milner, R. and Morozov, V. S. and Mueller, A. H. and M{\"u}ller, B. and M{\"u}ller, D. and {Nadel-Turonski}, P. and Paukkunen, H. and Prokudin, A. and Ptitsyn, V. and Qian, X. and Qiu, J.-W. and {Ramsey-Musolf}, M. and Roser, T. and Sabati{\'e}, F. and Sassot, R. and Schnell, G. and Schweitzer, P. and Sichtermann, E. and Stratmann, M. and Strikman, M. and Sullivan, M. and Taneja, S. and Toll, T. and Trbojevic, D. and Ullrich, T. and Venugopalan, R. and Vigdor, S. and Vogelsang, W. and Weiss, C. and Xiao, B.-W. and Yuan, F. and Zhang, Y.-H. and Zheng, L.},
  year = {2014},
  month = nov,
  number = {arXiv:1212.1701},
  eprint = {1212.1701},
  primaryclass = {hep-ex, physics:hep-ph, physics:nucl-ex, physics:nucl-th},
  publisher = {arXiv},
  urldate = {2024-04-25},
  abstract = {This White Paper presents the science case of an Electron-Ion Collider (EIC), focused on the structure and interactions of gluon-dominated matter, with the intent to articulate it to the broader nuclear science community. It was commissioned by the managements of Brookhaven National Laboratory (BNL) and Thomas Jefferson National Accelerator Facility (JLab) with the objective of presenting a summary of scientific opportunities and goals of the EIC as a follow-up to the 2007 NSAC Long Range plan. This document is a culmination of a community-wide effort in nuclear science following a series of workshops on EIC physics and, in particular, the focused ten-week program on "Gluons and quark sea at high energies" at the Institute for Nuclear Theory in Fall 2010. It contains a brief description of a few golden physics measurements along with accelerator and detector concepts required to achieve them, and it benefited from inputs from the users' communities of BNL and JLab. This White Paper offers the promise to propel the QCD science program in the U.S., established with the CEBAF accelerator at JLab and the RHIC collider at BNL, to the next QCD frontier.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {High Energy Physics - Experiment,High Energy Physics - Phenomenology,Nuclear Experiment,Nuclear Theory},
  note = {Comment: The Second Edition, 164 pages},
  file = {/home/vatai/Sync/zotero-data/storage/V2WQ3IAI/Accardi et al. - 2014 - Electron Ion Collider The Next QCD Frontier - Und.pdf}
}

@book{aho2007compilers,
  title = {Compilers: Principles, Techniques, \& Tools},
  shorttitle = {Compilers},
  author = {Aho, Alfred V. and Lam, Monica S. and Sethi, Ravi and Ullman, Jeffrey D.},
  year = {2007},
  edition = {Second edition},
  publisher = {Pearson/Addison Wesley},
  address = {Boston},
  urldate = {2024-08-29},
  abstract = {"This new edition of the classic "Dragon" book has been completely revised to include the most recent developments to compiling. The book provides a thorough introduction to compiler design and continues to emphasize the applicability of compiler technology to a broad range of problems in software design and development. The first hall of the book is designed for use in an undergraduate compilers course while the second half can be used in a graduate course stressing code optimization."--Jacket},
  isbn = {978-0-321-48681-3},
  langid = {english},
  keywords = {Biographies,Compilateurs (Logiciels),Compiler,Compilers (Computer programs),Computer,Entwurf,Montadores e compiladores},
  annotation = {OCLC: 70775643}
}

@inproceedings{alappat2020performance,
  title = {Performance {{Modeling}} of {{Streaming Kernels}} and {{Sparse Matrix-Vector Multiplication}} on {{A64FX}}},
  booktitle = {2020 {{IEEE}}/{{ACM Performance Modeling}}, {{Benchmarking}} and {{Simulation}} of {{High Performance Computer Systems}} ({{PMBS}})},
  author = {Alappat, Christie and Laukemann, Jan and Gruber, Thomas and Hager, Georg and Wellein, Gerhard and Meyer, Nils and Wettig, Tilo},
  year = {2020},
  month = nov,
  pages = {1--7},
  doi = {10.1109/PMBS51919.2020.00006},
  abstract = {The A64FX CPU powers the current \#1 supercomputer on the Top500 list. Although it is a traditional cache-based multicore processor, its peak performance and memory bandwidth rival accelerator devices. Generating efficient code for such a new architecture requires a good understanding of its performance features. Using these features, we construct the Execution-Cache-Memory (ECM) performance model for the A64FX processor in the FX700 supercomputer and validate it using streaming loops. We also identify architectural peculiarities and derive optimization hints. Applying the ECM model to sparse matrix-vector multiplication (SpMV), we motivate why the CRS matrix storage format is inappropriate and how the SELL-C-{$\sigma$} format with suitable code optimizations can achieve bandwidth saturation for SpMV.},
  keywords = {A64FX,Bandwidth,Benchmark testing,Computational modeling,ECM model,Electronic countermeasures,Kernel,Sparse matrices,sparse matrix-vector multiplication,Throughput},
  file = {/home/vatai/Sync/zotero-data/pdfs/alappat2020performance_modeling_of_streaming_kernels_and_sparse_matrix-vector.pdf;/home/vatai/Sync/zotero-data/storage/36SUT3B5/9307836.html}
}

@article{alappat2022executioncachememory,
  title = {Execution-{{Cache-Memory}} Modeling and Performance Tuning of Sparse Matrix-Vector Multiplication and {{Lattice}} Quantum Chromodynamics on {{A64FX}}},
  author = {Alappat, Christie and Meyer, Nils and Laukemann, Jan and Gruber, Thomas and Hager, Georg and Wellein, Gerhard and Wettig, Tilo},
  year = {2022},
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {34},
  number = {20},
  pages = {e6512},
  issn = {1532-0634},
  doi = {10.1002/cpe.6512},
  urldate = {2022-09-06},
  abstract = {The A64FX CPU is arguably the most powerful Arm-based processor design to date. Although it is a traditional cache-based multicore processor, its peak performance and memory bandwidth rival accelerator devices. A good understanding of its performance features is of paramount importance for developers who wish to leverage its full potential. We present an architectural analysis of the A64FX used in the Fujitsu FX1000 supercomputer at a level of detail that allows for the construction of Execution-Cache-Memory performance models for steady-state loops. In the process we identify architectural peculiarities that point to viable generic optimization strategies. After validating the model using simple streaming loops we apply the insight gained to sparse matrix-vector multiplication (SpMV) and the domain wall (DW) kernel from quantum chromodynamics. For SpMV we show why the compressed row storage (CRS) matrix storage format is not a good practical choice on this architecture and how the SELL-C- format can achieve bandwidth saturation. For the DW kernel we provide a cache-reuse analysis and show how an appropriate choice of data layout for complex arrays can realize memory-bandwidth saturation in this case as well. A comparison with state-of-the-art high-end Intel Cascade Lake AP and Nvidia V100 systems puts the capabilities of the A64FX into perspective. We also explore the potential for power optimizations using the tuning knobs provided by the Fugaku system, achieving energy savings of about 31\% for SpMV and 18\% for DW.},
  langid = {english},
  keywords = {A64FX,ECM model,Lattice quantum chromodynamics,sparse matrix-vector multiplication},
  file = {/home/vatai/Sync/zotero-data/pdfs/alappat2022execution-cache-memory_modeling_and_performance_tuning_of_sparse_matrix-vector.pdf;/home/vatai/Sync/zotero-data/storage/GZ87JUIM/cpe.html}
}

@article{alappat2023levelbased,
  title = {Level-{{Based Blocking}} for {{Sparse Matrices}}: {{Sparse Matrix-Power-Vector Multiplication}}},
  shorttitle = {Level-{{Based Blocking}} for {{Sparse Matrices}}},
  author = {Alappat, Christie and Hager, Georg and Schenk, Olaf and Wellein, Gerhard},
  year = {2023},
  month = feb,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {34},
  number = {2},
  pages = {581--597},
  issn = {1558-2183},
  doi = {10.1109/TPDS.2022.3223512},
  abstract = {The multiplication of a sparse matrix with a dense vector (SpMV) is a key component in many numerical schemes and its performance is known to be severely limited by main memory access. Several numerical schemes require the multiplication of a sparse matrix polynomial with a dense vector which is typically implemented as a sequence of SpMVs. This results in low performance and ignores the potential to increase the arithmetic intensity by reusing the matrix data from cache. In this work we use the recursive algebraic coloring engine (RACE) to enable blocking of sparse matrix data across the polynomial computations. In the graph representing the sparse matrix we form levels using a breadth-first search. Locality relations of these levels are then used to improve spatial and temporal locality when accessing the matrix data and to implement an efficient multithreaded parallelization. Our approach is independent of the matrix structure and avoids shortcomings of existing ``blocking'' strategies in terms of hardware efficiency and parallelization overhead. We quantify the quality of our implementation using performance modelling and demonstrate speedups of up to 3{\texttimes} and 5{\texttimes} compared to an optimal SpMV-based baseline on a single multicore chip of recent Intel and AMD architectures. Various numerical schemes like \$s\$s-step Krylov solvers, polynomial preconditioners and power clustering algorithms will benefit from our development.},
  keywords = {Algorithm design and analysis,Bandwidth,computer architecture,Computer architecture,graph algorithms,Hardware,Kernel,kernel optimization,memory hierarchies,Multicore processing,Optimization,performance evaluation,sparse matrices,Sparse matrices},
  file = {/home/vatai/Sync/zotero-data/pdfs/alappat2023level-based_blocking_for_sparse_matrices.pdf;/home/vatai/Sync/zotero-data/storage/44BNKBKY/stamp.html}
}

@article{alhajri2020identifying,
  title = {Identifying Multi-Hit Carcinogenic Gene Combinations: {{Scaling}} up a Weighted Set Cover Algorithm Using Compressed Binary Matrix Representation on a {{GPU}}},
  shorttitle = {Identifying Multi-Hit Carcinogenic Gene Combinations},
  author = {Al Hajri, Qais and Dash, Sajal and Feng, Wu-chun and Garner, Harold R. and Anandakrishnan, Ramu},
  year = {2020},
  month = feb,
  journal = {Sci Rep},
  volume = {10},
  number = {1},
  pages = {2022},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-58785-y},
  urldate = {2024-10-28},
  abstract = {Methods) that it will take 253 days to calculate a set of 3-hit (h = 3) combinations for BRCA, without any additional optimization or parallelization. The goal of this work is to optimize the multi-hit algorithm to identify combinations of more than two hits in a practical time frame ({$<$}1 month). Achieving this level of speedup requires parallel execution across a large number of processors. Graphical processing units (GPUs) with thousands of processors are a natural choice for massively parallel processing42. However, GPUs have three key limitations that must be addressed to achieve significant speedup. (1) Speed of memory access is significantly slower on GPUs compared to CPUs, e.g. on the Intel Xeon E5-2630 CPU L1 and L2 cache access require 4 and 11 cycles respectively43, compared to 28 and 193 cycles for the NVIDIA V100 GPU44. Therefore, speedup from parallelization will be offset by slower memory access for algorithms that require access to a large amount of data from memory. (2) GPUs have limited amount of accessible memory, e.g. 32GB for the NVIDIA V100, compared to 1.5TB for Intel Xeon E5-263045. (3) On NVIDIA GPUs, divergent branching during execution will result in unbalanced processor load, which also limits the achievable speedup from parallelization46--50. To address these GPU limitations, we employed two general strategies. (1) We used a compressed binary representation for the Gene-Sample Mutation matrix (described in Methods), which reduced memory requirement by 16-fold and resulted in an average 10 fold speedup (see Results). (2) We restructured and optimized the algorithm for parallel execution on a NVIDIA Tesla V100 PCIe graphical processing unit by minimizing divergent branching in addition to other optimizations described in the Methods section. The compressed binary representation alone resulted in a 0.4--18 fold speedup for the 2-hit algorithm, compared to the original integer matrix, depending on cancer type. This additional speedup, and the associated increase in software complexity, was not necessary for the identification of 2-hit combinations, and insufficient by itself for the identification of 3-hit combinations on the CPU. However, the optimized GPU implementation combined with the compressed binary representation was 0.7--224 times faster than the original CPU based integer matrix implementation, for the 2-hit algorithm, depending on cancer type. The 3-hit algorithm was an estimated 29--33,690 times faster for the optimized GPU implementation compared to the original CPU implementation. For the breast cancer samples mentioned above, we were able to compute a set of 3-hit combinations in 23 minutes with the optimized GPU implementation compared to the estimated 253 days for the original CPU implementation. The set of 3-hit combinations identified using a randomly partitioned training set was able to differentiate between tumor and normal samples in separate test data with overall sensitivity of 90\% (95\% confidence interval (CI) = 88--91\%) and overall specificity of 93\% (95\% CI = 92--94\%). Despite this relatively high accuracy, the multi-hit gene combinations identified by our algorithm may not represent cancer genes (see Discussion). Further experimental validation will be required to determine if mutations within these genes may play a role in cancer genesis or progression. The remainder of this manuscript is organized as follows. In the Results section, we describe the speedup achieved by the optimized parallel implementation, the breakdown of the contribution of different optimizations, and the accuracy of the multi-hit combinations identified. In the Discussion section, we illustrate how the distribution of somatic mutations in tumor and normal samples in the gene combinations can be used to identify potential driver mutations for further investigation. Our approach and results are summarized in the Conclusions. In the Methods section, we describe the multi-hit algorithm, the compressed binary representation of the input matrix, the mapping of the algorithm to the GPU, and its optimization for parallel execution.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/Carcinogenic/al hajri2020identifying_multi-hit_carcinogenic_gene_combinations_scaling_up_a_weighted_set_cover_algorithm_usin.pdf}
}

@inproceedings{alpay2020sycl,
  title = {{{SYCL}} beyond {{OpenCL}}: {{The}} Architecture, Current State and Future Direction of {{hipSYCL}}},
  shorttitle = {{{SYCL}} beyond {{OpenCL}}},
  booktitle = {Proceedings of the {{International Workshop}} on {{OpenCL}}},
  author = {Alpay, Aksel and Heuveline, Vincent},
  year = {2020},
  month = apr,
  pages = {1--1},
  publisher = {ACM},
  address = {Munich Germany},
  doi = {10.1145/3388333.3388658},
  urldate = {2022-07-30},
  isbn = {978-1-4503-7531-3},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/alpay2020sycl_beyond_opencl.pdf}
}

@inproceedings{ancourt1991scanning,
  title = {Scanning Polyhedra with {{DO}} Loops},
  booktitle = {Proceedings of the Third {{ACM SIGPLAN}} Symposium on {{Principles}} and Practice of Parallel Programming},
  author = {Ancourt, Corinne and Irigoin, Fran{\c c}ois},
  year = {1991},
  month = apr,
  series = {{{PPOPP}} '91},
  pages = {39--50},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/109625.109631},
  urldate = {2023-04-16},
  isbn = {978-0-89791-390-4},
  file = {/home/vatai/Sync/zotero-data/pdfs/ancourt_irigoin1991scanning_polyhedra_with_do_loops2.pdf;/home/vatai/Sync/zotero-data/pdfs/ancourt_irigoin1991scanning_polyhedra_with_do_loops3.pdf}
}

@techreport{ang2021reimagining,
  title = {Reimagining {{Codesign}} for {{Advanced Scientific Computing}}: {{Unlocking Transformational Opportunities}} for {{Future Computing Systems}} for {{Science}}},
  shorttitle = {Reimagining {{Codesign}} for {{Advanced Scientific Computing}}},
  author = {Ang, James and Chien, Andrew A. and Hammond, Simon David and Hoisie, Adolfy and Karlin, Ian and Pakin, Scott and Shalf, John and Vetter, Jeffery},
  year = {2021},
  month = oct,
  institution = {USDOE Office of Science (SC) (United States)},
  doi = {10.2172/1822198},
  urldate = {2022-04-15},
  abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/ang2021reimagining_codesign_for_advanced_scientific_computing.pdf;/home/vatai/Sync/zotero-data/storage/ZRPRUNK7/1822198-reimagining-codesign-advanced-scientific-computing-unlocking-transformational-opportuni.html}
}

@inproceedings{anzt2019continuous,
  title = {Towards {{Continuous Benchmarking}}: {{An Automated Performance Evaluation Framework}} for {{High Performance Software}}},
  shorttitle = {Towards {{Continuous Benchmarking}}},
  booktitle = {Proceedings of the {{Platform}} for {{Advanced Scientific Computing Conference}}},
  author = {Anzt, Hartwig and Chen, Yen-Chen and Cojean, Terry and Dongarra, Jack and Flegar, Goran and Nayak, Pratik and {Quintana-Ort{\'i}}, Enrique S. and Tsai, Yuhsiang M. and Wang, Weichung},
  year = {2019},
  month = jun,
  series = {{{PASC}} '19},
  pages = {1--11},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3324989.3325719},
  urldate = {2022-02-06},
  abstract = {We present an automated performance evaluation framework that enables an automated workflow for testing and performance evaluation of software libraries. Integrating this component into an ecosystem enables sustainable software development, as a community effort, via a web application for interactively evaluating the performance of individual software components. The performance evaluation tool is based exclusively on web technologies, which removes the burden of downloading performance data or installing additional software. We employ this framework for the Ginkgo software ecosystem, but the framework can be used with essentially any software project, including the comparison between different software libraries. The Continuous Integration (CI) framework of Ginkgo is also extended to automatically run a benchmark suite on predetermined HPC systems, store the state of the machine and the environment along with the compiled binaries, and collect results in a publicly accessible performance data repository based on Git. The Ginkgo performance explorer (GPE) can be used to retrieve the performance data from the repository, and visualizes it in a web browser. GPE also implements an interface that allows users to write scripts, archived in a Git repository, to extract particular data, compute particular metrics, and visualize them in many different formats (as specified by the script). The combination of these approaches creates a workflow which enables performance reproducibility and software sustainability of scientific software. In this paper, we present example scripts that extract and visualize performance data for Ginkgo's SpMV kernels that allow users to identify the optimal kernel for specific problem characteristics.},
  isbn = {978-1-4503-6770-7},
  keywords = {automated performance benchmarking,continuous integration,healthy software lifecycle,interactive performance visualization},
  file = {/home/vatai/Sync/zotero-data/pdfs/anzt2019towards_continuous_benchmarking.pdf}
}

@inproceedings{argonne2022femtoscale,
  title = {Femtoscale {{Imaging}} of {{Nuclei}} Using {{Exascale Platforms}}},
  author = {Argonne, {\relax VT}},
  year = {2022},
  file = {/home/vatai/Sync/zotero-data/pdfs/argonne2022femtoscale_imaging_of_nuclei_using_exascale_platforms.pdf}
}

@misc{baghdadi2018tiramisu,
  title = {Tiramisu: {{A Polyhedral Compiler}} for {{Expressing Fast}} and {{Portable Code}}},
  shorttitle = {Tiramisu},
  author = {Baghdadi, Riyadh and Ray, Jessica and Romdhane, Malek Ben and Del Sozzo, Emanuele and Akkas, Abdurrahman and Zhang, Yunming and Suriana, Patricia and Kamil, Shoaib and Amarasinghe, Saman},
  year = {2018},
  month = dec,
  number = {arXiv:1804.10694},
  eprint = {1804.10694},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1804.10694},
  urldate = {2023-06-13},
  abstract = {This paper introduces Tiramisu, a polyhedral framework designed to generate high performance code for multiple platforms including multicores, GPUs, and distributed machines. Tiramisu introduces a scheduling language with novel extensions to explicitly manage the complexities that arise when targeting these systems. The framework is designed for the areas of image processing, stencils, linear algebra and deep learning. Tiramisu has two main features: it relies on a flexible representation based on the polyhedral model and it has a rich scheduling language allowing fine-grained control of optimizations. Tiramisu uses a four-level intermediate representation that allows full separation between the algorithms, loop transformations, data layouts, and communication. This separation simplifies targeting multiple hardware architectures with the same algorithm. We evaluate Tiramisu by writing a set of image processing, deep learning, and linear algebra benchmarks and compare them with state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu matches or outperforms existing compilers and libraries on different hardware architectures, including multicore CPUs, GPUs, and distributed machines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Mathematical Software,Computer Science - Neural and Evolutionary Computing,Computer Science - Performance,Computer Science - Programming Languages},
  note = {Comment: arXiv admin note: substantial text overlap with arXiv:1803.00419},
  file = {/home/vatai/Sync/zotero-data/pdfs/baghdadi2018tiramisu.pdf;/home/vatai/Sync/zotero-data/storage/B7SZVU3S/1804.html}
}

@misc{baghdadi2021deep,
  title = {A {{Deep Learning Based Cost Model}} for {{Automatic Code Optimization}}},
  author = {Baghdadi, Riyadh and Merouani, Massinissa and Leghettas, Mohamed-Hicham and Abdous, Kamel and Arbaoui, Taha and Benatchba, Karima and Amarasinghe, Saman},
  year = {2021},
  month = apr,
  number = {arXiv:2104.04955},
  eprint = {2104.04955},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.04955},
  urldate = {2023-06-27},
  abstract = {Enabling compilers to automatically optimize code has been a longstanding goal for the compiler community. Efficiently solving this problem requires using precise cost models. These models predict whether applying a sequence of code transformations reduces the execution time of the program. Building an analytical cost model to do so is hard in modern x86 architectures due to the complexity of the microarchitecture. In this paper, we present a novel deep learning based cost model for automatic code optimization. This model was integrated in a search method and implemented in the Tiramisu compiler to select the best code transformations. The input of the proposed model is a set of simple features representing the unoptimized code and a sequence of code transformations. The model predicts the speedup expected when the code transformations are applied. Unlike previous models, the proposed one works on full programs and does not rely on any heavy feature engineering. The proposed model has only 16\% of mean absolute percentage error in predicting speedups on full programs. The proposed model enables Tiramisu to automatically find code transformations that match or are better than state-of-the-art compilers without requiring the same level of heavy feature engineering required by those compilers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {/home/vatai/Sync/zotero-data/pdfs/baghdadi2021a_deep_learning_based_cost_model_for_automatic_code_optimization.pdf;/home/vatai/Sync/zotero-data/storage/N8U9PDYZ/2104.html}
}

@inproceedings{baghdadi2021deepa,
  title = {A {{Deep Learning Based Cost Model}} for {{Automatic Code Optimization}}},
  booktitle = {Proceedings of the {{Fourth Conference}} on {{Machine Learning}} and {{Systems}}, {{MLSys}} 2021, Virtual, {{April}} 5-9, 2021},
  author = {Baghdadi, Riyadh and Merouani, Massinissa and Leghettas, Mohamed-Hicham and Abdous, Kamel and Arbaoui, Taha and Benatchba, Karima and Amarasinghe, Saman P.},
  editor = {Smola, Alex and Dimakis, Alex and Stoica, Ion},
  year = {2021},
  publisher = {mlsys.org},
  urldate = {2024-08-25}
}

@article{balaprakash2018autotuning,
  title = {Autotuning in {{High-Performance Computing Applications}}},
  author = {Balaprakash, Prasanna and Dongarra, Jack and Gamblin, Todd and Hall, Mary and Hollingsworth, Jeffrey K. and Norris, Boyana and Vuduc, Richard},
  year = {2018},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {106},
  number = {11},
  pages = {2068--2083},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2018.2841200},
  abstract = {Autotuning refers to the automatic generation of a search space of possible implementations of a computation that are evaluated through models and/or empirical measurement to identify the most desirable implementation. Autotuning has the potential to dramatically improve the performance portability of petascale and exascale applications. To date, autotuning has been used primarily in high-performance applications through tunable libraries or previously tuned application code that is integrated directly into the application. This paper draws on the authors' extensive experience applying autotuning to high-performance applications, describing both successes and future challenges. If autotuning is to be widely used in the HPC community, researchers must address the software engineering challenges, manage configuration overheads, and continue to demonstrate significant performance gains and portability across architectures. In particular, tools that configure the application must be integrated into the application build process so that tuning can be reapplied as the application and target architectures evolve.},
  keywords = {Computer architecture,High performance computing,High-performance computing,Performance evaluation,performance tuning programming systems,Programming,Runtime,Tuning},
  file = {/home/vatai/Sync/zotero-data/pdfs/balaprakash2018autotuning_in_high-performance_computing_applications.pdf;/home/vatai/Sync/zotero-data/storage/MIKAQV8T/8423171.html}
}

@inproceedings{barvinok1999algorithmic,
  title = {An {{Algorithmic Theory}} of {{Lattice Points}} in {{Polyhedra}}},
  author = {Barvinok, A. and Pommersheim, James},
  year = {1999},
  urldate = {2024-11-10},
  abstract = {We discuss topics related to lattice points in rational polyhedra, including efficient enumeration of lattice points, ``short'' generating functions for lattice points in rational polyhedra, relations to classical and higher-dimensional Dedekind sums, complexity of the Presburger arithmetic, efficient computations with rational functions, and others. Although the main slant is algorithmic, structural results are discussed, such as relations to the general theory of valuations on polyhedra and connections with the theory of toric varieties. The paper surveys known results and presents some new results and connections.}
}

@inproceedings{bastoul2004code,
  title = {Code Generation in the Polyhedral Model Is Easier than You Think},
  booktitle = {Proceedings. 13th {{International Conference}} on {{Parallel Architecture}} and {{Compilation Techniques}}, 2004. {{PACT}} 2004.},
  author = {Bastoul, C.},
  year = {2004},
  pages = {7--16},
  publisher = {IEEE},
  address = {Antibes Juan-les-Pins, France},
  doi = {10.1109/PACT.2004.1342537},
  urldate = {2022-02-16},
  abstract = {Many advances in automatic parallelization and optimization have been achieved through the polyhedral model. It has been extensively shown that this computational model provides convenient abstractions to reason about and apply program transformations. Nevertheless, the complexity of code generation has long been a deterrent for using polyhedral representation in optimizing compilers. First, code generators have a hard time coping with generated code size and control overhead that may spoil theoretical benefits achieved by the transformations. Second, this step is usually time consuming, hampering the integration of the polyhedral framework in production compilers or feedback-directed, iterative optimization schemes. Moreover, current code generation algorithms only cover a restrictive set of possible transformation functions. This paper discusses a general transformation framework able to deal with non-unimodular, non-invertible, non-integral or even non-uniform functions. It presents several improvements to a state-of-the-art code generation algorithm. Two directions are explored: generated code size and code generator efficiency. Experimental evidence proves the ability of the improved method to handle real-life problems.},
  isbn = {978-0-7695-2229-6},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/bastoul2004code_generation_in_the_polyhedral_model_is_easier_than_you_think.pdf}
}

@inproceedings{beckingsale2019performance,
  title = {Performance Portable {{C}}++ Programming with {{RAJA}}},
  booktitle = {Proceedings of the 24th {{Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  author = {Beckingsale, David and Hornung, Richard and Scogland, Tom and Vargas, Arturo},
  year = {2019},
  month = feb,
  series = {{{PPoPP}} '19},
  pages = {455--456},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3293883.3302577},
  urldate = {2022-07-29},
  abstract = {With the rapid change of computing architectures, and variety of programming models; the ability to develop performance portable applications has become of great importance. This is particularly true in large production codes where developing and maintaining hardware specific versions is untenable. To simplify the development of performance portable code, we introduce RAJA, our C++ library that allows developers to write single-source applications that can target multiple hardware and programming model back-ends. We provide a thorough introduction to all of RAJA features, and walk through some hands-on examples that will allow attendees to understand how RAJA might benefit their own applications. Attendees should bring a laptop computer to participate in the hands-on exercises. This tutorial will introduce attendees to RAJA, a C++ library for developing performance portable applications. Attendees will learn how to write performance portable code that can execute on a range of programming models (OpenMP, CUDA, Intel TBB, and HCC) and hardware (CPU, GPU, Xeon Phi). Specifically, attendees will learn how to convert existing C++ applications to use RAJA, and how to use RAJA's programming abstractions to expose existing parallelism in their applications without complex algorithm rewrites. We will also cover specific guidelines for using RAJA in a large application, including some common "gotchas" and how to handle memory management. Finally, attendees will learn how to categorize loops to allow for simple and systematic performance tuning on any architecture.},
  isbn = {978-1-4503-6225-2},
  keywords = {parallel programming,performance portability},
  file = {/home/vatai/Sync/zotero-data/pdfs/beckingsale2019performance_portable_c++_programming_with_raja.pdf}
}

@inproceedings{ben-nun2018neural,
  title = {Neural Code Comprehension: A Learnable Representation of Code Semantics},
  shorttitle = {Neural Code Comprehension},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {{Ben-Nun}, Tal and Jakobovits, Alice Shoshana and Hoefler, Torsten},
  year = {2018},
  month = dec,
  series = {{{NIPS}}'18},
  pages = {3589--3601},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2022-08-23},
  abstract = {With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.},
  file = {/home/vatai/Sync/zotero-data/pdfs/ben-nun2018neural_code_comprehension.pdf}
}

@misc{bezanson2012julia,
  title = {Julia: {{A Fast Dynamic Language}} for {{Technical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Karpinski, Stefan and Shah, Viral B. and Edelman, Alan},
  year = {2012},
  month = sep,
  number = {arXiv:1209.5145},
  eprint = {1209.5145},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1209.5145},
  urldate = {2022-07-30},
  abstract = {Dynamic languages have become popular for scientific computing. They are generally considered highly productive, but lacking in performance. This paper presents Julia, a new dynamic language for technical computing, designed for performance from the beginning by adapting and extending modern programming language techniques. A design based on generic functions and a rich type system simultaneously enables an expressive programming model and successful type inference, leading to good performance for a wide range of programs. This makes it possible for much of the Julia library to be written in Julia itself, while also incorporating best-of-breed C and Fortran libraries.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Engineering Finance and Science,Computer Science - Programming Languages,D.3.2},
  file = {/home/vatai/Sync/zotero-data/pdfs/bezanson2012julia.pdf;/home/vatai/Sync/zotero-data/storage/9QDAU4U2/1209.html}
}

@article{bezanson2017julia,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  year = {2017},
  month = jan,
  journal = {SIAM Rev.},
  volume = {59},
  number = {1},
  pages = {65--98},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/141000671},
  urldate = {2022-07-30},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast and questions notions generally held to be ``laws of nature" by practitioners of numerical computing: {\textbackslash}beginlist {\textbackslash}item High-level dynamic programs have to be slow. {\textbackslash}item One must prototype in one language and then rewrite in another language for speed or deployment. {\textbackslash}item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. {\textbackslash}endlist We introduce the Julia programming language and its design---a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that one can achieve machine performance without sacrificing human convenience.},
  keywords = {65Y05,68N15,97P40,Julia,numerical,parallel,scientific computing},
  file = {/home/vatai/Sync/zotero-data/pdfs/bezanson2017julia.pdf}
}

@article{bohr200730,
  title = {A 30 {{Year Retrospective}} on {{Dennard}}'s {{MOSFET Scaling Paper}}},
  author = {Bohr, Mark},
  year = {2007},
  journal = {IEEE Solid-State Circuits Society Newsletter},
  volume = {12},
  number = {1},
  pages = {11--13},
  issn = {1098-4232},
  doi = {10.1109/N-SSC.2007.4785534},
  abstract = {The MOSFET scaling principles for obtaining simultaneous improvements in transistor density, switching speed, and power dissipation described by Robert H. Dennard and others in "Design of Ion-implanted MOSFETs with Very Small Physical Dimensions" (1974 ) became a roadmap for the semiconductor industry to provide systematic and predictable transistor improvements. New technology generations emerging approximately every three years during the 1970's and 1980's and appearing every other year starting in the mid-1990's, promise to continue although we face growing challenges.},
  keywords = {Industries,Integrated circuit interconnections,Logic gates,MOSFET circuits,Seminal,Silicon,Transistors,Voltage control},
  file = {/home/vatai/Sync/zotero-data/storage/LT3JGBBD/4785534.html}
}

@inproceedings{bondhugula2007affine,
  title = {Affine {{Transformations}} for {{Communication Minimal Parallelization}} and {{Locality Optimization}} of {{Arbitrarily Nested Loop Sequences}}},
  author = {Bondhugula, Uday and Baskaran, M. and Krishnamoorthy, Sriram and Ramanujam, J. and Rountev, A. and Sadayappan, P.},
  year = {2007},
  urldate = {2024-11-10},
  abstract = {The polytope model provides powerful abstractions to optimize loop nests with regular accesses for parallel execution. Affine transformations in the polytope model encompass compositions of loop permutation, skewing, reversal, relative shifting, and fusion. Though significant amount of research has dealt with affine scheduling and partitioning, the problem of finding good affine transforms for communication-minimal coarse-grained parallelization as well as locality optimization for the general case of arbitrarily-nested loop sequences has not been addressed. Also, many frameworks do not treat parallelization and locality optimization in an integrated manner, and/or optimize across a sequence of producer/consumer loops. In this paper, we develop an algorithm for communication minimal and locality optimal tiling of arbitrarily nested loop sequences. The transformed loop nests are a hierarchy of fully permutable loop nest sets such that tiling those leads to minimal communication in the processor space as well as minimal reuse distances for local execution at each node. The approach also finds maximal fusion structures across a sequence of loop nests that have a producer/consumer relationship. Programs with one-dimensional and multi-dimensional schedules are all handled with the same algorithm. Synchronization-free parallelism, permutable loops or pipelined parallelism, and inner parallel loops can be detected. Examples are provided that demonstrate the effectiveness of the approach. The algorithm has been implemented into a tool to generate transformations from C/Fortran code in a fully automatic fashion.},
  note = {[TLDR] An algorithm for communication minimal and locality optimal tiling of arbitrarily nested loop sequences and has been implemented into a tool to generate transformations from C/Fortran code in a fully automatic fashion.}
}

@article{bondhugula2008pluto,
  title = {{{PLuTo}}: {{A Practical}} and {{Fully Automatic Polyhedral Program Optimization System}}},
  author = {Bondhugula, Uday and Ramanujam, J and Sadayappan, P},
  year = {2008},
  pages = {15},
  abstract = {We present the design and implementation of a fully automatic polyhedral source-to-source transformation framework that can optimize regular programs (sequences of possibly imperfectly nested loops) for parallelism and locality simultaneously. Through this work, we show the practicality of analytical model-driven automatic transformation in the polyhedral model -- far beyond what is possible by current production compilers. Unlike previous works, our approach is an end-to-end fully automatic one driven by an integer linear optimization framework that takes an explicit view of finding good ways of tiling for parallelism and locality using affine transformations. We also address generation of tiled code for multiple statement domains of arbitrary dimensionalities under (statement-wise) affine transformations -- an issue that has not been addressed previously. Experimental results from the implemented system show very high speedups for local and parallel execution on multi-cores over state-of-the-art compiler frameworks from the research community as well as the best native compilers. The system also enables the easy use of powerful empirical/iterative optimization for general arbitrarily nested loop sequences.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/bondhugula2008pluto.pdf}
}

@article{bondhugula2008practicala,
  title = {A {{Practical Automatic Polyhedral Parallelizer}} and {{Locality Optimizer}}},
  author = {Bondhugula, Uday and Hartono, Albert and Ramanujam, J and Sadayappan, P},
  year = {2008},
  pages = {13},
  abstract = {We present the design and implementation of an automatic polyhedral source-to-source transformation framework that can optimize regular programs (sequences of possibly imperfectly nested loops) for parallelism and locality simultaneously. Through this work, we show the practicality of analytical model-driven automatic transformation in the polyhedral model.Unlike previous polyhedral frameworks, our approach is an end-to-end fully automatic one driven by an integer linear optimization framework that takes an explicit view of finding good ways of tiling for parallelism and locality using affine transformations. The framework has been implemented into a tool to automatically generate OpenMP parallel code from C program sections. Experimental results from the tool show very high performance for local and parallel execution on multi-cores, when compared with state-of-the-art compiler frameworks from the research community as well as the best native production compilers. The system also enables the easy use of powerful empirical/iterative optimization for general arbitrarily nested loop sequences.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/bondhugula2008a_practical_automatic_polyhedral_parallelizer_and_locality_optimizer.pdf}
}

@article{bondhugula2016pluto,
  title = {The {{Pluto}}+ {{Algorithm}}: {{A Practical Approach}} for {{Parallelization}} and {{Locality Optimization}} of {{Affine Loop Nests}}},
  shorttitle = {The {{Pluto}}+ {{Algorithm}}},
  author = {Bondhugula, Uday and Acharya, Aravind and Cohen, Albert},
  year = {2016},
  month = apr,
  journal = {ACM Trans. Program. Lang. Syst.},
  volume = {38},
  number = {3},
  pages = {12:1--12:32},
  issn = {0164-0925},
  doi = {10.1145/2896389},
  urldate = {2022-02-16},
  abstract = {Affine transformations have proven to be powerful for loop restructuring due to their ability to model a very wide range of transformations. A single multidimensional affine function can represent a long and complex sequence of simpler transformations. Existing affine transformation frameworks such as the Pluto algorithm, which include a cost function for modern multicore architectures for which coarse-grained parallelism and locality are crucial, consider only a subspace of transformations to avoid a combinatorial explosion in finding transformations. The ensuing practical trade-offs lead to the exclusion of certain useful transformations: in particular, transformation compositions involving loop reversals and loop skewing by negative factors. In addition, there is currently no proof that the algorithm successfully finds a tree of permutable loop bands for all affine loop nests. In this article, we propose an approach to address these two issues (1) by modeling a much larger space of practically useful affine transformations in conjunction with the existing cost function of Pluto, and (2) by extending the Pluto algorithm in a way that allows a proof for its soundness and completeness for all affine loop nests. We perform an experimental evaluation of both, the effect on compilation time, and performance of generated codes. The evaluation shows that our new framework, Pluto+, provides no degradation in performance for any benchmark from Polybench. For the Lattice Boltzmann Method (LBM) simulations with periodic boundary conditions, it provides a mean speedup of 1.33 {\texttimes} over Pluto. We also show that Pluto+ does not increase compilation time significantly. Experimental results on Polybench show that Pluto+ increases overall polyhedral source-to-source optimization time by only 15\%. In cases in which it improves execution time significantly, it increased polyhedral optimization time by only 2.04 {\texttimes} .},
  keywords = {affine transformations,Automatic parallelization,locality optimization,loop transformations,polyhedral model,tiling},
  file = {/home/vatai/Sync/zotero-data/pdfs/bondhugula2016the_pluto+_algorithm.pdf}
}

@article{brenes2019massively,
  title = {Massively Parallel Implementation and Approaches to Simulate Quantum Dynamics Using {{Krylov}} Subspace Techniques},
  author = {Brenes, Marlon and Varma, Vipin Kerala and Scardicchio, Antonello and Girotto, Ivan},
  year = {2019},
  month = feb,
  journal = {Computer Physics Communications},
  volume = {235},
  pages = {477--488},
  issn = {00104655},
  doi = {10.1016/j.cpc.2018.08.010},
  urldate = {2022-10-15},
  abstract = {We have developed an application and implemented parallel algorithms in order to provide a computational framework suitable for massively parallel supercomputers to study the unitary dynamics of quantum systems. We use renowned parallel libraries such as PETSc/SLEPc combined with highperformance computing approaches in order to overcome the large memory requirements to be able to study systems whose Hilbert space dimension comprises over 9 billion independent quantum states. Moreover, we provide descriptions of the parallel approach used for the three most important stages of the simulation: handling the Hilbert subspace basis, constructing a matrix representation for a generic Hamiltonian operator and the time evolution of the system by means of the Krylov subspace methods. We employ our setup to study the evolution of quasidisordered and clean many-body systems, focussing on the return probability and related dynamical exponents: the large system sizes accessible provide novel insights into their thermalization properties.},
  langid = {english},
  keywords = {CompSpinChem},
  note = {Memory table},
  file = {/home/vatai/Sync/zotero-data/pdfs/brenes2019massively_parallel_implementation_and_approaches_to_simulate_quantum_dynamics.pdf}
}

@article{carteredwards2014kokkos,
  title = {Kokkos: {{Enabling}} Manycore Performance Portability through Polymorphic Memory Access Patterns},
  shorttitle = {Kokkos},
  author = {Carter Edwards, H. and Trott, Christian R. and Sunderland, Daniel},
  year = {2014},
  month = dec,
  journal = {Journal of Parallel and Distributed Computing},
  series = {Domain-{{Specific Languages}} and {{High-Level Frameworks}} for {{High-Performance Computing}}},
  volume = {74},
  number = {12},
  pages = {3202--3216},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2014.07.003},
  urldate = {2022-07-30},
  abstract = {The manycore revolution can be characterized by increasing thread counts, decreasing memory per thread, and diversity of continually evolving manycore architectures. High performance computing (HPC) applications and libraries must exploit increasingly finer levels of parallelism within their codes to sustain scalability on these devices. A major obstacle to performance portability is the diverse and conflicting set of constraints on memory access patterns across devices. Contemporary portable programming models address manycore parallelism (e.g., OpenMP, OpenACC, OpenCL) but fail to address memory access patterns. The Kokkos C++~library enables applications and domain libraries to achieve performance portability on diverse manycore architectures by unifying abstractions for both fine-grain data parallelism and memory access patterns. In this paper we describe Kokkos' abstractions, summarize its application programmer interface (API), present performance results for unit-test kernels and mini-applications, and outline an incremental strategy for migrating legacy C++~codes to Kokkos. The Kokkos library is under active research and development to incorporate capabilities from new generations of manycore architectures, and to address a growing list of applications and domain libraries.},
  langid = {english},
  keywords = {GPU,Manycore,Mini-application,Multidimensional array,Parallel computing,Performance portability,Thread parallelism},
  file = {/home/vatai/Sync/zotero-data/pdfs/carter_edwards2014kokkos.pdf;/home/vatai/Sync/zotero-data/storage/3LHWRHX8/S0743731514001257.html}
}

@article{chelini2019declarative,
  title = {Declarative {{Loop Tactics}} for {{Domain-specific Optimization}}},
  author = {Chelini, Lorenzo and Zinenko, Oleksandr and Grosser, Tobias and Corporaal, Henk},
  year = {2019},
  month = dec,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {16},
  number = {4},
  pages = {1--25},
  issn = {1544-3566, 1544-3973},
  doi = {10.1145/3372266},
  urldate = {2022-02-15},
  abstract = {Increasingly complex hardware makes the design of effective compilers difficult. To reduce this problem, we introduce               Declarative Loop Tactics               , which is a novel framework of composable program transformations based on an internal tree-like program representation of a polyhedral compiler. The framework is based on a declarative C++ API built around easy-to-program matchers and builders, which provide the foundation to develop loop optimization strategies. Using our matchers and builders, we express computational patterns and core building blocks, such as loop tiling, fusion, and data-layout transformations, and compose them into algorithm-specific optimizations. Declarative Loop Tactics (Loop Tactics for short) can be applied to many domains. For two of them, stencils and linear algebra, we show how developers can express sophisticated domain-specific optimizations as a set of composable transformations or calls to optimized libraries. By allowing developers to add highly customized optimizations for a given computational pattern, we expect our approach to reduce the need for DSLs and to extend the range of optimizations that can be performed by a current general-purpose compiler.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/chelini2019declarative_loop_tactics_for_domain-specific_optimization.pdf}
}

@article{chen2021evaluating,
  title = {Evaluating {{Large Language Models Trained}} on {{Code}}},
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde, Henrique and Kaplan, Jared and Edwards, Harrison and Burda, Yura and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, F. and Cummings, D. and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and {Herbert-Voss}, Ariel and Guss, William H. and Nichol, Alex and Babuschkin, I. and Balaji, S. and Jain, Shantanu and Carr, A. and Leike, J. and Achiam, Joshua and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, M. and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, P. and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  year = {2021},
  journal = {ArXiv},
  abstract = {It is found that repeated sampling from the GPT language model is a surprisingly effective strategy for producing working solutions to difficult prompts, and the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics are discussed. We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Fur-thermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
  file = {/home/vatai/Sync/zotero-data/pdfs/chen2021evaluating_large_language_models_trained_on_code2.pdf}
}

@techreport{chen2022interpretable,
  title = {Interpretable {{Uncertainty Quantification}} in {{AI}} for {{HEP}}},
  author = {Chen, Thomas Y. and Dey, Biprateep and Ghosh, Aishik and Kagan, Michael and Nord, Brian and Ramachandra, Nesar},
  year = {2022},
  month = aug,
  eprint = {2208.03284},
  primaryclass = {hep-ex, physics:hep-ph, stat},
  pages = {FERMILAB-FN-1179-SCD, arXiv:2208.03284, 1886020},
  doi = {10.2172/1886020},
  urldate = {2023-03-29},
  abstract = {Estimating uncertainty is at the core of performing scientific measurements in HEP: a measurement is not useful without an estimate of its uncertainty. The goal of uncertainty quantification (UQ) is inextricably linked to the question, ``how do we physically and statistically interpret these uncertainties?'' The answer to this question depends not only on the computational task we aim to undertake, but also on the methods we use for that task. For artificial intelligence (AI) applications in HEP, there are several areas where interpretable methods for UQ are essential, including inference, simulation, and control/decision-making. There exist some methods for each of these areas, but they have not yet been demonstrated to be as trustworthy as more traditional approaches currently employed in physics (e.g., non-AI frequentist and Bayesian methods).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,High Energy Physics - Experiment,High Energy Physics - Phenomenology,Statistics - Machine Learning},
  note = {Comment: Submitted to the Proceedings of the US Community Study on the Future of Particle Physics (Snowmass 2021)},
  file = {/home/vatai/Sync/zotero-data/storage/ZG7G9WCK/Chen et al. - 2022 - Interpretable Uncertainty Quantification in AI for.pdf}
}

@inproceedings{cherniavskii2022acceptability,
  title = {Acceptability {{Judgements}} via {{Examining}} the {{Topology}} of {{Attention Maps}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Cherniavskii, Daniil and Tulchinskii, Eduard and Mikhailov, Vladislav and Proskurina, Irina and Kushnareva, Laida and Artemova, Ekaterina and Barannikov, Serguei and Piontkovskaya, Irina and Piontkovski, Dmitri and Burnaev, Evgeny},
  year = {2022},
  month = dec,
  pages = {88--107},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  urldate = {2023-02-24},
  abstract = {The role of the attention mechanism in encoding linguistic knowledge has received special interest in NLP. However, the ability of the attention heads to judge the grammatical acceptability of a sentence has been underexplored. This paper approaches the paradigm of acceptability judgments with topological data analysis (TDA), showing that the geometric properties of the attention graph can be efficiently exploited for two standard practices in linguistics: binary judgments and linguistic minimal pairs. Topological features enhance the BERT-based acceptability classifier scores by 8\%-24\% on CoLA in three languages (English, Italian, and Swedish). By revealing the topological discrepancy between attention maps of minimal pairs, we achieve the human-level performance on the BLiMP benchmark, outperforming nine statistical and Transformer LM baselines. At the same time, TDA provides the foundation for analyzing the linguistic functions of attention heads and interpreting the correspondence between the graph features and grammatical phenomena. We publicly release the code and other materials used in the experiments.},
  file = {/home/vatai/Sync/zotero-data/pdfs/cherniavskii2022acceptability_judgements_via_examining_the_topology_of_attention_maps.pdf}
}

@misc{chetlur2014cudnn,
  title = {{{cuDNN}}: {{Efficient Primitives}} for {{Deep Learning}}},
  shorttitle = {{{cuDNN}}},
  author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
  year = {2014},
  month = dec,
  number = {arXiv:1410.0759},
  eprint = {1410.0759},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1410.0759},
  urldate = {2022-07-28},
  abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36\% on a standard model while also reducing memory consumption.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Computer Science - Neural and Evolutionary Computing},
  file = {/home/vatai/Sync/zotero-data/pdfs/chetlur2014cudnn.pdf;/home/vatai/Sync/zotero-data/storage/PWFSJDHR/1410.html}
}

@article{chronopoulos1989sstep,
  title = {S-Step Iterative Methods for Symmetric Linear Systems},
  author = {Chronopoulos, A. T. and Gear, C. W.},
  year = {1989},
  month = feb,
  journal = {Journal of Computational and Applied Mathematics},
  volume = {25},
  number = {2},
  pages = {153--168},
  issn = {0377-0427},
  doi = {10.1016/0377-0427(89)90045-9},
  urldate = {2022-06-04},
  abstract = {In this paper we introduce s-step Conjugate Gradient Method for Symmetric and Positive Definite (SPD) linear systems of equations and discuss its convergence. In the s-step Conjugate Gradient Method iteration s new directions are formed simultaneously from ≎ri, Ari,{\dots},As-1ri≎ and the preceding s directions. All s directions are chosen to be A-orthogonal to the preceding s directions. The approximation to the solution is then advanced by minimizing an error functional simultaneously in all s directions. This intuitively means that the progress towards the solution in one iteration of the s-step method equals the progress made over s consecutive steps of the one-step method. This is proven to be true.},
  langid = {english},
  keywords = {-step,conjugate gradient,convergence,Iterative methods},
  file = {/home/vatai/Sync/zotero-data/pdfs/chronopoulos1989s-step_iterative_methods_for_symmetric_linear_systems.pdf;/home/vatai/Sync/zotero-data/storage/D5FGT3MR/0377042789900459.html}
}

@inproceedings{cummins2017endtoend,
  title = {End-to-{{End Deep Learning}} of {{Optimization Heuristics}}},
  booktitle = {2017 26th {{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}} ({{PACT}})},
  author = {Cummins, Chris and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},
  year = {2017},
  month = sep,
  pages = {219--232},
  publisher = {IEEE},
  address = {Portland, OR},
  doi = {10.1109/PACT.2017.24},
  urldate = {2022-08-23},
  abstract = {Accurate automatic optimization heuristics are necessary for dealing with the complexity and diversity of modern hardware and software. Machine learning is a proven technique for learning such heuristics, but its success is bound by the quality of the features used. These features must be hand crafted by developers through a combination of expert domain knowledge and trial and error. This makes the quality of the final model directly dependent on the skill and available time of the system architect.},
  isbn = {978-1-5090-6764-0},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/cummins2017end-to-end_deep_learning_of_optimization_heuristics.pdf}
}

@article{cummins2020programl,
  title = {{{ProGraML}}: {{Graph-based Deep Learning}} for {{Program Optimization}} and {{Analysis}}},
  shorttitle = {{{ProGraML}}},
  author = {Cummins, Chris and Fisches, Zacharias V. and {Ben-Nun}, Tal and Hoefler, Torsten and Leather, Hugh},
  year = {2020},
  month = mar,
  doi = {10.48550/arXiv.2003.10536},
  urldate = {2022-03-11},
  abstract = {The increasing complexity of computing systems places a tremendous burden on optimizing compilers, requiring ever more accurate and aggressive optimizations. Machine learning offers significant benefits for constructing optimization heuristics but there remains a gap between what state-of-the-art methods achieve and the performance of an optimal heuristic. Closing this gap requires improvements in two key areas: a representation that accurately captures the semantics of programs, and a model architecture with sufficient expressiveness to reason about this representation. We introduce ProGraML - Program Graphs for Machine Learning - a novel graph-based program representation using a low level, language agnostic, and portable format; and machine learning models capable of performing complex downstream tasks over these graphs. The ProGraML representation is a directed attributed multigraph that captures control, data, and call relations, and summarizes instruction and operand types and ordering. Message Passing Neural Networks propagate information through this structured representation, enabling whole-program or per-vertex classification tasks. ProGraML provides a general-purpose program representation that equips learnable models to perform the types of program analysis that are fundamental to optimization. To this end, we evaluate the performance of our approach first on a suite of traditional compiler analysis tasks: control flow reachability, dominator trees, data dependencies, variable liveness, and common subexpression detection. On a benchmark dataset of 250k LLVM-IR files covering six source programming languages, ProGraML achieves an average 94.0 F1 score, significantly outperforming the state-of-the-art approaches. We then apply our approach to two high-level tasks - heterogeneous device mapping and program classification - setting new state-of-the-art performance in both.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/cummins2020programl.pdf;/home/vatai/Sync/zotero-data/storage/YXB4J7FP/2003.html}
}

@inproceedings{cummins2022compilergym,
  title = {{{CompilerGym}}: Robust, Performant Compiler Optimization Environments for {{AI}} Research},
  shorttitle = {{{CompilerGym}}},
  booktitle = {Proceedings of the 20th {{IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}}},
  author = {Cummins, Chris and Wasti, Bram and Guo, Jiadong and Cui, Brandon and Ansel, Jason and Gomez, Sahir and Jain, Somya and Liu, Jia and Teytaud, Olivier and Steiner, Benoit and Tian, Yuandong and Leather, Hugh},
  year = {2022},
  month = may,
  series = {{{CGO}} '22},
  pages = {92--105},
  publisher = {IEEE Press},
  address = {Virtual Event, Republic of Korea},
  doi = {10.1109/CGO53902.2022.9741258},
  urldate = {2024-08-29},
  abstract = {Interest in applying Artificial Intelligence (AI) techniques to compiler optimizations is increasing rapidly, but compiler research has a high entry barrier. Unlike in other domains, compiler and AI researchers do not have access to the datasets and frameworks that enable fast iteration and development of ideas, and getting started requires a significant engineering investment. What is needed is an easy, reusable experimental infrastructure for real world compiler optimization tasks that can serve as a common benchmark for comparing techniques, and as a platform to accelerate progress in the field.We introduce CompilerGym, a set of environments for real world compiler optimization tasks, and a toolkit for exposing new optimization tasks to compiler researchers. CompilerGym enables anyone to experiment on production compiler optimization problems through an easy-to-use package, regardless of their experience with compilers. We build upon the popular OpenAI Gym interface enabling researchers to interact with compilers using Python and a familiar API.We describe the CompilerGym architecture and implementation, characterize the optimization spaces and computational efficiencies of three included compiler environments, and provide extensive empirical evaluations. Compared to prior works, CompilerGym offers larger datasets and optimization spaces, is 27X more computationally efficient, is fault-tolerant, and capable of detecting reproducibility bugs in the underlying compilers.In making it easy for anyone to experiment with compilers - irrespective of their background - we aim to accelerate progress in the AI and compiler research domains.},
  isbn = {978-1-66540-584-3},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/Codegen/cummins2022compilergym_robust,_performant_compiler_optimization_environments_for_ai_research.pdf}
}

@misc{cummins2024meta,
  title = {Meta {{Large Language Model Compiler}}: {{Foundation Models}} of {{Compiler Optimization}}},
  shorttitle = {Meta {{Large Language Model Compiler}}},
  author = {Cummins, Chris and Seeker, Volker and Grubisic, Dejan and Roziere, Baptiste and Gehring, Jonas and Synnaeve, Gabriel and Leather, Hugh},
  year = {2024},
  month = jun,
  number = {arXiv:2407.02524},
  eprint = {2407.02524},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.02524},
  urldate = {2024-08-25},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86\_64 and ARM assembly back into LLVM-IR. These achieve 77\% of the optimising potential of an autotuning search, and 45\% disassembly round trip (14\% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Programming Languages},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/Codegen/cummins2024meta_large_language_model_compiler_foundation_models_of_compiler_optimization.pdf;/home/vatai/Sync/zotero-data/storage/BZH9TK53/2407.html}
}

@inproceedings{dash2021scaling,
  title = {Scaling {{Out}} a {{Combinatorial Algorithm}} for {{Discovering Carcinogenic Gene Combinations}} to {{Thousands}} of {{GPUs}}},
  booktitle = {2021 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Dash, Sajal and {Al-Hajri}, Qais and Feng, Wu-chun and Garner, Harold R and Anandakrishnan, Ramu},
  year = {2021},
  month = may,
  pages = {837--846},
  issn = {1530-2075},
  doi = {10.1109/IPDPS49936.2021.00093},
  urldate = {2024-11-11},
  abstract = {Cancer is a leading cause of death in the US, second only to heart disease. It is primarily a result of a combination of an estimated two-nine genetic mutations (multi-hit combinations). Although a body of research has identified hundreds of cancer-causing genetic mutations, we don't know the specific combination of mutations responsible for specific instances of cancer for most cancer types. An approximate algorithm for solving the weighted set cover problem was previously adapted to identify combinations of genes with mutations that may be responsible for individual instances of cancer. However, the algorithm's computational requirement scales exponentially with the number of genes, making it impractical for identifying more than three-hit combinations, even after the algorithm was parallelized and scaled up to a V100 GPU. Since most cancers have been estimated to require more than three hits, we scaled out the algorithm to identify combinations of four or more hits using 1000 nodes (6000 V100 GPUs with {$\approx$} 48{\texttimes}106 processing cores) on the Summit supercomputer at Oak Ridge National Laboratory. Efficiently scaling out the algorithm required a series of algorithmic innovations and optimizations for balancing an exponentially divergent workload across processors and for minimizing memory latency and inter-node communication. We achieved an average strong scaling efficiency of 90.14\% (80.96\%-97.96\% for 200 to 1000 nodes), compared to a 100 node run, with 84.18\% scaling efficiency for 1000 nodes. With experimental validation, the multi-hit combinations identified here could provide further insight into the etiology of different cancer subtypes and provide a rational basis for targeted combination therapy.},
  keywords = {Approximation algorithms,Cancer genomics,Genetic mutations,GPU,Graphics processing units,Heart,Medical treatment,Memory management,Parallel computing,Set Cover algorithm,Technological innovation},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/Carcinogenic/dash2021scaling_out_a_combinatorial_algorithm_for_discovering_carcinogenic_gene_combinations_to_thousands_of.pdf;/home/vatai/Sync/zotero-data/storage/YVVIUWDI/9460537.html}
}

@article{dehaerne2022code,
  title = {Code {{Generation Using Machine Learning}}: {{A Systematic Review}}},
  shorttitle = {Code {{Generation Using Machine Learning}}},
  author = {Dehaerne, Enrique and Dey, Bappaditya and Halder, Sandip and De Gendt, Stefan and Meert, Wannes},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {82434--82455},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3196347},
  urldate = {2024-08-24},
  abstract = {Recently, machine learning (ML) methods have been used to create powerful language models for a broad range of natural language processing tasks. An important subset of this field is that of generating code of programming languages for automatic software development. This review provides a broad and detailed overview of studies for code generation using ML. We selected 37 publications indexed in arXiv and IEEE Xplore databases that train ML models on programming language data to generate code. The three paradigms of code generation we identified in these studies are description-to-code, code-to-description, and code-to-code. The most popular applications that work in these paradigms were found to be code generation from natural language descriptions, documentation generation, and automatic program repair, respectively. The most frequently used ML models in these studies include recurrent neural networks, transformers, and convolutional neural networks. Other neural network architectures, as well as non-neural techniques, were also observed. In this review, we have summarized the applications, models, datasets, results, limitations, and future work of 37 publications. Additionally, we include discussions on topics general to the literature reviewed. This includes comparing different model types, comparing tokenizers, the volume and quality of data used, and methods for evaluating synthesized code. Furthermore, we provide three suggestions for future work for code generation using ML.},
  keywords = {Automatic programming,Codes,computer languages,data collection,Databases,Documentation,machine learning,Machine learning,natural language processing,neural networks,recurrent neural networks,Recurrent neural networks,Software,software debugging,software maintenance,Task analysis,text mining},
  file = {/home/vatai/Sync/zotero-data/storage/TL24MJCG/Dehaerne et al. - 2022 - Code Generation Using Machine Learning A Systematic Review.pdf;/home/vatai/Sync/zotero-data/storage/ZMSDIKKD/9849664.html}
}

@inproceedings{demmel2008avoiding,
  title = {Avoiding Communication in Sparse Matrix Computations},
  booktitle = {2008 {{IEEE International Symposium}} on {{Parallel}} and {{Distributed Processing}}},
  author = {Demmel, James and Hoemmen, Mark and Mohiyuddin, Marghoob and Yelick, Katherine},
  year = {2008},
  month = apr,
  pages = {1--12},
  issn = {1530-2075},
  doi = {10.1109/IPDPS.2008.4536305},
  abstract = {The performance of sparse iterative solvers is typically limited by sparse matrix-vector multiplication, which is itself limited by memory system and network performance. As the gap between computation and communication speed continues to widen, these traditional sparse methods will suffer. In this paper we focus on an alternative building block for sparse iterative solvers, the "matrix powers kernel" [x, Ax, A2x, ..., Akx], and show that by organizing computations around this kernel, we can achieve near-minimal communication costs. We consider communication very broadly as both network communication in parallel code and memory hierarchy access in sequential code. In particular, we introduce a parallel algorithm for which the number of messages (total latency cost) is independent of the power k, and a sequential algorithm, that reduces both the number and volume of accesses, so that it is independent of k in both latency and bandwidth costs. This is part of a larger project to develop "communication-avoiding Krylov subspace methods," which also addresses the numerical issues associated with these methods. Our algorithms work for general sparse matrices that "partition well". We introduce parallel performance models of matrices arising from 2D and 3D problems and show predicted speedups over a conventional algorithm of up to 7times on a petaflop-scale machine and up to 22times on computation across the grid. Analogous sequential performance models of the same problems predict speedups over a conventional algorithm of up to 10times on an out-of-core implementation, and up to 2.5times when we use our ideas to reduce off-chip latency and bandwidth to DRAM. Finally, we validate the model on an out-of-core sequential implementation and measured a speedup of over 3times, which is close to the predicted speedup.},
  keywords = {Bandwidth,Concurrent computing,Costs,Delay,Kernel,Organizing,Parallel algorithms,Partitioning algorithms,Predictive models,Sparse matrices},
  file = {/home/vatai/Sync/zotero-data/pdfs/demmel2008avoiding_communication_in_sparse_matrix_computations.pdf;/home/vatai/Sync/zotero-data/storage/MVB8KNQK/4536305.html}
}

@article{dennard1974design,
  title = {Design of Ion-Implanted {{MOSFET}}'s with Very Small Physical Dimensions},
  author = {Dennard, R.H. and Gaensslen, F.H. and Yu, Hwa-Nien and Rideout, V.L. and Bassous, E. and LeBlanc, A.R.},
  year = {1974},
  month = oct,
  journal = {IEEE Journal of Solid-State Circuits},
  volume = {9},
  number = {5},
  pages = {256--268},
  issn = {1558-173X},
  doi = {10.1109/JSSC.1974.1050511},
  abstract = {This paper considers the design, fabrication, and characterization of very small Mosfet switching devices suitable for digital integrated circuits, using dimensions of the order of 1 /spl mu/. Scaling relationships are presented which show how a conventional MOSFET can be reduced in size. An improved small device structure is presented that uses ion implantation, to provide shallow source and drain regions and a nonuniform substrate doping profile. One-dimensional models are used to predict the substrate doping profile and the corresponding threshold voltage versus source voltage characteristic. A two-dimensional current transport model is used to predict the relative degree of short-channel effects for different device parameter combinations. Polysilicon-gate MOSFET's with channel lengths as short as 0.5 /spl mu/ were fabricated, and the device characteristics measured and compared with predicted values. The performance improvement expected from using these very small devices in highly miniaturized integrated circuits is projected.},
  keywords = {Digital integrated circuits,Doping profiles,Fabrication,Ion implantation,Length measurement,MOSFET circuits,Predictive models,Semiconductor process modeling,Seminal,Switching circuits,Threshold voltage},
  file = {/home/vatai/Sync/zotero-data/pdfs/dennard1974design_of_ion-implanted_mosfet's_with_very_small_physical_dimensions.pdf;/home/vatai/Sync/zotero-data/storage/AUN3YV2B/1050511.html}
}

@inproceedings{devlin2019bert,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = jun,
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  urldate = {2022-09-29},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  file = {/home/vatai/Sync/zotero-data/pdfs/devlin2019bert.pdf}
}

@article{devlin2023diffusion,
  title = {Diffusion Model Approach to Simulating Electron-Proton Scattering Events},
  author = {Devlin, Peter and Qiu, Jian-Wei and Ringer, Felix and Sato, Nobuo},
  year = {2023},
  month = oct,
  keywords = {diffusion: model,EIC,electron nucleon: colliding beams,electron p: scattering,machine learning,new physics,Newport News CEBAF Linac,nuclear physics,nucleon,sum rule: momentum},
  file = {/home/vatai/Sync/zotero-data/pdfs/devlin2023diffusion_model_approach_to_simulating_electron-proton_scattering_events.pdf}
}

@article{dorosario2022fast,
  title = {Fast Selection of Compiler Optimizations Using Performance Prediction with Graph Neural Networks},
  author = {{do Rosario}, Vanderson Martins and {da Silva}, Anderson Faustino and Zanella, Andr{\'e} Felipe and Napoli, Ot{\'a}vio O. and Borin, Edson},
  year = {2022},
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {n/a},
  number = {n/a},
  pages = {e6869},
  issn = {1532-0634},
  doi = {10.1002/cpe.6869},
  urldate = {2022-03-23},
  abstract = {Tuning application performance on modern computing infrastructures involves choices in a vast design space as modern computing architectures can have several complex structures impacting performance. Moreover, different applications use these structures in different ways, leading to a challenging performance function. Consequently, it is hard for compilers or experts to find optimal compilation parameters for an application that maximizes such performance function. One approach to tackle this problem is to evaluate many possible optimization plans and select the best among them. However, executing an application to measure its performance for every plan can be very expensive. To tackle this problem, previous work has investigated the use of Machine Learning techniques to predict the performance of the applications without executing them quickly. In this work, we evaluate the use of graph neural networks (GNN) to make fast predictions without executing the application to guide the selection of good optimization sequences. We propose a GNN architecture to make such predictions. We train and test it using 30 thousand different compilation plans applied to 300 different applications, using ARM64 and LLVM IR code representations as input. Our results indicate that the control and data flow graph can then learn features from the control and data flow graph to outperform nongraph-aware Machine Learning models. Our GNN architecture achieved 91\% accuracy in our dataset compared to 79\% when using a nongraph-aware architecture--taking only 16ms to predict a given input. If the application been optimized took an average of 10 s to execute, and we evaluated 1000 optimization sequences, it would take almost 9 h to assess all pairs, but only 16 s with our GNN .},
  langid = {english},
  keywords = {compiler auto-tuning,graph neural networks,optimizer compilers},
  file = {/home/vatai/Sync/zotero-data/pdfs/do_rosariofast_selection_of_compiler_optimizations_using_performance_prediction_with.pdf;/home/vatai/Sync/zotero-data/storage/BMWPMS3C/cpe.html}
}

@article{edwards2012parallel,
  title = {Parallel Density Matrix Propagation in Spin Dynamics Simulations},
  author = {Edwards, Luke J. and Kuprov, Ilya},
  year = {2012},
  month = jan,
  journal = {The Journal of Chemical Physics},
  volume = {136},
  number = {4},
  pages = {044108},
  publisher = {American Institute of PhysicsAIP},
  issn = {0021-9606},
  doi = {10.1063/1.3679656},
  urldate = {2022-10-15},
  abstract = {Several methods for density matrix propagation in parallel computing environments are proposed and evaluated. It is demonstrated that the large communication overhead associated with each propagation step (two-sided multiplication of the density matrix by an exponential propagator and its conjugate) may be avoided and the simulation recast in a form that requires virtually no inter-thread communication. Good scaling is demonstrated on a 128-core (16 nodes, 8 cores each) cluster.},
  copyright = {{\copyright} 2012 American Institute of Physics.},
  langid = {english},
  keywords = {CompSpinChem},
  file = {/home/vatai/Sync/zotero-data/pdfs/edwards_kuprov2012parallel_density_matrix_propagation_in_spin_dynamics_simulations2.pdf;/home/vatai/Sync/zotero-data/storage/FZ4V4ZZ7/1.html}
}

@inproceedings{elibol2020variance,
  title = {Variance {{Reduction With Sparse Gradients}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Elibol, Melih and Lei, Lihua and Jordan, Michael I.},
  year = {2020},
  month = mar,
  urldate = {2023-02-07},
  abstract = {Variance reduction methods such as SVRG and SpiderBoost use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD, these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/elibol2020variance_reduction_with_sparse_gradients.pdf}
}

@article{ernst2022analytical,
  title = {Analytical {{Performance Estimation}} during {{Code Generation}} on {{Modern GPUs}}},
  author = {Ernst, Dominik and Holzer, Markus and Hager, Georg and Knorr, Matthias and Wellein, Gerhard},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.14242 [cs]},
  eprint = {2204.14242},
  primaryclass = {cs},
  urldate = {2022-05-03},
  abstract = {Automatic code generation is frequently used to create implementations of algorithms specifically tuned to particular hardware and application parameters. The code generation process involves the selection of adequate code transformations, tuning parameters, and parallelization strategies. We propose an alternative to time-intensive autotuning, scenario-specific performance models, or black-box machine learning to select the best-performing configuration.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing},
  note = {Comment: arXiv admin note: substantial text overlap with arXiv:2107.01143},
  file = {/home/vatai/Sync/zotero-data/pdfs/ernst2022analytical_performance_estimation_during_code_generation_on_modern_gpus.pdf}
}

@article{essadki2023code,
  title = {Code {{Generation}} for {{In-Place Stencils}}},
  author = {Essadki, Mohamed and Michel, Bertrand and Maugars, Bruno and Zinenko, Oleksandr and Vasilache, Nicolas and Cohen, Albert},
  year = {2023},
  abstract = {Numerical simulation often resorts to iterative in-place stencils such as the Gauss-Seidel or Successive Overrelaxation (SOR) methods. Writing high performance implementations of such stencils requires significant effort and time; it also involves non-local transformations beyond the stencil kernel itself. While automated code generation is a mature technology for image processing stencils, convolutions and out-ofplace iterative stencils (such as the Jacobi method), the optimization of in-place stencils requires manual craftsmanship. Building on recent advances in tensor compiler construction, we propose the first domain-specific code generator for iterative in-place stencils. Starting from a generic tensor compiler implemented in the MLIR framework, tensor abstractions are incrementally refined and lowered down to parallel, tiled, fused and vectorized code. We used our generator to implement a realistic, implicit solver for structured meshes, and demonstrate results competitive with an industrial computational fluid dynamics framework. We also compare with stand-alone stencil kernels for dense tensors.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/essadki2023code_generation_for_in-place_stencils.pdf}
}

@article{fawzi2022discovering,
  title = {Discovering Faster Matrix Multiplication Algorithms with Reinforcement Learning},
  author = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and {Romera-Paredes}, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and R. Ruiz, Francisco J. and Schrittwieser, Julian and Swirszcz, Grzegorz and Silver, David and Hassabis, Demis and Kohli, Pushmeet},
  year = {2022},
  month = oct,
  journal = {Nature},
  volume = {610},
  number = {7930},
  pages = {47--53},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-022-05172-4},
  urldate = {2022-10-07},
  abstract = {Improving the efficiency of algorithms for fundamental computations can have a widespread impact, as it can affect the overall speed of a large amount of computations. Matrix multiplication is one such primitive task, occurring in many systems---from neural networks to scientific computing routines. The automatic discovery of algorithms using machine learning offers the prospect of reaching beyond human intuition and outperforming the current best human-designed algorithms. However, automating the algorithm discovery procedure is intricate, as the space of possible algorithms is enormous. Here we report a deep reinforcement learning approach based on AlphaZero1 for discovering efficient and provably correct algorithms for the multiplication of arbitrary matrices. Our agent, AlphaTensor, is trained to play a single-player game where the objective is finding tensor decompositions within a finite factor space. AlphaTensor discovered algorithms that outperform the state-of-the-art complexity for many matrix sizes. Particularly relevant is the case of 4\,{\texttimes}\,4 matrices in a finite field, where AlphaTensor's algorithm improves on Strassen's two-level algorithm for the first time, to our knowledge, since its discovery 50 years ago2. We further showcase the flexibility of AlphaTensor through different use-cases: algorithms with state-of-the-art complexity for structured matrix multiplication and improved practical efficiency by optimizing matrix multiplication for runtime on specific hardware. Our results highlight AlphaTensor's ability to accelerate the process of algorithmic discovery on a range of problems, and to optimize for different criteria.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Applied mathematics,Computer science},
  file = {/home/vatai/Sync/zotero-data/pdfs/fawzi2022discovering_faster_matrix_multiplication_algorithms_with_reinforcement_learning.pdf;/home/vatai/Sync/zotero-data/storage/A7CU2C5G/s41586-022-05172-4.html}
}

@article{fay2019how,
  title = {How Quantum Is Radical Pair Magnetoreception?},
  author = {Fay, Thomas P. and Lindoy, Lachlan P. and Manolopoulos, David E. and Hore, P. J.},
  year = {2019},
  month = dec,
  journal = {Faraday Discuss.},
  volume = {221},
  number = {0},
  pages = {77--91},
  publisher = {The Royal Society of Chemistry},
  issn = {1364-5498},
  doi = {10.1039/C9FD00049F},
  urldate = {2024-02-13},
  abstract = {Currently the most likely mechanism of the magnetic compass sense in migratory songbirds relies on the coherent spin dynamics of pairs of photochemically formed radicals in the retina. Spin-conserving electron transfer reactions are thought to result in radical pairs whose near-degenerate electronic singlet and triplet states interconvert coherently as a result of hyperfine, exchange, and dipolar couplings and, crucially for a compass sensor, Zeeman interactions with the geomagnetic field. In this way, the yields of the reaction products can be influenced by magnetic interactions a million times smaller than kBT. The question we ask here is whether one can only account for the coherent spin dynamics using quantum mechanics. We find that semiclassical approximations to the spin dynamics of radical pairs only provide a satisfactory description of the anisotropic product yields when there is no electron spin--spin coupling, a situation unlikely to be consistent with a magnetic sensing function. Although these methods perform reasonably well for shorter-lived radical pairs with stronger electron-spin coupling, the accurate simulation of anisotropic magnetic field effects relevant to magnetoreception seems to require full quantum mechanical calculations.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/fay2019how_quantum_is_radical_pair_magnetoreception.pdf}
}

@article{fay2021spin,
  title = {Spin Relaxation in Radical Pairs from the Stochastic {{Schr{\"o}dinger}} Equation},
  author = {Fay, Thomas P. and Lindoy, Lachlan P. and Manolopoulos, David E.},
  year = {2021},
  month = feb,
  journal = {The Journal of Chemical Physics},
  volume = {154},
  number = {8},
  pages = {084121},
  issn = {0021-9606},
  doi = {10.1063/5.0040519},
  urldate = {2024-02-26},
  abstract = {We show that the stochastic Schr{\"o}dinger equation (SSE) provides an ideal way to simulate the quantum mechanical spin dynamics of radical pairs. Electron spin relaxation effects arising from fluctuations in the spin Hamiltonian are straightforward to include in this approach, and their treatment can be combined with a highly efficient stochastic evaluation of the trace over nuclear spin states that is required to compute experimental observables. These features are illustrated in example applications to a flavin--tryptophan radical pair of interest in avian magnetoreception and to a problem involving spin-selective radical pair recombination along a molecular wire. In the first of these examples, the SSE is shown to be both more efficient and more widely applicable than a recent stochastic implementation of the Lindblad equation, which only provides a valid treatment of relaxation in the extreme-narrowing limit. In the second, the exact SSE results are used to assess the accuracy of a recently proposed combination of Nakajima--Zwanzig theory for the spin relaxation and Schulten--Wolynes theory for the spin dynamics, which is applicable to radical pairs with many more nuclear spins. We also analyze the efficiency of trace sampling in some detail, highlighting the particular advantages of sampling with SU(N) coherent states.},
  file = {/home/vatai/Sync/zotero-data/pdfs/fay2021spin_relaxation_in_radical_pairs_from_the_stochastic_schrödinger_equation.pdf;/home/vatai/Sync/zotero-data/storage/MCZ3SBRC/Spin-relaxation-in-radical-pairs-from-the.html}
}

@article{fay2022simple,
  title = {A Simple Improved Low Temperature Correction for the Hierarchical Equations of Motion},
  author = {Fay, Thomas P.},
  year = {2022},
  month = aug,
  journal = {The Journal of Chemical Physics},
  volume = {157},
  number = {5},
  pages = {054108},
  issn = {0021-9606},
  doi = {10.1063/5.0100365},
  urldate = {2023-07-28},
  abstract = {The study of open system quantum dynamics has been transformed by the hierarchical equations of motion (HEOM) method, which gives the exact dynamics for a system coupled to a harmonic bath at arbitrary temperature and system--bath coupling strength. However, in its standard form, this method is only consistent with the weak-coupling quantum master equation at all temperatures when many auxiliary density operators are included in the hierarchy, even when low temperature corrections are included. Here, we propose a new low temperature correction scheme for the termination of the hierarchy based on Zwanzig projection, which alleviates this problem and restores consistency with the weak-coupling master equation with a minimal hierarchy. The utility of the new correction scheme is demonstrated on a range of model systems, including the Fenna--Matthews--Olson complex. The new closure is found to improve convergence of the HEOM even beyond the weak-coupling limit and is very straightforward to implement in existing HEOM codes.},
  file = {/home/vatai/Sync/zotero-data/pdfs/fay2022a_simple_improved_low_temperature_correction_for_the_hierarchical_equations_of.pdf;/home/vatai/Sync/zotero-data/pdfs/fay2022a_simple_improved_low_temperature_correction_for_the_hierarchical_equations_of2.pdf;/home/vatai/Sync/zotero-data/storage/CMUCF2FV/A-simple-improved-low-temperature-correction-for.html}
}

@article{feautrier1991dataflow,
  title = {Dataflow {{Analysis}} of {{Array}} and {{Scalar References}}},
  author = {Feautrier, Paul},
  year = {1991},
  pages = {37},
  abstract = {Given a program written in a simple imperative language (assignment statements, for loops, a ne indices and loop limits), this paper presents an algorithm for analyzing the patterns along which values ow as the execution proceeds. For each array or scalar reference, the result is the name and iteration vector of the source statement as a function of the iteration vector of the referencing statement. The paper discusses several applications of the method: conversion of a program to a set of recurrence equations, array and scalar expansion, program veri cation and parallel program construction.},
  langid = {english},
  note = {Introduction of SCoP},
  file = {/home/vatai/Sync/zotero-data/pdfs/feautrier1991dataflow_analysis_of_array_and_scalar_references.pdf}
}

@article{feautrier1996efficient,
  title = {Some Efficient Solutions to the Affine Scheduling Problem {{Part I One-dimensional Time}}},
  author = {Feautrier, Paul},
  year = {1996},
  month = aug,
  journal = {International Journal of Parallel Programming},
  volume = {21},
  doi = {10.1007/BF01407835},
  abstract = {Programs and systems of recurrence equations may be represented as sets of actions which are to be executed subject to precedence constraints. In many cases, actions may be labelled by integral vectors in some iteration domain, and precedence constraints may be described by affine relations. A schedule for such a program is a function which assigns an execution date to each action. Knowledge of such a schedule allows one to estimate the intrinsic degree of parallelism of the program and to compile a parallel version for multiprocessor architectures or systolic arrays. This paper deals with the problem of finding closed form schedules as affine or piecewise affine functions of the iteration vector. An efficient algorithm is presented which reduces the scheduling problem to e-mail : Paul.Feautrier@prism.uvsq.fr 1 2 a parametric linear program of small size, which can be readily solved by an efficient algorithm. R'esum'e De nombreux programmes ou syst`emes d"equations de ...},
  file = {/home/vatai/Sync/zotero-data/pdfs/feautrier1996some_efficient_solutions_to_the_affine_scheduling_problem_part_i.pdf}
}

@article{feautrier1997efficient,
  title = {Some Efficient Solutions to the Affine Scheduling Problem {{Part II Multidimensional}} Time},
  author = {Feautrier, Paul},
  year = {1997},
  month = jan,
  journal = {International Journal of Parallel Programming},
  volume = {21},
  doi = {10.1007/BF01379404},
  abstract = {This paper extends the algorithms which were given in Part I to cases in which there is no affine schedule, i.e. to problems whose parallel complexity is polynomial but not linear. The natural generalization is to multidimensional schedules with lexicographic ordering as temporal succession. Multidimensional affine schedules, are, in a sense, equivalent to polynomial schedules, and are much easier to handle automatically. Furthermore, there is a strong connexion between multidimensional schedules and loop nests, which allows one to prove that a static control program always has a multidimensional schedule. Roughly, a larger dimension indicates less parallelism. In the algorithm which is presented here, this dimension is computed dynamically, and is just sufficient for scheduling the source program. The algorithm lends itself to a "divide and conquer" strategy. The paper gives some experimental evidence for the applicability, performances and limitations of the algorithm. 1 2 R'esum'e...},
  keywords = {read},
  file = {/home/vatai/Sync/zotero-data/pdfs/feautrier1997some_efficient_solutions_to_the_affine_scheduling_problem_part_ii.pdf}
}

@misc{feng2020codebert,
  title = {{{CodeBERT}}: {{A Pre-Trained Model}} for {{Programming}} and {{Natural Languages}}},
  shorttitle = {{{CodeBERT}}},
  author = {Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and Zhou, Ming},
  year = {2020},
  month = sep,
  number = {arXiv:2002.08155},
  eprint = {2002.08155},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2002.08155},
  urldate = {2022-09-29},
  abstract = {We present CodeBERT, a bimodal pre-trained model for programming language (PL) and nat-ural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language codesearch, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both bimodal data of NL-PL pairs and unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation tasks. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NL-PL probing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Programming Languages},
  note = {Comment: Accepted to Findings of EMNLP 2020. 12 pages},
  file = {/home/vatai/Sync/zotero-data/pdfs/feng2020codebert.pdf;/home/vatai/Sync/zotero-data/storage/7ABGZPGC/2002.html}
}

@misc{feng2023largescale,
  title = {Large-{{Scale Genomic Diversity Analysis}} of {{Patients}} with {{COVID-19}}},
  author = {Feng, Wu-chun},
  year = {2023},
  file = {/home/vatai/Sync/zotero-data/pdfs/feng2023large-scale_genomic_diversity_analysis_of_patients_with_covid-19.pdf}
}

@inproceedings{finnie-ansley2022robots,
  title = {The {{Robots Are Coming}}: {{Exploring}} the {{Implications}} of {{OpenAI Codex}} on {{Introductory Programming}}},
  shorttitle = {The {{Robots Are Coming}}},
  booktitle = {Australasian {{Computing Education Conference}}},
  author = {{Finnie-Ansley}, James and Denny, Paul and Becker, Brett A. and {Luxton-Reilly}, Andrew and Prather, James},
  year = {2022},
  month = feb,
  series = {{{ACE}} '22},
  pages = {10--19},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3511861.3511863},
  urldate = {2022-08-04},
  abstract = {Recent advances in artificial intelligence have been driven by an exponential growth in digitised data. Natural language processing, in particular, has been transformed by machine learning models such as OpenAI's GPT-3 which generates human-like text so realistic that its developers have warned of the dangers of its misuse. In recent months OpenAI released Codex, a new deep learning model trained on Python code from more than 50 million GitHub repositories. Provided with a natural language description of a programming problem as input, Codex generates solution code as output. It can also explain (in English) input code, translate code between programming languages, and more. In this work, we explore how Codex performs on typical introductory programming problems. We report its performance on real questions taken from introductory programming exams and compare it to results from students who took these same exams under normal conditions, demonstrating that Codex outscores most students. We then explore how Codex handles subtle variations in problem wording using several published variants of the well-known ``Rainfall Problem'' along with one unpublished variant we have used in our teaching. We find the model passes many test cases for all variants. We also explore how much variation there is in the Codex generated solutions, observing that an identical input prompt frequently leads to very different solutions in terms of algorithmic approach and code length. Finally, we discuss the implications that such technology will have for computing education as it continues to evolve, including both challenges and opportunities.},
  isbn = {978-1-4503-9643-1},
  keywords = {academic integrity,AI,artificial intelligence,code generation,code writing,Codex,copilot,CS1,deep learning,GitHub,GPT-3,introductory programming,machine learning,neural networks,novice programming,OpenAI},
  file = {/home/vatai/Sync/zotero-data/pdfs/finnie-ansley2022the_robots_are_coming.pdf}
}

@incollection{floyd1993assigning,
  title = {Assigning {{Meanings}} to {{Programs}}},
  booktitle = {Program {{Verification}}: {{Fundamental Issues}} in {{Computer Science}}},
  author = {Floyd, Robert W.},
  editor = {Colburn, Timothy R. and Fetzer, James H. and Rankin, Terry L.},
  year = {1993},
  series = {Studies in {{Cognitive Systems}}},
  pages = {65--81},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-011-1793-7_4},
  urldate = {2023-06-14},
  abstract = {This paper attempts to provide an adequate basis for formal definitions of the meanings of programs in appropriately defined programming languages, in such a way that a rigorous standard is established for proofs about computer programs, including proofs of correctness, equivalence, and termination. The basis of our approach is the notion of an interpretation of a program: that is, an association of a proposition with each connection in the flow of control through a program, where the proposition is asserted to hold whenever that connection is taken. To prevent an interpretation from being chosen arbitrarily, a condition is imposed on each command of the program. This condition guarantees that whenever a command is reached by way of a connection whose associated proposition is then true, it will be left (if at all) by a connection whose associated proposition will be true at that time. Then by induction on the number of commands executed, one sees that if a program is entered by a connection whose associated proposition is then true, it will be left (if at all) by a connection whose associated proposition will be true at that time. By this means, we may prove certain properties of programs, particularly properties of the form: `If the initial values of the program variables satisfy the relation Rl, the final values on completion will satisfy the relation R2'.},
  isbn = {978-94-011-1793-7},
  langid = {english},
  keywords = {Assignment Statement,Deductive System,Free Variable,Statement List,Verification Condition}
}

@article{franchetti2018spiral,
  title = {{{SPIRAL}}: {{Extreme Performance Portability}}},
  shorttitle = {{{SPIRAL}}},
  author = {Franchetti, Franz and Low, Tze Meng and Popovici, Doru Thom and Veras, Richard M. and Spampinato, Daniele G. and Johnson, Jeremy R. and Puschel, Markus and Hoe, James C. and Moura, Jose M. F.},
  year = {2018},
  month = nov,
  journal = {Proc. IEEE},
  volume = {106},
  number = {11},
  pages = {1935--1968},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2018.2873289},
  urldate = {2022-10-17},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/franchetti2018spiral.pdf}
}

@misc{frankle2019lottery,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2019},
  month = mar,
  number = {arXiv:1803.03635},
  eprint = {1803.03635},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-07},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: ICLR camera ready},
  file = {/home/vatai/Sync/zotero-data/pdfs/frankle_carbin2019the_lottery_ticket_hypothesis.pdf}
}

@inproceedings{gale2020sparse,
  title = {Sparse {{GPU}} Kernels for Deep Learning},
  booktitle = {Proceedings of the {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Gale, Trevor and Zaharia, Matei and Young, Cliff and Elsen, Erich},
  year = {2020},
  month = nov,
  series = {{{SC}} '20},
  pages = {1--14},
  publisher = {IEEE Press},
  address = {Atlanta, Georgia},
  urldate = {2022-09-07},
  abstract = {Scientific workloads have traditionally exploited high levels of sparsity to accelerate computation and reduce memory requirements. While deep neural networks can be made sparse, achieving practical speedups on GPUs is difficult because these applications have relatively moderate levels of sparsity that are not sufficient for existing sparse kernels to outperform their dense counterparts. In this work, we study sparse matrices from deep learning applications and identify favorable properties that can be exploited to accelerate computation. Based on these insights, we develop high-performance GPU kernels for two sparse matrix operations widely applicable in neural networks: sparse matrix-dense matrix multiplication and sampled dense-dense matrix multiplication. Our kernels reach 27\% of single-precision peak on Nvidia V100 GPUs. Using our kernels, we demonstrate sparse Transformer and MobileNet models that achieve 1.2--2.1X speedups and up to 12.8X memory savings without sacrificing accuracy.},
  isbn = {978-1-72819-998-6},
  keywords = {graphics processing units,neural networks,sparse matrices},
  file = {/home/vatai/Sync/zotero-data/pdfs/gale2020sparse_gpu_kernels_for_deep_learning.pdf}
}

@inproceedings{gardner2006parallel,
  title = {Parallel {{Genomic Sequence-Searching}} on an {{Ad-Hoc Grid}}: {{Experiences}}, {{Lessons Learned}}, and {{Implications}}},
  shorttitle = {Parallel {{Genomic Sequence-Searching}} on an {{Ad-Hoc Grid}}},
  booktitle = {{{ACM}}/{{IEEE SC}} 2006 {{Conference}} ({{SC}}'06)},
  author = {Gardner, Mark K. and Feng, Wu-chun and Archuleta, Jeremy and Lin, Heshan and Ma, Xiaosong},
  year = {2006},
  month = nov,
  pages = {22--22},
  publisher = {IEEE},
  address = {Tampa, FL},
  doi = {10.1109/SC.2006.46},
  urldate = {2023-02-28},
  abstract = {The Basic Local Alignment Search Tool (BLAST) allows bioinformaticists to characterize an unknown sequence by comparing it against a database of known sequences. The similarity between sequences enables biologists to detect evolutionary relationships and infer biological properties of the unknown sequence. mpiBLAST, our parallel BLAST, decreases the search time of a 300 KB query on the current NT database from over two full days to under 10 minutes on a 128processor cluster and allows larger query files to be compared. Consequently, we propose to compare the largest query available, the entire NT database, against the largest database available, the entire NT database. The result of this comparison will provide critical information to the biology community, including insightful evolutionary, structural, and functional relationships between every sequence and family in the NT database. Preliminary projections indicated that to complete the above task in a reasonable length of time required more processors than were available to us at a single site. Hence, we assembled GreenGene, an ad-hoc grid that was constructed ``on the fly'' from donated computational, network, and storage resources during last year's SC{\textbar}05. GreenGene consisted of 3048 processors from machines that were distributed across the United States. This paper presents a case study of mpiBLAST on GreenGene --- specifically, a pre-run characterization of the computation, the hardware and software architectural design, experimental results, and future directions.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/gardner2006parallel_genomic_sequence-searching_on_an_ad-hoc_grid.pdf}
}

@article{girbal2006semiautomatic,
  title = {Semi-{{Automatic Composition}} of {{Loop Transformations}} for {{Deep Parallelism}} and {{Memory Hierarchies}}},
  author = {Girbal, Sylvain and Vasilache, Nicolas and Bastoul, C{\'e}dric and Cohen, Albert and Parello, David and Sigler, Marc and Temam, Olivier},
  year = {2006},
  month = jun,
  journal = {Int J Parallel Prog},
  volume = {34},
  number = {3},
  pages = {261--317},
  issn = {0885-7458, 1573-7640},
  doi = {10.1007/s10766-006-0012-3},
  urldate = {2022-05-09},
  abstract = {Modern compilers are responsible for translating the idealistic operational semantics of the source program into a form that makes efficient use of a highly complex heterogeneous machine. Since optimization problems are associated with huge and unstructured search spaces, this combinational task is poorly achieved in general, resulting in weak scalability and disappointing sustained performance. We address this challenge by working on the program representation itself, using a semi-automatic optimization approach to demonstrate that current compilers offen suffer from unnecessary constraints and intricacies that can be avoided in a semantically richer transformation framework. Technically, the purpose of this paper is threefold: (1) to show that syntactic code representations close to the operational semantics lead to rigid phase ordering and cumbersome expression of architecture-aware loop transformations, (2) to illustrate how complex transformation sequences may be needed to achieve significant performance benefits, (3) to facilitate the automatic search for program transformation sequences, improving on classical polyhedral representations to better support operation research strategies in a simpler, structured search space. The proposed framework relies on a unified polyhedral representation of loops and statements, using normalization rules to allow flexible and expressive transformation sequencing. This representation allows to extend the scalability of polyhedral dependence analysis, and to delay the (automatic) legality checks until the end of a transformation sequence. Our work leverages on algorithmic advances in polyhedral code generation and has been implemented in a modern research compiler.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/girbal2006semi-automatic_composition_of_loop_transformations_for_deep_parallelism_and.pdf}
}

@misc{github2022github,
  title = {{{GitHub Copilot}} {$\cdot$} {{Your AI}} Pair Programmer},
  author = {{Github}},
  year = {2022},
  urldate = {2022-08-04},
  abstract = {GitHub Copilot works alongside you directly in your editor, suggesting whole lines or entire functions for you.},
  howpublished = {https://github.com/features/copilot},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/storage/8V6DE2UG/copilot.html}
}

@article{grosser2012polly,
  title = {Polly - {{Performing Polyhedral Optimizations}} on a {{Low-Level Intermediate Representation}}},
  author = {Grosser, Tobias and Groesslinger, Armin and Lengauer, Christian},
  year = {2012},
  month = dec,
  journal = {Parallel Process. Lett.},
  volume = {22},
  number = {04},
  pages = {1250010},
  issn = {0129-6264, 1793-642X},
  doi = {10.1142/S0129626412500107},
  urldate = {2024-08-29},
  abstract = {The polyhedral model for loop parallelization has proved to be an effective tool for advanced optimization and automatic parallelization of programs in higher-level languages. Yet, to integrate such optimizations seamlessly into production compilers, they must be performed on the compiler's internal, low-level, intermediate representation (IR). With Polly, we present an infrastructure for polyhedral optimizations on such an IR. We describe the detection of program parts amenable to a polyhedral optimization (so-called static control parts), their translation to a Z-polyhedral representation, optimizations on this representation and the generation of optimized IR code. Furthermore, we define an interface for connecting external optimizers and present a novel way of using the parallelism they introduce to generate SIMD and OpenMP code. To evaluate Polly, we compile the PolyBench 2.0 benchmarks fully automatically with PLuTo as external optimizer and parallelizer. We can report on significant speedups.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/Codegen/grosser2012polly_—_performing_polyhedral_optimizations_on_a_low-level_intermediate_representation.pdf}
}

@article{grosser2015polyhedral,
  title = {Polyhedral {{AST Generation Is More Than Scanning Polyhedra}}},
  author = {Grosser, Tobias and Verdoolaege, Sven and Cohen, Albert},
  year = {2015},
  month = jul,
  journal = {ACM Trans. Program. Lang. Syst.},
  volume = {37},
  number = {4},
  pages = {12:1--12:50},
  issn = {0164-0925},
  doi = {10.1145/2743016},
  urldate = {2022-02-16},
  abstract = {Abstract mathematical representations such as integer polyhedra have been shown to be useful to precisely analyze computational kernels and to express complex loop transformations. Such transformations rely on abstract syntax tree (AST) generators to convert the mathematical representation back to an imperative program. Such generic AST generators avoid the need to resort to transformation-specific code generators, which may be very costly or technically difficult to develop as transformations become more complex. Existing AST generators have proven their effectiveness, but they hit limitations in more complex scenarios. Specifically, (1) they do not support or may fail to generate control flow for complex transformations using piecewise schedules or mappings involving modulo arithmetic; (2) they offer limited support for the specialization of the generated code exposing compact, straightline, vectorizable kernels with high arithmetic intensity necessary to exploit the peak performance of modern hardware; (3) they offer no support for memory layout transformations; and (4) they provide insufficient control over the AST generation strategy, preventing their application to complex domain-specific optimizations. We present a new AST generation approach that extends classical polyhedral scanning to the full generality of Presburger arithmetic, including existentially quantified variables and piecewise schedules, and introduce new optimizations for the detection of components and shifted strides. Not limiting ourselves to control flow generation, we expose functionality to generate AST expressions from arbitrary piecewise quasi-affine expressions, which enables the use of our AST generator for data-layout transformations. We complement this with support for specialization by polyhedral unrolling, user-directed versioning, and specialization of AST expressions according to the location at which they are generated, and we complete this work with fine-grained user control over the AST generation strategies used. Using this generalized idea of AST generation, we present how to implement complex domain-specific transformations without the need to write specialized code generators, but instead relying on a generic AST generator parametrized to a specific problem domain.},
  keywords = {code generation,index set splitting,Polyhedral compilation,Presburger relations,unrolling},
  file = {/home/vatai/Sync/zotero-data/pdfs/grosser2015polyhedral_ast_generation_is_more_than_scanning_polyhedra.pdf}
}

@inproceedings{hall2010loop,
  title = {Loop {{Transformation Recipes}} for {{Code Generation}} and {{Auto-Tuning}}},
  booktitle = {Languages and {{Compilers}} for {{Parallel Computing}}},
  author = {Hall, Mary and Chame, Jacqueline and Chen, Chun and Shin, Jaewook and Rudy, Gabe and Khan, Malik Murtaza},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Gao, Guang R. and Pollock, Lori L. and Cavazos, John and Li, Xiaoming},
  year = {2010},
  volume = {5898},
  pages = {50--64},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-13374-9_4},
  urldate = {2024-11-10},
  abstract = {In this paper, we describe transformation recipes, which provide a high-level interface to the code transformation and code generation capability of a compiler. These recipes can be generated by compiler decision algorithms or savvy software developers. This interface is part of an auto-tuning framework that explores a set of different implementations of the same computation and automatically selects the best-performing implementation. Along with the original computation, a transformation recipe specifies a range of implementations of the computation resulting from composing a set of high-level code transformations. In our system, an underlying polyhedral framework coupled with transformation algorithms takes this set of transformations, composes them and automatically generates correct code. We first describe an abstract interface for transformation recipes, which we propose to facilitate interoperability with other transformation frameworks. We then focus on the specific transformation recipe interface used in our compiler and present performance results on its application to kernel and library tuning and tuning of key computations in high-end applications. We also show how this framework can be used to generate and auto-tune parallel OpenMP or CUDA code from a high-level specification.},
  isbn = {978-3-642-13373-2 978-3-642-13374-9},
  note = {[TLDR] An abstract interface for transformation recipes is described, which is proposed to facilitate interoperability with other transformation frameworks and present performance results on its application to kernel and library tuning and tuning of key computations in high-end applications.}
}

@misc{higham2023diffusion,
  title = {Diffusion {{Models}} for {{Generative Artificial Intelligence}}: {{An Introduction}} for {{Applied Mathematicians}}},
  shorttitle = {Diffusion {{Models}} for {{Generative Artificial Intelligence}}},
  author = {Higham, Catherine F. and Higham, Desmond J. and Grindrod, Peter},
  year = {2023},
  month = dec,
  number = {arXiv:2312.14977},
  eprint = {2312.14977},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-15},
  abstract = {Generative artificial intelligence (AI) refers to algorithms that create synthetic but realistic output. Diffusion models currently offer state of the art performance in generative AI for images. They also form a key component in more general tools, including text-to-image generators and large language models. Diffusion models work by adding noise to the available training data and then learning how to reverse the process. The reverse operation may then be applied to new random data in order to produce new outputs. We provide a brief introduction to diffusion models for applied mathematicians and statisticians. Our key aims are (a) to present illustrative computational examples, (b) to give a careful derivation of the underlying mathematical formulas involved, and (c) to draw a connection with partial differential equation (PDE) diffusion models. We provide code for the computational experiments. We hope that this topic will be of interest to advanced undergraduate students and postgraduate students. Portions of the material may also provide useful motivational examples for those who teach courses in stochastic processes, inference, machine learning, PDEs or scientific computing.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T07 60J60,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2,I.2.6},
  file = {/home/vatai/Sync/zotero-data/storage/P6D5X8R7/Higham et al. - 2023 - Diffusion Models for Generative Artificial Intelli.pdf}
}

@article{hiscock2016quantum,
  title = {The Quantum Needle of the Avian Magnetic Compass},
  author = {Hiscock, Hamish G. and Worster, Susannah and Kattnig, Daniel R. and Steers, Charlotte and Jin, Ye and Manolopoulos, David E. and Mouritsen, Henrik and Hore, P. J.},
  year = {2016},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {17},
  pages = {4634--4639},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1600341113},
  urldate = {2022-11-02},
  abstract = {Migratory birds have a light-dependent magnetic compass, the mechanism of which is thought to involve radical pairs formed photochemically in cryptochrome proteins in the retina. Theoretical descriptions of this compass have thus far been unable to account for the high precision with which birds are able to detect the direction of the Earth's magnetic field. Here we use coherent spin dynamics simulations to explore the behavior of realistic models of cryptochrome-based radical pairs. We show that when the spin coherence persists for longer than a few microseconds, the output of the sensor contains a sharp feature, referred to as a spike. The spike arises from avoided crossings of the quantum mechanical spin energy-levels of radicals formed in cryptochromes. Such a feature could deliver a heading precision sufficient to explain the navigational behavior of migratory birds in the wild. Our results (i) afford new insights into radical pair magnetoreception, (ii) suggest ways in which the performance of the compass could have been optimized by evolution, (iii) may provide the beginnings of an explanation for the magnetic disorientation of migratory birds exposed to anthropogenic electromagnetic noise, and (iv) suggest that radical pair magnetoreception may be more of a quantum biology phenomenon than previously realized.},
  file = {/home/vatai/Sync/zotero-data/pdfs/hiscock2016the_quantum_needle_of_the_avian_magnetic_compass.pdf}
}

@misc{ho2020denoising,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-01-17},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/vatai/Sync/zotero-data/storage/DTFAQSNR/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@article{hoare1969axiomatic,
  title = {An Axiomatic Basis for Computer Programming},
  author = {Hoare, C A R},
  year = {1969},
  volume = {12},
  number = {10},
  abstract = {In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantages, both theoretical and practical, may follow from a pursuance of these topics.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/hoare1969an_axiomatic_basis_for_computer_programming.pdf}
}

@misc{horger2018arbitrary,
  title = {Towards {{Arbitrary Noise Augmentation}} - {{Deep Learning}} for {{Sampling}} from {{Arbitrary Probability Distributions}}},
  author = {Horger, Felix and W{\"u}rfl, Tobias and Christlein, Vincent and Maier, Andreas},
  year = {2018},
  month = jul,
  number = {arXiv:1801.04211},
  eprint = {1801.04211},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.04211},
  urldate = {2023-08-15},
  abstract = {Accurate noise modelling is important for training of deep learning reconstruction algorithms. While noise models are well known for traditional imaging techniques, the noise distribution of a novel sensor may be difficult to determine a priori. Therefore, we propose learning arbitrary noise distributions. To do so, this paper proposes a fully connected neural network model to map samples from a uniform distribution to samples of any explicitly known probability density function. During the training, the Jensen-Shannon divergence between the distribution of the model's output and the target distribution is minimized. We experimentally demonstrate that our model converges towards the desired state. It provides an alternative to existing sampling methods such as inversion sampling, rejection sampling, Gaussian mixture models and Markov-Chain-Monte-Carlo. Our model has high sampling efficiency and is easily applied to any probability distribution, without the need of further analytical or numerical calculations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/vatai/Sync/zotero-data/pdfs/horger2018towards_arbitrary_noise_augmentation_-_deep_learning_for_sampling_from.pdf;/home/vatai/Sync/zotero-data/storage/GRZ84WWN/1801.html}
}

@techreport{hornung2014raja,
  title = {The {{RAJA Portability Layer}}: {{Overview}} and {{Status}}},
  shorttitle = {The {{RAJA Portability Layer}}},
  author = {Hornung, Richard D. and Keasler, Jeffrey A.},
  year = {2014},
  month = sep,
  number = {LLNL-TR-661403},
  institution = {Lawrence Livermore National Lab. (LLNL), Livermore, CA (United States)},
  doi = {10.2172/1169830},
  urldate = {2022-07-30},
  abstract = {As computer architectures become increasingly complex and diverse, application developers face difficult challenges to achieve high performance while maintaining code portability. The problem is especially acute for large ASC multiphysics codes. Efficient parallel execution often requires tuning algorithms and data access to match processor and memory system constraints. Changing compiler directives and parallel programming model constructs on thousands of individual loops in a large code is disruptive and unwieldy. RAJA is a programming approach that we have been developing at Lawrence Livermore National Laboratory to encapsulate platform-specific concerns, related to both hardware and parallel programming models. The RAJA abstraction layer simplifies porting C/C++ codes to various programming models and architectures by reducing effort and developer disruption. In this report, we motivate and describe key aspects of RAJA. We also present a preliminary assessment of RAJA based on exploration in three ASC hydrodynamics codes at LLNL, which was one part of a three-part ASC Level 2 milestone, completed in September 2014.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/hornung2014the_raja_portability_layer.pdf;/home/vatai/Sync/zotero-data/storage/JSPQUL32/1169830.html}
}

@misc{hu2021lora,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.09685},
  urldate = {2023-05-08},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency},
  file = {/home/vatai/Sync/zotero-data/pdfs/hu2021lora.pdf;/home/vatai/Sync/zotero-data/storage/4MZSKETX/2106.html}
}

@misc{hu2021loraa,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-27},
  abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency},
  file = {/home/vatai/Sync/zotero-data/pdfs/hu2021lora2.pdf}
}

@inproceedings{jain2018microscope,
  title = {Microscope on {{Memory}}: {{MPSoC-Enabled Computer Memory System Assessments}}},
  shorttitle = {Microscope on {{Memory}}},
  booktitle = {2018 {{IEEE}} 26th {{Annual International Symposium}} on {{Field-Programmable Custom Computing Machines}} ({{FCCM}})},
  author = {Jain, Abhishek Kumar and Lloyd, Scott and Gokhale, Maya},
  year = {2018},
  month = apr,
  pages = {173--180},
  issn = {2576-2621},
  doi = {10.1109/FCCM.2018.00035},
  abstract = {Recent advances in new memory technologies and packaging options has focused attention on computer memory system design and evaluation. Examples include high bandwidth memories such as Hybrid Memory Cube and HBM, and 3DXPoint non-volatile memory. Emerging memories display a wide range of bandwidths, latencies, and capacities, making it challenging for the computer architect to navigate the design space of potential memory configurations, and for the application developer to assess performance implications of using such memories. In this work, we describe the Logic in Memory Emulator (LiME), a hardware/software tool specially designed for memory system evaluation and experiment. LiME uses the Xilinx Zynq UltraScale+ MPSoC on the ZCU102 board to capture any/all memory access, either from the Processing System (PS) or the Programmable Logic (PL). LiME employs novel loopback circuitry in conjunction with address map relocation to pass memory references from the PS into the PL side. The memory request is looped back into the PS DRAM memory controller and concurrently processed by LiME. We have demonstrated four high value use cases: full external memory access logging and replay, emulation of various memory system latencies by passing the memory request through delay registers before entering the PS memory subsystem, emulation of acceleration engines that can independently access memory, and performance comparison of two CPU architectures with respect to their memory behavior. In this paper we will describe this novel application of state-of-the-art MPSoC embodied by the LiME framework and highlight its uses.},
  keywords = {Acceleration,Bandwidth,Delays,Emulation,Field programmable gate arrays,Logic in Memory Emulator,Memory Access Logging and Replay,Near Memory Accelators,Program processors,Random access memory},
  note = {Artur suggested this paper, to introduce latency to cache/memory using FPGAs.
\par
Sano san has FPGA{\dots}},
  file = {/home/vatai/Sync/zotero-data/pdfs/jain2018microscope_on_memory.pdf;/home/vatai/Sync/zotero-data/storage/7KJWXF66/8457650.html}
}

@misc{jaszczur2021sparsea,
  title = {Sparse Is {{Enough}} in {{Scaling Transformers}}},
  author = {Jaszczur, Sebastian and Chowdhery, Aakanksha and Mohiuddin, Afroz and Kaiser, {\L}ukasz and Gajewski, Wojciech and Michalewski, Henryk and Kanerva, Jonni},
  year = {2021},
  month = nov,
  number = {arXiv:2111.12763},
  eprint = {2111.12763},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-09-07},
  abstract = {Large Transformer models yield impressive results on many tasks, but are expensive to train, or even fine-tune, and so slow at decoding that their use and study becomes out of reach. We address this problem by leveraging sparsity. We study sparse variants for all layers in the Transformer and propose Scaling Transformers, a family of next generation Transformer models that use sparse layers to scale efficiently and perform unbatched decoding much faster than the standard Transformer as we scale up the model size. Surprisingly, the sparse layers are enough to obtain the same perplexity as the standard Transformer with the same number of parameters. We also integrate with prior sparsity approaches to attention and enable fast inference on long sequences even with limited memory. This results in performance competitive to the state-of-the-art on long text summarization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: NeurIPS 2021},
  file = {/home/vatai/Sync/zotero-data/pdfs/jaszczur2021sparse_is_enough_in_scaling_transformers2.pdf}
}

@article{johansson2012qutip,
  title = {{{QuTiP}}: {{An}} Open-Source {{Python}} Framework for the Dynamics of Open Quantum Systems},
  shorttitle = {{{QuTiP}}},
  author = {Johansson, J. R. and Nation, P. D. and Nori, Franco},
  year = {2012},
  month = aug,
  journal = {Computer Physics Communications},
  volume = {183},
  number = {8},
  pages = {1760--1772},
  issn = {0010-4655},
  doi = {10.1016/j.cpc.2012.02.021},
  urldate = {2022-04-27},
  abstract = {We present an object-oriented open-source framework for solving the dynamics of open quantum systems written in Python. Arbitrary Hamiltonians, including time-dependent systems, may be built up from operators and states defined by a quantum object class, and then passed on to a choice of master equation or Monte Carlo solvers. We give an overview of the basic structure for the framework before detailing the numerical simulation of open system dynamics. Several examples are given to illustrate the build up to a complete calculation. Finally, we measure the performance of our library against that of current implementations. The framework described here is particularly well suited to the fields of quantum optics, superconducting circuit devices, nanomechanics, and trapped ions, while also being ideal for use in classroom instruction. Program summary Program title: QuTiP: The Quantum Toolbox in Python Catalogue identifier: AEMB\_v1\_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEMB\_v1\_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 No. of lines in distributed program, including test data, etc.: 16\,482 No. of bytes in distributed program, including test data, etc.: 213\,438 Distribution format: tar.gz Programming language: Python Computer: i386, x86-64 Operating system: Linux, Mac OSX, Windows RAM: 2+ Gigabytes Classification: 7 External routines: NumPy (http://numpy.scipy.org/), SciPy (http://www.scipy.org/), Matplotlib (http://matplotlib.sourceforge.net/) Nature of problem: Dynamics of open quantum systems. Solution method: Numerical solutions to Lindblad master equation or Monte Carlo wave function method. Restrictions: Problems must meet the criteria for using the master equation in Lindblad form. Running time: A few seconds up to several tens of minutes, depending on size of underlying Hilbert space.},
  langid = {english},
  keywords = {Lindblad master equation,Open quantum systems,Python,Quantum Monte Carlo},
  file = {/home/vatai/Sync/zotero-data/pdfs/johansson2012qutip.pdf;/home/vatai/Sync/zotero-data/storage/Q7K9JFID/S0010465512000835.html}
}

@article{johansson2013qutip,
  title = {{{QuTiP}} 2: {{A Python}} Framework for the Dynamics of Open Quantum Systems},
  shorttitle = {{{QuTiP}} 2},
  author = {Johansson, J. R. and Nation, P. D. and Nori, Franco},
  year = {2013},
  month = apr,
  journal = {Computer Physics Communications},
  volume = {184},
  number = {4},
  pages = {1234--1240},
  issn = {0010-4655},
  doi = {10.1016/j.cpc.2012.11.019},
  urldate = {2022-04-27},
  abstract = {We present version 2 of QuTiP, the Quantum Toolbox in Python. Compared to the preceding version [J.R. Johansson, P.D. Nation, F. Nori, Comput. Phys. Commun. 183 (2012) 1760.], we have introduced numerous new features, enhanced performance, and made changes in the Application Programming Interface (API) for improved functionality and consistency within the package, as well as increased compatibility with existing conventions used in other scientific software packages for Python. The most significant new features include efficient solvers for arbitrary time-dependent Hamiltonians and collapse operators, support for the Floquet formalism, and new solvers for Bloch--Redfield and Floquet--Markov master equations. Here we introduce these new features, demonstrate their use, and give a summary of the important backward-incompatible API changes introduced in this version. Program Summary Program title: QuTiP: The Quantum Toolbox in Python Catalog identifier: AEMB\_v2\_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEMB\_v2\_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: GNU General Public License, version 3 No. of lines in distributed program, including test data, etc.: 33625 No. of bytes in distributed program, including test data, etc.: 410064 Distribution format: tar.gz Programming language: Python. Computer: i386, x86-64. Operating system: Linux, Mac OSX. RAM: 2+ Gigabytes Classification: 7. External routines: NumPy, SciPy, Matplotlib, Cython Catalog identifier of previous version: AEMB\_v1\_0 Journal reference of previous version: Comput. Phys. Comm. 183 (2012) 1760 Does the new version supercede the previous version?: Yes Nature of problem: Dynamics of open quantum systems Solution method: Numerical solutions to Lindblad, Floquet--Markov, and Bloch--Redfield master equations, as well as the Monte Carlo wave function method. Reasons for new version: Compared to the preceding version we have introduced numerous new features, enhanced performance, and made changes in the Application Programming Interface (API) for improved functionality and consistency within the package, as well as increased compatibility with existing conventions used in other scientific software packages for Python. The most significant new features include efficient solvers for arbitrary time-dependent Hamiltonians and collapse operators, support for the Floquet formalism, and new solvers for Bloch--Redfield and Floquet--Markov master equations. Restrictions: Problems must meet the criteria for using the master equation in Lindblad, Floquet--Markov, or Bloch--Redfield form. Running time: A few seconds up to several tens of hours, depending on size of the underlying Hilbert space.},
  langid = {english},
  keywords = {Bloch-Redfield,Floquet-Markov,Lindblad,Master equation,Open quantum systems,Python,Quantum Monte Carlo},
  file = {/home/vatai/Sync/zotero-data/pdfs/johansson2013qutip_2.pdf;/home/vatai/Sync/zotero-data/storage/KV6G3AGZ/S0010465512003955.html}
}

@article{johnson2020area,
  title = {Area {{Exam}}: {{General-Purpose Performance Portable Programming Models}} for {{Productive Exascale Computing}}},
  author = {Johnson, Alister},
  year = {2020},
  pages = {64},
  abstract = {Modern supercomputer architectures have grown increasingly complex and diverse since the end of Moore's law in the mid-2000s, and are far more difficult to program than their predecessors. While HPC programming models have improved such that applications are now generally portable between architectures, their performance can still vary wildly, and developers now need to spend a great deal of time tuning or even rewriting their applications for each new machine to get the performance they need. New performance portable programming models aim to solve this problem and give high performance on all architectures with minimal effort from developers. This area exam will survey many of these proposed general-purpose programming models, including libraries, parallel languages, directive-based language extensions, and source-to-source translators, and compare them in terms of use cases, performance, portability, and developer productivity. It will also discuss compiler and general-purpose language standard (e.g., C++) support for performance portability features.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/johnson2020area_exam.pdf}
}

@article{jones2011reaction,
  title = {Reaction Operators for Spin-Selective Chemical Reactions of Radical Pairs},
  author = {Jones, J. A. and Maeda, Kiminori and Hore, P. J.},
  year = {2011},
  month = may,
  journal = {Chemical Physics Letters},
  volume = {507},
  number = {4},
  pages = {269--273},
  issn = {0009-2614},
  doi = {10.1016/j.cplett.2011.03.082},
  urldate = {2022-10-08},
  abstract = {Spin-selective reactions of radical pairs have traditionally been modelled theoretically by adding phenomenological rate equations to the quantum mechanical equation of motion of the radical pair spin density matrix. More recently an alternative set of rate expressions, based on a quantum measurement approach, has been suggested. Here we show how these two reaction operators can be seen as limiting cases of a more general reaction scheme.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/jones2011reaction_operators_for_spin-selective_chemical_reactions_of_radical_pairs.pdf;/home/vatai/Sync/zotero-data/storage/XSZ895CM/S0009261411003666.html}
}

@article{justinek2004electrona,
  title = {Electron {{Self-Exchange Kinetics Determined}} by {{MARY Spectroscopy}}:\, {{Theory}} and {{Experiment}}},
  shorttitle = {Electron {{Self-Exchange Kinetics Determined}} by {{MARY Spectroscopy}}},
  author = {Justinek, M. and Grampp, G. and Landgraf, S. and Hore, P. J. and Lukzen, N. N.},
  year = {2004},
  month = may,
  journal = {J. Am. Chem. Soc.},
  volume = {126},
  number = {17},
  pages = {5635--5646},
  publisher = {American Chemical Society},
  issn = {0002-7863},
  doi = {10.1021/ja0394784},
  urldate = {2022-08-03},
  abstract = {The electron self-exchange between a neutral molecule and its charged radical, which is part of a spin-correlated radical ion pair, gives rise to line width effects in the fluorescence-detected MARY (magnetic field effect on reaction yield) spectrum similar to those observed in EPR spectroscopy. An increasing self-exchange rate (i.e., a higher concentration of the neutral molecule) leads to broadening and subsequent narrowing of the spectrum. Along with a series of MARY spectra recorded for several systems (the fluorophores pyrene, pyrene-d10 and N-methylcarbazole in combination with 1,2- and 1,4-dicyanobenzene) in various solvents, a theoretical model is developed that describes the spin evolution and the diffusive recombination of the radical pair under the influence of the external magnetic field and electron self-exchange, thereby allowing the simulation of MARY spectra of the systems investigated experimentally. The spin evolution of the radicals in the pair is calculated separately using spin correlation tensors, thereby allowing rigorous quantum mechanical calculations for real spin systems. It is shown that the combination of these simulations with high resolution, low noise experimental spectra makes the MARY technique a novel, quantitative method for the determination of self-exchange rate constants. In comparison to a simple analytical formula which estimates the self-exchange rate constant from the slope of the linear part of a line width vs concentration plot, the simulation method yields more reliable and accurate results. The correctness of the results obtained by the MARY method is proved by a comparison with corresponding data from the well-established EPR line broadening technique. With its less stringent restrictions on radical lifetime and stability, the MARY technique provides an alternative to the classical EPR method, in particular for systems involving short-lived and unstable radicals.},
  file = {/home/vatai/Sync/zotero-data/pdfs/justinek2004electron_self-exchange_kinetics_determined_by_mary_spectroscopy.pdf;/home/vatai/Sync/zotero-data/storage/26QJN2AN/ja0394784.html}
}

@inproceedings{katel2022mlirbased,
  title = {{{MLIR-based}} Code Generation for {{GPU}} Tensor Cores},
  booktitle = {Proceedings of the 31st {{ACM SIGPLAN International Conference}} on {{Compiler Construction}}},
  author = {Katel, Navdeep and Khandelwal, Vivek and Bondhugula, Uday},
  year = {2022},
  month = mar,
  pages = {117--128},
  publisher = {ACM},
  address = {Seoul South Korea},
  doi = {10.1145/3497776.3517770},
  urldate = {2022-03-31},
  abstract = {The state-of-the-art in high-performance deep learning today is primarily driven by manually developed libraries optimized and highly tuned by expert programmers using lowlevel abstractions with significant effort. This effort is often repeated for similar hardware and future ones. In this work, we pursue and evaluate the more modular and reusable approach of using compiler IR infrastructure to generate libraries by encoding all the required optimizations as a sequence of transformations and customized passes on an IR. We believe that until the recent introduction of MLIR (Multi-level intermediate representation), it had been hard to represent and transform computation at various levels of abstraction within a single IR.},
  isbn = {978-1-4503-9183-2},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/katel2022mlir-based_code_generation_for_gpu_tensor_cores.pdf}
}

@article{kattnig2016electron,
  title = {Electron Spin Relaxation Can Enhance the Performance of a Cryptochrome-Based Magnetic Compass Sensor},
  author = {Kattnig, Daniel R. and Sowa, Jakub K. and Solov'yov, Ilia A. and Hore, P. J.},
  year = {2016},
  month = jun,
  journal = {New J. Phys.},
  volume = {18},
  number = {6},
  pages = {063007},
  publisher = {IOP Publishing},
  issn = {1367-2630},
  doi = {10.1088/1367-2630/18/6/063007},
  urldate = {2022-10-10},
  abstract = {The radical pair model of the avian magnetoreceptor relies on long-lived electron spin coherence. Dephasing, resulting from interactions of the spins with their fluctuating environment, is generally assumed to degrade the sensitivity of this compass to the direction of the Earth's magnetic field. Here we argue that certain spin relaxation mechanisms can enhance its performance. We focus on the flavin--tryptophan radical pair in cryptochrome, currently the only candidate magnetoreceptor molecule. Correlation functions for fluctuations in the distance between the two radicals in Arabidopsis thaliana cryptochrome 1 were obtained from molecular dynamics (MD) simulations and used to calculate the spin relaxation caused by modulation of the exchange and dipolar interactions. We find that intermediate spin relaxation rates afford substantial enhancements in the sensitivity of the reaction yields to an Earth-strength magnetic field. Supported by calculations using toy radical pair models, we argue that these enhancements could be consistent with the molecular dynamics and magnetic interactions in avian cryptochromes.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/kattnig2016electron_spin_relaxation_can_enhance_the_performance_of_a_cryptochrome-based.pdf}
}

@inproceedings{kelly1995unifying,
  title = {A Unifying Framework for Iteration Reordering Transformations},
  booktitle = {Proceedings 1st {{International Conference}} on {{Algorithms}} and {{Architectures}} for {{Parallel Processing}}},
  author = {Kelly, W. and Pugh, W.},
  year = {1995},
  month = apr,
  volume = {1},
  pages = {153-162 vol.1},
  doi = {10.1109/ICAPP.1995.472180},
  abstract = {We present a framework for unifying iteration reordering transformations such as loop interchange, loop distribution, skewing, tiling, index set splitting and statement reordering. The framework is based on the idea that a transformation can be represented as a mapping from the original iteration space to a new iteration space. The framework is designed to provide a uniform way to represent and reason about transformations. We also provide algorithms to test the legality of mappings, and to generate optimized code for mappings.{$<>$}},
  keywords = {Optimizing compilers,Performance analysis,Programming environments},
  file = {/home/vatai/Sync/zotero-data/pdfs/kelly1995a_unifying_framework_for_iteration_reordering_transformations.pdf;/home/vatai/Sync/zotero-data/storage/QEWD2SE8/472180.html}
}

@article{kelly1996optimization,
  title = {Optimization within a {{Unified Transformation Framework}}},
  author = {Kelly, Wayne},
  year = {1996},
  urldate = {2022-05-09},
  abstract = {Programmers typically want to write scientific programs in a high level language with semantics based on a sequential execution model.  To execute efficiently on a parallel machine, however, a program typically needs to contain explicit parallelism and possibly explicit communication and synchronization.  So, we need compilers to convert programs from the first of these forms to the second.  There are two basic choices to be made when parallelizing a program.  First, the computations of the program need to be distributed amongst the set of available  processors.  Second, the computations on each processor need to be ordered.  My contribution has been the development of simple mathematical abstractions for representing these choices and the development of new algorithms for making these choices.  I have developed a new framework that achieves good performance by minimizing communication between processors, minimizing the time processors spend waiting for messages from other processors, and ordering data accesses so as to exploit the memory hierarchy.  This framework can be used by optimizing compilers, as well as by interactive transformation tools.  The state of the art for vectorizing compilers is already quite good, but much work remains to bring parallelizing compilers up to the same standard.  The main contribution of my work can be summarized as improving this situation by replacing existing ad hoc parallelization techniques with a sound underlying foundation on which future work can be built. (Also cross-referenced as UMIACS-TR-96-93)},
  langid = {american},
  annotation = {Accepted: 2004-05-31T22:43:07Z},
  file = {/home/vatai/Sync/zotero-data/pdfs/kelly1998optimization_within_a_unified_transformation_framework.pdf;/home/vatai/Sync/zotero-data/storage/2DHJL5X3/865.html}
}

@inproceedings{keryell2015khronos,
  title = {Khronos {{SYCL}} for {{OpenCL}}: A Tutorial},
  shorttitle = {Khronos {{SYCL}} for {{OpenCL}}},
  booktitle = {Proceedings of the 3rd {{International Workshop}} on {{OpenCL}}},
  author = {Keryell, Ronan and Reyes, Ruyman and Howes, Lee},
  year = {2015},
  month = may,
  series = {{{IWOCL}} '15},
  pages = {1},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2791321.2791345},
  urldate = {2022-07-29},
  abstract = {SYCL ([sikə l] as in sickle) is a royalty-free, cross-platform C++ abstraction layer that builds on the underlying concepts, portability and efficiency of OpenCL, while adding the ease-of-use and flexibility of modern C++11. For example, SYCL enables single source development where C++ template functions can contain both host and device code to construct complex algorithms that use OpenCL acceleration, and then re-use them throughout their source code on different types of data. In this tutorial we will introduce the concepts behind OpenCL SYCL, present an implementation of SYCL targeting OpenCL devices with SPIR based on Clang/LLVM and an open source CPU-only implementation based on C++1z, Boost and OpenMP. Attendees of the last session are encouraged to install the open-source CPU-only implementation of SYCL and code along on laptop/tablet.},
  isbn = {978-1-4503-3484-6},
  file = {/home/vatai/Sync/zotero-data/pdfs/keryell2015khronos_sycl_for_opencl.pdf}
}

@misc{kim2024pagoda,
  title = {{{PaGoDA}}: {{Progressive Growing}} of a {{One-Step Generator}} from a {{Low-Resolution Diffusion Teacher}}},
  shorttitle = {{{PaGoDA}}},
  author = {Kim, Dongjun and Lai, Chieh-Hsin and Liao, Wei-Hsiang and Takida, Yuhta and Murata, Naoki and Uesaka, Toshimitsu and Mitsufuji, Yuki and Ermon, Stefano},
  year = {2024},
  month = may,
  number = {arXiv:2405.14822},
  eprint = {2405.14822},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-06-04},
  abstract = {To accelerate sampling, diffusion models (DMs) are often distilled into generators that directly map noise to data in a single step. In this approach, the resolution of the generator is fundamentally limited by that of the teacher DM. To overcome this limitation, we propose Progressive Growing of Diffusion Autoencoder (PaGoDA), a technique to progressively grow the resolution of the generator beyond that of the original teacher DM. Our key insight is that a pre-trained, low-resolution DM can be used to deterministically encode high-resolution data to a structured latent space by solving the PF-ODE forward in time (data-to-noise), starting from an appropriately down-sampled image. Using this frozen encoder in an autoencoder framework, we train a decoder by progressively growing its resolution. From the nature of progressively growing decoder, PaGoDA avoids re-training teacher/student models when we upsample the student model, making the whole training pipeline much cheaper. In experiments, we used our progressively growing decoder to upsample from the pre-trained model's 642 resolution to generate 5122 samples, achieving 2{\texttimes} faster inference compared to single-step distilled Stable Diffusion like LCM [1]. PaGoDA also achieved state-of-the-art FIDs on ImageNet across all resolutions from 642 to 5122. Additionally, we demonstrated PaGoDA's effectiveness in solving inverse problems and enabling controllable generation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/QuantOm/kim2024pagoda_progressive_growing_of_a_one-step_generator_from_a_low-resolution_diffusion_teacher.pdf}
}

@inproceedings{kodama2020accuracy,
  title = {Accuracy {{Improvement}} of {{Memory System Simulation}} for {{Modern Shared Memory Processor}}},
  booktitle = {Proceedings of the {{International Conference}} on {{High Performance Computing}} in {{Asia-Pacific Region}}},
  author = {Kodama, Yuetsu and Odajima, Tetsuya and Asato, Akira and Sato, Mitsuhisa},
  year = {2020},
  month = jan,
  series = {{{HPCAsia2020}}},
  pages = {142--149},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3368474.3368483},
  urldate = {2022-03-13},
  abstract = {For the purpose of developing applications for supercomputer Fugaku at an early stage, RIKEN has developed a processor simulator. This simulator is based on the general-purpose processor simulator gem5. It does not simulate the actual hardware of a Fugaku processor. However, we believe that sufficient simulation accuracy can be obtained since it simulates the instruction pipeline of out-of-order execution with cycle-level accuracy along with performing detailed parameter tuning of out-of-order resources. In order to estimate the accurate execution time of a program, it is necessary to simulate with accuracy not only the instruction execution time, but also the access time of the cache memory hierarchy. Therefore, in the RIKEN simulator, we expanded gem5 to match the performance of the cache memory hierarchy to that of a Fugaku processor. In this simulator, we aim to estimate the execution cycles of one node application on a Fugaku processor with accuracy that enables relative evaluation and application tuning. In this paper, we show the details of the implementation of this simulator and verify its accuracy compared with that of a Fugaku processor test chip. In the evaluation of the total 46 kernel benchmarks, it was confirmed that the difference is 13\% or less for 85\% of the kernels. In the multithreaded execution of Stream Triad benchmark, scalable performance according to the number of threads was confirmed, and achieved over 80\% of memory throughput with enough accuracy.},
  isbn = {978-1-4503-7236-7},
  file = {/home/vatai/Sync/zotero-data/pdfs/kodama2020accuracy_improvement_of_memory_system_simulation_for_modern_shared_memory.pdf}
}

@article{kong2018performance,
  title = {A {{Performance Vocabulary}} for {{Affine Loop Transformations}}},
  author = {Kong, Martin and Pouchet, Louis-No{\"e}l},
  year = {2018},
  month = nov,
  doi = {10.48550/arXiv.1811.06043},
  urldate = {2022-10-21},
  abstract = {Modern polyhedral compilers excel at aggressively optimizing codes with static control parts, but the state-of-practice to find high-performance polyhedral transformations especially for different hardware targets still largely involves auto-tuning. In this work we propose a novel polyhedral scheduling technique, with the aim to reduce the need for auto-tuning while allowing to build customizable and specific transformation strategies. We design constraints and objectives that model several crucial aspects of performance such as stride optimization or the trade-off between parallelism and reuse, while taking into account important architectural features of the target machine. The developed set of objectives embody a Performance Vocabulary for loop transformations. The goal is to use this vocabulary, consisting of performance idioms, to construct transformation recipes adapted to a number of program classes. We evaluate our work using the PolyBench/C benchmark suite and experimentally validate it against large optimization spaces generated with the Pluto compiler on a 10-core Intel Core-i9 (Skylake-X). Our results show that we can achieve comparable or superior performance to Pluto on the majority of benchmarks, without implementing tiling in the source code nor using experimental autotuning.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/kong_pouchet2018a_performance_vocabulary_for_affine_loop_transformations.pdf;/home/vatai/Sync/zotero-data/storage/2IBN6X7U/1811.html}
}

@misc{kruse2020autotuning,
  title = {Autotuning {{Search Space}} for {{Loop Transformations}}},
  author = {Kruse, Michael and Finkel, Hal and Wu, Xingfu},
  year = {2020},
  month = oct,
  number = {arXiv:2010.06521},
  eprint = {2010.06521},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-10-21},
  abstract = {One of the challenges for optimizing compilers is to predict whether applying an optimization will improve its execution speed. Programmers may override the compiler's profitability heuristic using optimization directives such as pragmas in the source code. Machine learning in the form of autotuning can assist users in finding the best optimizations for each platform.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Distributed Parallel and Cluster Computing},
  note = {Comment: LLVM-in-HPC 2020 preprint},
  file = {/home/vatai/Sync/zotero-data/pdfs/kruse2020autotuning_search_space_for_loop_transformations.pdf}
}

@article{kuhl2022human,
  title = {Human vs. Supervised Machine Learning: {{Who}} Learns Patterns Faster?},
  shorttitle = {Human vs. Supervised Machine Learning},
  author = {K{\"u}hl, Niklas and Goutier, Marc and Baier, Lucas and Wolff, Clemens and Martin, Dominik},
  year = {2022},
  month = dec,
  journal = {Cognitive Systems Research},
  volume = {76},
  pages = {78--92},
  issn = {1389-0417},
  doi = {10.1016/j.cogsys.2022.09.002},
  urldate = {2024-08-30},
  abstract = {The capabilities of supervised machine learning (SML), especially compared to human abilities, are being discussed in scientific research and in the usage of SML. This study provides an answer to how learning performance differs between humans and machines when there is limited training data. We have designed an experiment in which 44 humans and three different machine learning algorithms identify patterns in labeled training data and have to label instances according to the patterns they find. The results show a high dependency between performance and the underlying patterns of the task. Whereas humans perform relatively similarly across all patterns, machines show large performance differences for the various patterns in our experiment. After seeing 20 instances in the experiment, human performance does not improve anymore, which we relate to theories of cognitive overload. Machines learn slower but can reach the same level or may even outperform humans in 2 of the 4 of used patterns. However, machines need more instances compared to humans for the same results. The performance of machines is comparably lower for the other 2 patterns due to the difficulty of combining input features.},
  keywords = {Cognitive psychology,Experimental study,Human learning,Pattern recognition,Small sample size,Supervised machine learning},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/Codegen/needed/kühl2022human_vs._supervised_machine_learning_who_learns_patterns_faster.pdf;/home/vatai/Sync/zotero-data/storage/LYC4BCFT/S1389041722000419.html}
}

@article{kuprov2007blochredfieldwangsness,
  title = {Bloch-{{Redfield-Wangsness}} Theory Engine Implementation Using Symbolic Processing Software},
  author = {Kuprov, Ilya and {Wagner-Rundell}, Nicola and Hore, P. J.},
  year = {2007},
  month = feb,
  journal = {Journal of Magnetic Resonance},
  volume = {184},
  number = {2},
  pages = {196--206},
  issn = {1090-7807},
  doi = {10.1016/j.jmr.2006.09.023},
  urldate = {2022-11-07},
  abstract = {We describe a general method for the automated symbolic processing of Bloch-Redfield-Wangsness relaxation theory equations for liquid-phase spin dynamics in the algebraically challenging case of rotationally modulated interactions. The processing typically takes no more than a few seconds (on a contemporary single-processor workstation) and yields relaxation rate expressions that are completely general with respect to the spectral density functions, relative orientations, and magnitudes of the interaction tensors, with all cross-correlations accounted for. The algorithm easily deals with fully rhombic interaction tensors, and is able, with little if any modification, to treat a large variety of the relaxation mechanisms encountered in NMR, EPR, and spin dynamics in general.},
  langid = {english},
  keywords = {Irreducible spherical tensor,Relaxation theory,Rotation group,Software,Symbolic processing,Wigner function},
  file = {/home/vatai/Sync/zotero-data/pdfs/kuprov2007bloch-redfield-wangsness_theory_engine_implementation_using_symbolic_processing.pdf;/home/vatai/Sync/zotero-data/storage/TWWD5ZDZ/S1090780706003259.html}
}

@article{kuprov2007polynomially,
  title = {Polynomially Scaling Spin Dynamics Simulation Algorithm Based on Adaptive State-Space Restriction},
  author = {Kuprov, Ilya and {Wagner-Rundell}, Nicola and Hore, P. J.},
  year = {2007},
  month = dec,
  journal = {Journal of Magnetic Resonance},
  volume = {189},
  number = {2},
  pages = {241--250},
  issn = {1090-7807},
  doi = {10.1016/j.jmr.2007.09.014},
  urldate = {2022-10-15},
  abstract = {We report progress with an old problem in magnetic resonance---that of the exponential scaling of simulation complexity with the number of spins. It is demonstrated below that a polynomially scaling algorithm can be obtained (and accurate simulations performed for over 200 coupled spins) if the dimension of the Liouville state space is reduced by excluding unimportant and unpopulated spin states. We found the class of such states to be surprisingly wide. It actually appears that a majority of states in large spin systems are not essential in magnetic resonance simulations and can safely be dropped from the state space. In restricted state spaces the spin dynamics simulations scale polynomially. In cases of favourable interaction topologies (sparse graphs, e.g. in protein NMR) the asymptotic scaling is linear, opening the way to direct fitting of molecular structures to experimental spectra.},
  langid = {english},
  keywords = {CompSpinChem,EPR,NMR,Polynomial scaling,Simulation,Spin},
  file = {/home/vatai/Sync/zotero-data/pdfs/kuprov2007polynomially_scaling_spin_dynamics_simulation_algorithm_based_on_adaptive2.pdf;/home/vatai/Sync/zotero-data/storage/YGYQFEWW/S109078070700273X.html}
}

@article{kuprov2008polynomially,
  title = {Polynomially Scaling Spin Dynamics {{II}}: {{Further}} State-Space Compression Using {{Krylov}} Subspace Techniques and Zero Track Elimination},
  shorttitle = {Polynomially Scaling Spin Dynamics {{II}}},
  author = {Kuprov, Ilya},
  year = {2008},
  month = nov,
  journal = {Journal of Magnetic Resonance},
  volume = {195},
  number = {1},
  pages = {45--51},
  issn = {1090-7807},
  doi = {10.1016/j.jmr.2008.08.008},
  urldate = {2022-10-15},
  abstract = {We extend the recently proposed state-space restriction (SSR) technique for quantum spin dynamics simulations [Kuprov et al., J. Magn. Reson. 189 (2007) 241--250] to include on-the-fly detection and elimination of unpopulated dimensions from the system density matrix. Further improvements in spin dynamics simulation speed, frequently by several orders of magnitude, are demonstrated. The proposed zero track elimination (ZTE) procedure is computationally inexpensive, reversible, numerically stable and easy to add to any existing simulation code. We demonstrate that it belongs to the same family of Krylov subspace techniques as the well-known Lanczos basis pruning procedure. The combined SSR+ZTE algorithm is recommended for simulations of NMR, EPR and Spin Chemistry experiments on systems containing between 10 and 104 coupled spins.},
  langid = {english},
  keywords = {CompSpinChem,EPR,NMR,Scaling,Simulation,Spin},
  file = {/home/vatai/Sync/zotero-data/pdfs/kuprov2008polynomially_scaling_spin_dynamics_ii.pdf;/home/vatai/Sync/zotero-data/storage/JPTRVRXP/S1090780708002668.html}
}

@article{kuprov2016fokkerplanck,
  title = {Fokker-{{Planck}} Formalism in Magnetic Resonance Simulations},
  author = {Kuprov, Ilya},
  year = {2016},
  month = sep,
  journal = {Journal of Magnetic Resonance},
  volume = {270},
  pages = {124--135},
  issn = {1090-7807},
  doi = {10.1016/j.jmr.2016.07.005},
  urldate = {2022-11-09},
  abstract = {This paper presents an overview of the Fokker-Planck formalism for non-biological magnetic resonance simulations, describes its existing applications and proposes some novel ones. The most attractive feature of Fokker-Planck theory compared to the commonly used Liouville - von Neumann equation is that, for all relevant types of spatial dynamics (spinning, diffusion, stationary flow, etc.), the corresponding Fokker-Planck Hamiltonian is time-independent. Many difficult NMR, EPR and MRI simulation problems (multiple rotation NMR, ultrafast NMR, gradient-based zero-quantum filters, diffusion and flow NMR, off-resonance soft microwave pulses in EPR, spin-spin coupling effects in MRI, etc.) are simplified significantly in Fokker-Planck space. The paper also summarises the author's experiences with writing and using the corresponding modules of the Spinach library -- the methods described below have enabled a large variety of simulations previously considered too complicated for routine practical use.},
  langid = {english},
  keywords = {Magnetic resonance,Simulation,Spin dynamics},
  file = {/home/vatai/Sync/zotero-data/pdfs/kuprov2016fokker-planck_formalism_in_magnetic_resonance_simulations.pdf;/home/vatai/Sync/zotero-data/storage/F658M3FJ/S1090780716301045.html}
}

@article{kuprov2019defeating,
  title = {Defeating the {{Matrix}}},
  author = {Kuprov, Ilya},
  year = {2019},
  month = sep,
  journal = {Journal of Magnetic Resonance},
  volume = {306},
  pages = {75--79},
  issn = {1090-7807},
  doi = {10.1016/j.jmr.2019.07.031},
  urldate = {2022-04-27},
  abstract = {These are personal recollections and musings, written for the 50th Anniversary of the Journal of Magnetic Resonance. They are distilled from twenty years of writing simulation code, and filtered through hindsight and sarcastic intransigence. To me, the biggest recent achievements of the magnetic resonance community in the field of theory and simulation have been the successful war on the exponential scaling, the powerful and general simulation software, and the return to elegant notation. It appears that our future will be defined by computers. Three aspects are pertinent: simulation as the experiment is designed, optimal control as the experiment proceeds, and machine learning at the data processing stage.},
  langid = {english},
  keywords = {CompSpinChem,Magnetic resonance,Simulation,Theory},
  file = {/home/vatai/Sync/zotero-data/pdfs/kuprov2019defeating_the_matrix2.pdf;/home/vatai/Sync/zotero-data/storage/WHAZNDSL/S1090780719301600.html}
}

@inproceedings{lattner2021mlira,
  title = {{{MLIR}}: {{Scaling Compiler Infrastructure}} for {{Domain Specific Computation}}},
  shorttitle = {{{MLIR}}},
  booktitle = {2021 {{IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}} ({{CGO}})},
  author = {Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  year = {2021},
  month = feb,
  pages = {2--14},
  doi = {10.1109/CGO51591.2021.9370308},
  abstract = {This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR addresses software fragmentation, compilation for heterogeneous hardware, significantly reducing the cost of building domain specific compilers, and connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, while identifying the challenges and opportunities posed by this novel design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.},
  keywords = {Buildings,Generators,Hardware,Optimization,Program processors,Semantics,Software},
  file = {/home/vatai/Sync/zotero-data/pdfs/lattner2021mlir.pdf;/home/vatai/Sync/zotero-data/storage/QPPIMSQT/9370308.html}
}

@inproceedings{leather2020machine,
  title = {Machine {{Learning}} in {{Compilers}}: {{Past}}, {{Present}} and {{Future}}},
  shorttitle = {Machine {{Learning}} in {{Compilers}}},
  booktitle = {2020 {{Forum}} for {{Specification}} and {{Design Languages}} ({{FDL}})},
  author = {Leather, Hugh and Cummins, Chris},
  year = {2020},
  month = sep,
  pages = {1--8},
  issn = {1636-9874},
  doi = {10.1109/FDL50818.2020.9232934},
  abstract = {Writing optimising compilers is difficult. The range of programs that may be presented to the compiler is huge and the systems on which they run are complex, heterogeneous, non-deterministic, and constantly changing. The space of possible optimisations is also vast, making it very hard for compiler writers to design heuristics that take all of these considerations into account. As a result, many compiler optimisations are out of date or poorly tuned. Near the turn of the century it was first shown how compilers could be made to automatically search the optimisation space, producing programs far better optimised than previously possible, and without the need for compiler writers to worry about architecture or program specifics. The searches, though, were slow, so in the years that followed, machine learning was developed to learn heuristics from the results of previous searches so that thereafter the search could be avoided and much of the benefit could be gained in a single shot. In this paper we will give a retrospective of machine learning in compiler optimisation from its earliest inception, through some of the works that set themselves apart, to today's deep learning, finishing with our vision of the field's future.},
  keywords = {compilers,Deep learning,History,machine learning,Neural networks,Optimization,Predictive models,Search problems,Tools},
  file = {/home/vatai/Sync/zotero-data/pdfs/leather_cummins2020machine_learning_in_compilers.pdf;/home/vatai/Sync/zotero-data/storage/QWCA3QJT/9232934.html}
}

@article{lehoucq1994deflation,
  title = {Deflation {{Techniques}} for an {{Implicitly Restarted Arnoldi Iteration}}},
  author = {Lehoucq, R. B. and Sorensen, Danny C.},
  year = {1994},
  month = sep,
  urldate = {2023-08-08},
  abstract = {A deflation procedure is introduced that is designed to improve convergence of an implicitly restarted Arnoldi iteration for computing a few eigenvalues of a large matrix. As the iteration progresses the Ritz value approximations of the eigenvalues of~A~converge at different rates. A numerically stable deflation scheme is introduced that implicitly deflates the converged approximations from the iteration. We present two forms of implicit deflation. The first, a~locking~operation, decouples converged Ritz values and associated vectors from the active part of the iteration. The second, a~purgingoperation, removes unwanted but converged Ritz pairs. Convergence of the iteration is improved and a reduction in computational effort is also achieved. The deflation strategies make it possible to compute multiple or clustered eigenvalues with a single vector restart method. A Block method is not required. These schemes are analyzed with respect to numerical stability and computational results are presented.},
  langid = {english},
  annotation = {Accepted: 2018-06-18T17:41:50Z},
  file = {/home/vatai/Sync/zotero-data/pdfs/lehoucq_sorensen1994deflation_techniques_for_an_implicitly_restarted_arnoldi_iteration.pdf}
}

@inproceedings{lewis2020retrievalaugmented,
  title = {Retrieval-Augmented Generation for Knowledge-Intensive {{NLP}} Tasks},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  year = {2020},
  month = dec,
  series = {{{NIPS}} '20},
  pages = {9459--9474},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2024-08-29},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) --- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  isbn = {978-1-71382-954-6},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/Codegen/needed/lewis2020retrieval-augmented_generation_for_knowledge-intensive_nlp_tasks.pdf}
}

@article{lezon1989statistical,
  title = {Statistical {{Mechanics}} of {{Chain Molecules}}: {{An Overview}}},
  author = {Lezon, Tim},
  year = {1989},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/storage/5JVJBEIM/Lezon - Statistical Mechanics of Chain Molecules An Overv.pdf}
}

@inproceedings{li2014understanding,
  title = {Understanding the Tradeoffs between Software-Managed vs. Hardware-Managed Caches in {{GPUs}}},
  booktitle = {2014 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}} ({{ISPASS}})},
  author = {Li, Chao and Yang, Yi and Dai, Hongwen and Yan, Shengen and Mueller, Frank and Zhou, Huiyang},
  year = {2014},
  month = mar,
  pages = {231--242},
  doi = {10.1109/ISPASS.2014.6844487},
  abstract = {On-chip caches are commonly used in computer systems to hide long off-chip memory access latencies. To manage on-chip caches, either software-managed or hardware-managed schemes can be employed. State-of-art accelerators, such as the NVIDIA Fermi or Kepler GPUs and Intel's forthcoming MIC ``Knights Landing'' (KNL), support both software-managed caches, aka. shared memory (GPUs) or near memory (KNL), and hardware-managed L1 data caches (D-caches). Furthermore, shared memory and the L1 D-cache on a GPU utilize the same physical storage and their capacity can be configured at runtime (same for KNL). In this paper, we present an in-depth study to reveal interesting and sometimes unexpected tradeoffs between shared memory and the hardware-managed L1 D- caches in GPU architecture. In our study, the kernels utilizing the L1 D-caches are generated from those leveraging shared memory to ensure that the same optimizations such as tiling are applied equally in both versions. Our detailed analyses reveal that rather than cache hit rates, the following tradeoffs often have more profound performance impacts. On one hand, the kernels utilizing the L1 caches may support higher degrees of thread-level parallelism, offer more opportunities for data to be allocated in registers, and sometimes result in lower dynamic instruction counts. On the other hand, the applications utilizing shared memory enable more coalesced accesses and tend to achieve higher degrees of memory-level parallelism. Overall, our results show that most benchmarks perform significantly better with shared memory than the L1 D-caches due to the high impact of memory-level parallelism and memory coalescing.},
  keywords = {Computer architecture,Graphics processing units,Kernel,Parallel processing,Prefetching,Tiles},
  file = {/home/vatai/Sync/zotero-data/pdfs/li2014understanding_the_tradeoffs_between_software-managed_vs.pdf;/home/vatai/Sync/zotero-data/storage/RHFG5DB5/6844487.html}
}

@misc{li2022competitionlevel,
  title = {Competition-{{Level Code Generation}} with {{AlphaCode}}},
  author = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and Hubert, Thomas and Choy, Peter and {d'Autume}, Cyprien de Masson and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Robson, Esme Sutherland and Kohli, Pushmeet and {de Freitas}, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
  year = {2022},
  month = feb,
  number = {arXiv:2203.07814},
  eprint = {2203.07814},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.07814},
  urldate = {2022-08-04},
  abstract = {Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3\% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages},
  note = {Comment: 74 pages},
  file = {/home/vatai/Sync/zotero-data/pdfs/li2022competition-level_code_generation_with_alphacode.pdf;/home/vatai/Sync/zotero-data/storage/BPEJR7AX/2203.html}
}

@inproceedings{li2022efficient,
  title = {Efficient Quantized Sparse Matrix Operations on Tensor Cores},
  booktitle = {Proceedings of the {{International Conference}} on {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Li, Shigang and Osawa, Kazuki and Hoefler, Torsten},
  year = {2022},
  month = nov,
  series = {{{SC}} '22},
  pages = {1--15},
  publisher = {IEEE Press},
  address = {Dallas, Texas},
  urldate = {2024-11-10},
  abstract = {The exponentially growing model size drives the continued success of deep learning, but it brings prohibitive computation and memory cost. From the algorithm perspective, model sparsification and quantization have been studied to alleviate the problem. From the architecture perspective, hardware vendors provide Tensor cores for acceleration. However, it is very challenging to gain practical speedups from sparse, low-precision matrix operations on Tensor cores, because of the strict requirements for data layout and lack of support for efficiently manipulating the low-precision integers. We propose Magicube, a high-performance sparse-matrix library for low-precision integers on Tensor cores. Magicube supports SpMM and SDDMM, two major sparse operations in deep learning with mixed precision. Experimental results on an NVIDIA A100 GPU show that Magicube achieves on average 1.44x (up to 2.37x) speedup over the vendor-optimized library for sparse kernels, and 1.43x speedup over the state-of-the-art with a comparable accuracy for end-to-end sparse Transformer inference.},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/dl-sparsity/Li et al. - 2022 - Efficient quantized sparse matrix operations on tensor cores 1.pdf}
}

@inproceedings{lialin2022life,
  title = {Life after {{BERT}}: {{What}} Do {{Other Muppets Understand}} about {{Language}}?},
  shorttitle = {Life after {{BERT}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lialin, Vladislav and Zhao, Kevin and Shivagunde, Namrata and Rumshisky, Anna},
  year = {2022},
  month = may,
  pages = {3180--3193},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.227},
  urldate = {2023-05-12},
  abstract = {Existing pre-trained transformer analysis works usually focus only on one or two model families at a time, overlooking the variability of the architecture and pre-training objectives. In our work, we utilize the oLMpics bench- mark and psycholinguistic probing datasets for a diverse set of 29 models including T5, BART, and ALBERT. Additionally, we adapt the oLMpics zero-shot setup for autoregres- sive models and evaluate GPT networks of different sizes. Our findings show that none of these models can resolve compositional questions in a zero-shot fashion, suggesting that this skill is not learnable using existing pre-training objectives. Furthermore, we find that global model decisions such as architecture, directionality, size of the dataset, and pre-training objective are not predictive of a model's linguistic capabilities.},
  file = {/home/vatai/Sync/zotero-data/pdfs/lialin2022life_after_bert.pdf}
}

@article{limited2022a64fx,
  title = {{{A64FX Microarchitecture Manual}}},
  author = {Limited, Fujitsu},
  year = {2022},
  pages = {136},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/limiteda64fx_microarchitecture_manual.pdf}
}

@inproceedings{lin2008massively,
  title = {Massively Parallel Genomic Sequence Search on the {{Blue Gene}}/{{P}} Architecture},
  booktitle = {2008 {{SC}} - {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Lin, Heshan and Balaji, Pavan and Poole, Ruth and Sosa, Carlos and Ma, Xiaosong and Feng, Wu-chun},
  year = {2008},
  month = nov,
  pages = {1--11},
  publisher = {IEEE},
  address = {Austin, TX, USA},
  doi = {10.1109/SC.2008.5222005},
  urldate = {2023-02-28},
  abstract = {This paper presents our first experiences in mapping and optimizing genomic sequence search onto the massively parallel IBM Blue Gene/P (BG/P) platform. Specifically, we performed our work on mpiBLAST, a parallel sequence-search code that has been optimized on numerous supercomputing environments. In doing so, we identify several critical performance issues. Consequently, we propose and study different approaches for mapping sequence-search and parallel I/O tasks on such massively parallel architectures. We demonstrate that our optimizations can deliver nearly linear scaling (93\% efficiency) on up to 32,768 cores of BG/P. In addition, we show that such scalability enables us to complete a large-scale bioinformatics problem --- sequence searching a microbial genome database against itself to support the discovery of missing genes in genomes --- in only a few hours on BG/P. Previously, this problem was viewed as computationally intractable in practice.},
  isbn = {978-1-4244-2834-2},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/lin2008massively_parallel_genomic_sequence_search_on_the_blue_gene-p_architecture.pdf}
}

@article{lin2011coordinating,
  title = {Coordinating {{Computation}} and {{I}}/{{O}} in {{Massively Parallel Sequence Search}}},
  author = {Lin, Heshan and Ma, Xiaosong and Feng, Wuchun and Samatova, Nagiza F.},
  year = {2011},
  month = apr,
  journal = {IEEE Trans. Parallel Distrib. Syst.},
  volume = {22},
  number = {4},
  pages = {529--543},
  issn = {1045-9219},
  doi = {10.1109/TPDS.2010.101},
  urldate = {2024-10-21},
  abstract = {With the explosive growth of genomic information, the searching of sequence databases has emerged as one of the most computation- and data-intensive scientific applications. Our previous studies suggested that parallel genomic sequence-search possesses highly irregular computation and I/O patterns. Effectively addressing these run-time irregularities is thus the key to designing scalable sequence-search tools on massively parallel computers. While the computation scheduling for irregular scientific applications and the optimization of noncontiguous file accesses have been well studied independently, little attention has been paid to the interplay between the two. In this paper, we systematically investigate the computation and I/O scheduling for data-intensive, irregular scientific applications within the context of genomic sequence search. Our study reveals that the lack of coordination between computation scheduling and I/O optimization could result in severe performance issues. We then propose an integrated scheduling approach that effectively improves sequence-search throughput by gracefully coordinating the dynamic load-balancing of computation and highperformance noncontiguous I/O.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/SparkLeBlast/Lin et al. - 2011 - Coordinating Computation and IO in Massively Parallel Sequence Search.pdf}
}

@misc{liu2022bayesian,
  title = {Bayesian {{Stochastic Gradient Descent}} for {{Stochastic Optimization}} with {{Streaming Input Data}}},
  author = {Liu, Tianyi and Lin, Yifan and Zhou, Enlu},
  year = {2022},
  month = feb,
  number = {arXiv:2202.07581},
  eprint = {2202.07581},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.07581},
  urldate = {2023-03-22},
  abstract = {We consider stochastic optimization under distributional uncertainty, where the unknown distributional parameter is estimated from streaming data that arrive sequentially over time. Moreover, data may depend on the decision of the time when they are generated. For both decision-independent and decision-dependent uncertainties, we propose an approach to jointly estimate the distributional parameter via Bayesian posterior distribution and update the decision by applying stochastic gradient descent on the Bayesian average of the objective function. Our approach converges asymptotically over time and achieves the convergence rates of classical SGD in the decision-independent case. We demonstrate the empirical performance of our approach on both synthetic test problems and a classical newsvendor problem.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control},
  file = {/home/vatai/Sync/zotero-data/pdfs/liu2022bayesian_stochastic_gradient_descent_for_stochastic_optimization_with_streaming.pdf;/home/vatai/Sync/zotero-data/storage/VZVMDBTW/2202.html}
}

@misc{liu2023sparse,
  title = {Sparse {{Bayesian Optimization}}},
  author = {Liu, Sulin and Feng, Qing and Eriksson, David and Letham, Benjamin and Bakshy, Eytan},
  year = {2023},
  month = mar,
  number = {arXiv:2203.01900},
  eprint = {2203.01900},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.01900},
  urldate = {2023-03-15},
  abstract = {Bayesian optimization (BO) is a powerful approach to sample-efficient optimization of black-box objective functions. However, the application of BO to areas such as recommendation systems often requires taking the interpretability and simplicity of the configurations into consideration, a setting that has not been previously studied in the BO literature. To make BO useful for this setting, we present several regularization-based approaches that allow us to discover sparse and more interpretable configurations. We propose a novel differentiable relaxation based on homotopy continuation that makes it possible to target sparsity by working directly with \$L\_0\$ regularization. We identify failure modes for regularized BO and develop a hyperparameter-free method, sparsity exploring Bayesian optimization (SEBO) that seeks to simultaneously maximize a target objective and sparsity. SEBO and methods based on fixed regularization are evaluated on synthetic and real-world problems, and we show that we are able to efficiently optimize for sparsity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/vatai/Sync/zotero-data/pdfs/liu2023sparse_bayesian_optimization.pdf;/home/vatai/Sync/zotero-data/storage/7C8I38AI/2203.html}
}

@misc{liu2023summary,
  title = {Summary of {{ChatGPT}}/{{GPT-4 Research}} and {{Perspective Towards}} the {{Future}} of {{Large Language Models}}},
  author = {Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and Wu, Zihao and Zhu, Dajiang and Li, Xiang and Qiang, Ning and Shen, Dingang and Liu, Tianming and Ge, Bao},
  year = {2023},
  month = apr,
  number = {arXiv:2304.01852},
  eprint = {2304.01852},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-04-07},
  abstract = {This paper presents a comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT/GPT-4 research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabilities, potential implications, ethical concerns, and offer direction for future advancements in this field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/vatai/Sync/zotero-data/storage/NRQ4SI5J/Liu et al. - 2023 - Summary of ChatGPTGPT-4 Research and Perspective .pdf}
}

@misc{liu2024think,
  title = {Think {{While You Generate}}: {{Discrete Diffusion}} with {{Planned Denoising}}},
  shorttitle = {Think {{While You Generate}}},
  author = {Liu, Sulin and Nam, Juno and Campbell, Andrew and St{\"a}rk, Hannes and Xu, Yilun and Jaakkola, Tommi and {G{\'o}mez-Bombarelli}, Rafael},
  year = {2024},
  month = oct,
  number = {arXiv:2410.06264},
  eprint = {2410.06264},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-18},
  abstract = {Discrete diffusion has achieved state-of-the-art performance, outperforming or approaching autoregressive models on standard benchmarks. In this work, we introduce Discrete Diffusion with Planned Denoising (DDPD), a novel framework that separates the generation process into two models: a planner and a denoiser. At inference time, the planner selects which positions to denoise next by identifying the most corrupted positions in need of denoising, including both initially corrupted and those requiring additional refinement. This plan-and-denoise approach enables more efficient reconstruction during generation by iteratively identifying and denoising corruptions in the optimal order. DDPD outperforms traditional denoiser-only mask diffusion methods, achieving superior results on language modeling benchmarks such as text8, OpenWebText, and token-based generation on ImageNet 256 {\texttimes} 256. Notably, in language modeling, DDPD significantly reduces the performance gap between diffusion-based and autoregressive methods in terms of generative perplexity. Code is available at github.com/liusulin/DDPD.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/vatai/Sync/zotero-data/storage/E7EDW88C/Liu et al. - 2024 - Think While You Generate Discrete Diffusion with Planned Denoising.pdf}
}

@article{madsen2023posthoc,
  title = {Post-Hoc {{Interpretability}} for {{Neural NLP}}: {{A Survey}}},
  shorttitle = {Post-Hoc {{Interpretability}} for {{Neural NLP}}},
  author = {Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
  year = {2023},
  month = aug,
  journal = {ACM Comput. Surv.},
  volume = {55},
  number = {8},
  pages = {1--42},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3546577},
  urldate = {2023-03-01},
  abstract = {Neural networks for NLP are becoming increasingly complex and widespread, and there is a growing concern if these models are responsible to use. Explaining models helps to address the safety and ethical concerns and is essential for accountability. Interpretability serves to provide these explanations in terms that are understandable to humans. Additionally, post-hoc methods provide explanations after a model is learned and are generally model-agnostic. This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, it discusses each method in-depth, and how they are validated, as the latter is often a common concern.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/madsen2023post-hoc_interpretability_for_neural_nlp.pdf}
}

@article{maeda2006practical,
  title = {A Practical Simulation and a Novel Insight to the Magnetic Field Effect on a Radical Pair in a Micelle},
  author = {Maeda, K. and Miura, T. and Arai, T.},
  year = {2006},
  month = may,
  journal = {Molecular Physics},
  volume = {104},
  number = {10-11},
  pages = {1779--1788},
  publisher = {Taylor \& Francis},
  issn = {0026-8976},
  doi = {10.1080/14767050600588106},
  urldate = {2024-03-10},
  abstract = {The spin mixing process of a radical pair in a micelle in a low magnetic field is investigated by transient absorption detected MARY spectra. In a system comprising the radical pair generated by photoreaction of 2-methyl-1,4-naphthoquinone in sodium dodecylsulfate micelle, two techniques were applied: time resolved MARY spectra and pulsed MARY spectra. In order to analyse the results, a practical calculation method that lends itself to the simulation of experimental results has been developed. In the calculation, a semi-classical model was used for the evaluation of the hyperfine mechanism (HFM) in the single-site Liouville equation in which one can take spin relaxation and dephasing phenomena into account. The interrelation between the HFM and relaxation mechanism (RM) was discussed. A new insight into the HFM induced spin mixing process, which is assisted by fast spin dephasing phenomena, is proposed. This mechanism is important for the MARY spectral shape and connects the HFM and RM smoothly.},
  file = {/home/vatai/Sync/zotero-data/pdfs/maeda2006a_practical_simulation_and_a_novel_insight_to_the_magnetic_field_effect_on_a.pdf}
}

@article{maeda2012magnetically,
  title = {Magnetically Sensitive Light-Induced Reactions in Cryptochrome Are Consistent with Its Proposed Role as a Magnetoreceptor},
  author = {Maeda, Kiminori and Robinson, Alexander J. and Henbest, Kevin B. and Hogben, Hannah J. and Biskup, Till and Ahmad, Margaret and Schleicher, Erik and Weber, Stefan and Timmel, Christiane R. and Hore, P. J.},
  year = {2012},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {109},
  number = {13},
  pages = {4774--4779},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1118959109},
  urldate = {2022-07-29},
  file = {/home/vatai/Sync/zotero-data/pdfs/maeda2012magnetically_sensitive_light-induced_reactions_in_cryptochrome_are_consistent.pdf}
}

@article{maeda2013spinselective,
  title = {Spin-Selective Recombination Reactions of Radical Pairs: {{Experimental}} Test of Validity of Reaction Operators},
  shorttitle = {Spin-Selective Recombination Reactions of Radical Pairs},
  author = {Maeda, Kiminori and Liddell, Paul and Gust, Devens and Hore, P. J.},
  year = {2013},
  month = dec,
  journal = {J. Chem. Phys.},
  volume = {139},
  number = {23},
  pages = {234309},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.4844355},
  urldate = {2022-10-08},
  abstract = {Spin-selective reactions of radical pairs are conventionally modelled using an approach that dates back to the 1970s [R. Haberkorn, Mol. Phys. 32, 1491 (1976)]. An alternative approach based on the theory of quantum measurements has recently been suggested [J. A. Jones and P. J. Hore, Chem. Phys. Lett. 488, 90 (2010)]. We present here the first experimental attempt to discriminate between the two models. Pulsed electron paramagnetic resonance spectroscopy has been used to investigate intramolecular electron transfer in the radical pair form of a carotenoid-porphyrin-fullerene molecular triad. The rate of spin-spin relaxation of the fullerene radical in the triad was found to be inconsistent with the quantum measurement description of the spin-selective kinetics, and in accord with the conventional model when combined with spin-dephasing caused by rotational modulation of the anisotropic g-tensor of the fullerene radical.},
  file = {/home/vatai/Sync/zotero-data/pdfs/maeda2013spin-selective_recombination_reactions_of_radical_pairs.pdf}
}

@misc{mandt2018stochastic,
  title = {Stochastic {{Gradient Descent}} as {{Approximate Bayesian Inference}}},
  author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
  year = {2018},
  month = jan,
  number = {arXiv:1704.04289},
  eprint = {1704.04289},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1704.04289},
  urldate = {2023-03-22},
  abstract = {Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also propose SGD with momentum for sampling and show how to adjust the damping coefficient accordingly. (4) We analyze MCMC algorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: 35 pages, published version (JMLR 2017)},
  file = {/home/vatai/Sync/zotero-data/pdfs/mandt2018stochastic_gradient_descent_as_approximate_bayesian_inference.pdf;/home/vatai/Sync/zotero-data/storage/E2HSAC6N/1704.html}
}

@article{mani2022molecular,
  title = {Molecular Qubits Based on Photogenerated Spin-Correlated Radical Pairs for Quantum Sensing},
  author = {Mani, Tomoyasu},
  year = {2022},
  month = jun,
  journal = {Chem. Phys. Rev.},
  volume = {3},
  number = {2},
  pages = {021301},
  publisher = {American Institute of Physics},
  doi = {10.1063/5.0084072},
  urldate = {2022-12-16}
}

@article{mankowitz2023faster,
  title = {Faster Sorting Algorithms Discovered Using Deep Reinforcement Learning},
  author = {Mankowitz, Daniel J. and Michi, Andrea and Zhernov, Anton and Gelmi, Marco and Selvi, Marco and Paduraru, Cosmin and Leurent, Edouard and Iqbal, Shariq and Lespiau, Jean-Baptiste and Ahern, Alex and K{\"o}ppe, Thomas and Millikin, Kevin and Gaffney, Stephen and Elster, Sophie and Broshear, Jackson and Gamble, Chris and Milan, Kieran and Tung, Robert and Hwang, Minjae and Cemgil, Taylan and Barekatain, Mohammadamin and Li, Yujia and Mandhane, Amol and Hubert, Thomas and Schrittwieser, Julian and Hassabis, Demis and Kohli, Pushmeet and Riedmiller, Martin and Vinyals, Oriol and Silver, David},
  year = {2023},
  month = jun,
  journal = {Nature},
  volume = {618},
  number = {7964},
  pages = {257--263},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06004-9},
  urldate = {2023-06-14},
  abstract = {Fundamental algorithms such as sorting or hashing are used trillions of times on any given day1. As demand for computation grows, it has become critical for these algorithms to be as performant as possible. Whereas remarkable progress has been achieved in the past2, making further improvements on the efficiency of these routines has proved challenging for both human scientists and computational approaches. Here we show how artificial intelligence can go beyond the current state of the art by discovering hitherto unknown routines. To realize this, we formulated the task of finding a better sorting routine as a single-player game. We then trained a new deep reinforcement learning agent, AlphaDev, to play this game. AlphaDev discovered small sorting algorithms from scratch that outperformed previously known human benchmarks. These algorithms have been integrated into the LLVM standard C++ sort library3. This change to this part of the sort library represents the replacement of a component with an algorithm that has been automatically discovered using reinforcement learning. We also present results in extra domains, showcasing the generality of the approach.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computer science,Software},
  file = {/home/vatai/Sync/zotero-data/pdfs/mankowitz2023faster_sorting_algorithms_discovered_using_deep_reinforcement_learning.pdf}
}

@article{manolopoulos2013improved,
  title = {An Improved Semiclassical Theory of Radical Pair Recombination Reactions},
  author = {Manolopoulos, D. E. and Hore, P. J.},
  year = {2013},
  month = sep,
  journal = {The Journal of Chemical Physics},
  volume = {139},
  number = {12},
  pages = {124106},
  issn = {0021-9606},
  doi = {10.1063/1.4821817},
  urldate = {2024-02-13},
  abstract = {We present a practical semiclassical method for computing the electron spin dynamics of a radical in which the electron spin is hyperfine coupled to a large number of nuclear spins. This can be used to calculate the singlet and triplet survival probabilities and quantum yields of radical recombination reactions in the presence of magnetic fields. Our method differs from the early semiclassical theory of Schulten and Wolynes [J. Chem. Phys.\hphantom{,}68, 3292 (1978)] in allowing each individual nuclear spin to precess around the electron spin, rather than assuming that the hyperfine coupling-weighted sum of nuclear spin vectors is fixed in space. The downside of removing this assumption is that one can no longer obtain a simple closed-form expression for the electron spin correlation tensor: our method requires a numerical calculation. However, the computational effort increases only linearly with the number of nuclear spins, rather than exponentially as in an exact quantum mechanical calculation. The method is therefore applicable to arbitrarily large radicals. Moreover, it approaches quantitative agreement with quantum mechanics as the number of nuclear spins increases and the environment of the electron spin becomes more complex, owing to the rapid quantum decoherence in complex systems. Unlike the Schulten-Wolynes theory, the present semiclassical theory predicts the correct long-time behaviour of the electron spin correlation tensor, and it therefore correctly captures the low magnetic field effect in the singlet yield of a radical recombination reaction with a slow recombination rate.},
  file = {/home/vatai/Sync/zotero-data/pdfs/manolopoulos_hore2013an_improved_semiclassical_theory_of_radical_pair_recombination_reactions.pdf;/home/vatai/Sync/zotero-data/storage/ZT44Y5L2/An-improved-semiclassical-theory-of-radical-pair.html}
}

@inproceedings{matsumura2020an5d,
  title = {{{AN5D}}: Automated Stencil Framework for High-Degree Temporal Blocking on {{GPUs}}},
  shorttitle = {{{AN5D}}},
  booktitle = {Proceedings of the 18th {{ACM}}/{{IEEE International Symposium}} on {{Code Generation}} and {{Optimization}}},
  author = {Matsumura, Kazuaki and Zohouri, Hamid Reza and Wahib, Mohamed and Endo, Toshio and Matsuoka, Satoshi},
  year = {2020},
  month = feb,
  series = {{{CGO}} 2020},
  pages = {199--211},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3368826.3377904},
  urldate = {2022-10-10},
  abstract = {Stencil computation is one of the most widely-used compute patterns in high performance computing applications. Spatial and temporal blocking have been proposed to overcome the memory-bound nature of this type of computation by moving memory pressure from external memory to on-chip memory on GPUs. However, correctly implementing those optimizations while considering the complexity of the architecture and memory hierarchy of GPUs to achieve high performance is difficult. We propose AN5D, an automated stencil framework which is capable of automatically transforming and optimizing stencil patterns in a given C source code, and generating corresponding CUDA code. Parameter tuning in our framework is guided by our performance model. Our novel optimization strategy reduces shared memory and register pressure in comparison to existing implementations, allowing performance scaling up to a temporal blocking degree of 10. We achieve the highest performance reported so far for all evaluated stencil benchmarks on the state-of-the-art Tesla V100 GPU.},
  isbn = {978-1-4503-7047-9},
  keywords = {Automatic Code Generation,GPU,Stencil Computation,Temporal Blocking},
  file = {/home/vatai/Sync/zotero-data/pdfs/matsumura2020an5d.pdf}
}

@misc{matsuoka2022preparing,
  title = {Preparing for the {{Future}} -- {{Rethinking Proxy Apps}}},
  author = {Matsuoka, Satoshi and Domke, Jens and Wahib, Mohamed and Drozd, Aleksandr and Bair, Ray and Chien, Andrew A. and Vetter, Jeffrey S. and Shalf, John},
  year = {2022},
  month = apr,
  number = {arXiv:2204.07336},
  eprint = {2204.07336},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-10-17},
  abstract = {A considerable amount of research and engineering went into designing proxy applications, which represent common high-performance computing workloads, to co-design and evaluate the current generation of supercomputers, e.g., RIKEN's Supercomputer Fugaku, ANL's Aurora, or ORNL's Frontier. This process was necessary to standardize the procurement while avoiding duplicated effort at each HPC center to develop their own benchmarks. Unfortunately, proxy applications force HPC centers and providers (vendors) into a an undesirable state of rigidity, in contrast to the fast-moving trends of current technology and future heterogeneity. To accommodate an extremelyheterogeneous future, we have to reconsider how to co-design supercomputers during the next decade, and avoid repeating the past mistakes. This position paper outlines the current state-of-the-art in system co-design, challenges encountered over the past years, and a proposed plan to move forward.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing},
  file = {/home/vatai/Sync/zotero-data/pdfs/matsuoka2022preparing_for_the_future_--_rethinking_proxy_apps.pdf}
}

@misc{mccoy2020berts,
  title = {{{BERTs}} of a Feather Do Not Generalize Together: {{Large}} Variability in Generalization across Models with Similar Test Set Performance},
  shorttitle = {{{BERTs}} of a Feather Do Not Generalize Together},
  author = {McCoy, R. Thomas and Min, Junghyun and Linzen, Tal},
  year = {2020},
  month = nov,
  number = {arXiv:1911.02969},
  eprint = {1911.02969},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.02969},
  urldate = {2023-05-12},
  abstract = {If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6\% and 84.8\%. In stark contrast, the same models varied widely in their generalization performance. For example, on the simple case of subject-object swap (e.g., determining that "the doctor visited the lawyer" does not entail "the lawyer visited the doctor"), accuracy ranged from 0.00\% to 66.2\%. Such variation is likely due to the presence of many local minima that are equally attractive to a low-bias learner such as a neural network; decreasing the variability may therefore require models with stronger inductive biases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: 11 pages, 7 figures; accepted to the 2020 BlackboxNLP workshop},
  file = {/home/vatai/Sync/zotero-data/pdfs/mccoy2020berts_of_a_feather_do_not_generalize_together.pdf;/home/vatai/Sync/zotero-data/storage/4CR62RT8/1911.html}
}

@inproceedings{mezdour2023deep,
  title = {A {{Deep Learning Model}} for {{Loop Interchange}}},
  booktitle = {Proceedings of the 32nd {{ACM SIGPLAN International Conference}} on {{Compiler Construction}}},
  author = {Mezdour, Lina and Kadem, Khadidja and Merouani, Massinissa and Haichour, Amina Selma and Amarasinghe, Saman and Baghdadi, Riyadh},
  year = {2023},
  month = feb,
  series = {{{CC}} 2023},
  pages = {50--60},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3578360.3580257},
  urldate = {2024-11-10},
  abstract = {Loop interchange is an important code optimization that improves data locality and extracts parallelism. While previous research in compilers has tried to automate the selection of which loops to interchange, existing methods have an important limitation. They use less precise machine models. This is mainly because developing a model to predict whether to interchange two loops is challenging since such a prediction depends on many factors.   While state-of-the-art methods try to avoid this problem by using a deep-learning based cost model, they suffer from another limitation. They scale proportionally with the number of loop levels of a given loop nest. This is mainly because they use the model to evaluate all the possible loop interchanges (or a subset of the most promising ones).   In this paper, we propose a novel deep-learning model for loop interchange that addresses the previous limitations. It takes a code representation as input and predicts the best pair of loops to interchange. Compared to state-of-the-art deep-learning based cost models,   it requires constant time to predict the best loop interchange.   This is in contrast to state-of-the-art deep learning models that are used to evaluate all the loop pairs and then pick the best one.   The proposed model is the first deep learning model that requires a constant time to predict the best loops to interchange. The model is implemented and evaluated in the Tiramisu compiler, a state-of-the-art polyhedral compiler.   We evaluate the proposed model on a benchmark of Tiramisu programs and show an accuracy of 78.57\% for 1-shot and 85.71\% for 2-shots. Experiments show that our model outperforms the cost model currently used by the Tiramisu compiler by 8.57\% in terms of 1-shot accuracy, and 5.71\% with 2-shots accuracy, while at the same time reducing the total execution time needed for predicting the best pair of loops to interchange.},
  isbn = {9798400700880},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/Mezdour et al. - 2023 - A Deep Learning Model for Loop Interchange.pdf}
}

@article{miura2015effect,
  title = {Effect of {{Molecular Diffusion}} on the {{Spin Dynamics}} of a {{Micellized Radical Pair}} in {{Low Magnetic Fields Studied}} by {{Monte Carlo Simulation}}},
  author = {Miura, Tomoaki and Murai, Hisao},
  year = {2015},
  month = jun,
  journal = {J. Phys. Chem. A},
  volume = {119},
  number = {22},
  pages = {5534--5544},
  publisher = {American Chemical Society},
  issn = {1089-5639},
  doi = {10.1021/acs.jpca.5b02183},
  urldate = {2022-10-18},
  abstract = {Magnetic field effect is a powerful tool to study dynamics and kinetics of radical pairs (RPs), which are one of the most important intermediates for organic photon-energy conversion reactions. However, quantitative discussion regarding the relationship between the modulation of interelectron interactions and spin dynamics at low magnetic fields ({$<$}10 mT) is still an open question. We have studied the spin dynamics of a long-lived RP in a micelle by newly developed Monte Carlo simulation, in which fluctuations of the exchange and magnetic dipolar interactions by in-cage diffusion are directly introduced to the time-domain spin dynamics calculation. State-dependent relaxation/dephasing times of a few to a few tens of nanoseconds are obtained by simulations without hyperfine interactions (HFIs) as a function of the mutual diffusion constant ({$\sim$}10--6 cm2/s). Simulations with the HFIs exhibit incoherent singlet--triplet (S--T) mixings resulting from interplay between the HFIs and the fluctuating spin--spin interactions. The experimentally observed incoherent S--T mixing of {$\sim$}20 ns at 3 mT for a singlet-born RP in a sodium dodecyl sulfate micelle is reproduced by the simulation with reasonable diffusion coefficients. The computational method developed here contributes to quantitative detection of molecular motion that governs the recombination efficiency of RPs.},
  file = {/home/vatai/Sync/zotero-data/pdfs/miura_murai2015effect_of_molecular_diffusion_on_the_spin_dynamics_of_a_micellized_radical_pair.pdf;/home/vatai/Sync/zotero-data/pdfs/miura_murai2015effect_of_molecular_diffusion_on_the_spin_dynamics_of_a_micellized_radical_pair2.pdf}
}

@misc{moraitis2022softhebb,
  title = {{{SoftHebb}}: {{Bayesian Inference}} in {{Unsupervised Hebbian Soft Winner-Take-All Networks}}},
  shorttitle = {{{SoftHebb}}},
  author = {Moraitis, Timoleon and Toichkin, Dmitry and Journ{\'e}, Adrien and Chua, Yansong and Guo, Qinghai},
  year = {2022},
  month = nov,
  number = {arXiv:2107.05747},
  eprint = {2107.05747},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.05747},
  urldate = {2023-03-28},
  abstract = {Hebbian plasticity in winner-take-all (WTA) networks is highly attractive for neuromorphic on-chip learning, owing to its efficient, local, unsupervised, and on-line nature. Moreover, its biological plausibility may help overcome important limitations of artificial algorithms, such as their susceptibility to adversarial attacks, and their high demands for training-example quantity and repetition. However, Hebbian WTA learning has found little use in machine learning (ML), likely because it has been missing an optimization theory compatible with deep learning (DL). Here we show rigorously that WTA networks constructed by standard DL elements, combined with a Hebbian-like plasticity that we derive, maintain a Bayesian generative model of the data. Importantly, without any supervision, our algorithm, SoftHebb, minimizes cross-entropy, i.e. a common loss function in supervised DL. We show this theoretically and in practice. The key is a "soft" WTA where there is no absolute "hard" winner neuron. Strikingly, in shallow-network comparisons with backpropagation (BP), SoftHebb shows advantages beyond its Hebbian efficiency. Namely, it converges in fewer iterations, and is significantly more robust to noise and adversarial attacks. Notably, attacks that maximally confuse SoftHebb are also confusing to the human eye, potentially linking human perceptual robustness, with Hebbian WTA circuits of cortex. Finally, SoftHebb can generate synthetic objects as interpolations of real object classes. All in all, Hebbian efficiency, theoretical underpinning, cross-entropy-minimization, and surprising empirical advantages, suggest that SoftHebb may inspire highly neuromorphic and radically different, but practical and advantageous learning algorithms and hardware accelerators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/home/vatai/Sync/zotero-data/pdfs/moraitis2022softhebb.pdf;/home/vatai/Sync/zotero-data/storage/TV66EHC2/2107.html}
}

@article{moses2021polygeist,
  title = {Polygeist: {{Affine C}} in {{MLIR}}},
  author = {Moses, William S and Zhao, Ruizhe and Chelini, Lorenzo and Zinenko, Oleksandr},
  year = {2021},
  pages = {12},
  abstract = {We present Polygeist, a new tool that reroutes polyhedral compilation flows to use the representation available in the recent MLIR compilation infrastructure. It consists of two parts: a C and C++ frontend capable of converting a wide variety of existing codes into MLIR suitable for polyhedral transformation, and a bi-directional conversion between MLIR's polyhedral representation and existing polyhedral exchange formats. We demonstrate Polygeist's flow by converting the entire Polybench/C benchmark suite into MLIR, and by performing an IR-to-IR optimization leveraging an existing polyhedral compiler (Pluto). Our flow produces results within 1.25\% of the state-of-the-art Clang compiler, enabling direct comparison of source-to-source and IR-to-binary compilers. We believe Polygeist can improve the interoperation between MLIR and the existing polyhedral tooling, benefiting both the research and the production compiler communities.},
  langid = {english},
  keywords = {LLVM},
  file = {/home/vatai/Sync/zotero-data/pdfs/moses2021polygeist.pdf}
}

@misc{munroe2017machine,
  type = {Web Comic},
  title = {Machine {{Learning}}},
  author = {Munroe, Randall},
  year = {2017},
  month = may,
  journal = {xkcd},
  urldate = {2024-08-29},
  howpublished = {https://xkcd.com/1838/},
  file = {/home/vatai/Sync/zotero-data/storage/ANAY4NZ6/1838.html}
}

@misc{narayanan2021efficient,
  title = {Efficient {{Large-Scale Language Model Training}} on {{GPU Clusters Using Megatron-LM}}},
  author = {Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay Anand and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and Phanishayee, Amar and Zaharia, Matei},
  year = {2021},
  month = aug,
  number = {arXiv:2104.04473},
  eprint = {2104.04473},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.04473},
  urldate = {2023-04-07},
  abstract = {Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress. In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+\% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52\% of theoretical peak. Our code is open sourced at https://github.com/nvidia/megatron-lm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Distributed Parallel and Cluster Computing},
  note = {Comment: Accepted to SC 2021
\par
\href{https://github.com/rioyokotalab/DeepSpeedFugaku/blob/main/megatron/utils.py\#L248}{https://github.com/rioyokotalab/DeepSpeedFugaku/blob/main/megatron/utils.py\#L248}},
  file = {/home/vatai/Sync/zotero-data/pdfs/narayanan2021efficient_large-scale_language_model_training_on_gpu_clusters_using_megatron-lm.pdf;/home/vatai/Sync/zotero-data/storage/P85DHVCQ/2104.html}
}

@article{nielsen2019molspin,
  title = {{{MolSpin}}---{{Flexible}} and Extensible General Spin Dynamics Software},
  author = {Nielsen, Claus and Solov'yov, Ilia A.},
  year = {2019},
  month = nov,
  journal = {J. Chem. Phys.},
  volume = {151},
  number = {19},
  pages = {194105},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.5125043},
  urldate = {2022-12-26},
  file = {/home/vatai/Sync/zotero-data/pdfs/nielsen_solov’yov2019molspin—flexible_and_extensible_general_spin_dynamics_software.pdf}
}

@misc{nijkamp2022conversational,
  title = {A {{Conversational Paradigm}} for {{Program Synthesis}}},
  author = {Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  year = {2022},
  month = mar,
  number = {arXiv:2203.13474},
  eprint = {2203.13474},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.13474},
  urldate = {2022-08-04},
  abstract = {Program synthesis strives to generate a computer program as a solution to a given problem specification. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent specification faced in prior approaches. Our new approach casts the process of writing a specification and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in natural language and the desired program is conditionally sampled. We train a family of large language models, called CodeGen, on natural language and programming language data. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autoregressive language modeling. To study the model behavior on conversational program synthesis, we develop a multi-turn programming benchmark (MTPB), where solving each problem requires multi-step synthesis via multi-turn conversation between the user and the model. Our findings show the emergence of conversational capabilities and the effectiveness of the proposed conversational program synthesis paradigm. In addition, our model CodeGen (with up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the HumanEval benchmark. We make the training library JaxFormer including checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {/home/vatai/Sync/zotero-data/pdfs/nijkamp2022a_conversational_paradigm_for_program_synthesis.pdf;/home/vatai/Sync/zotero-data/storage/KQDKJY2B/2203.html}
}

@article{olshansky2019photogenerated,
  title = {Photogenerated {{Spin-Entangled Qubit}} ({{Radical}}) {{Pairs}} in {{DNA Hairpins}}: {{Observation}} of {{Spin Delocalization}} and {{Coherence}}},
  shorttitle = {Photogenerated {{Spin-Entangled Qubit}} ({{Radical}}) {{Pairs}} in {{DNA Hairpins}}},
  author = {Olshansky, Jacob H. and Krzyaniak, Matthew D. and Young, Ryan M. and Wasielewski, Michael R.},
  year = {2019},
  month = feb,
  journal = {J. Am. Chem. Soc.},
  volume = {141},
  number = {5},
  pages = {2152--2160},
  publisher = {American Chemical Society},
  issn = {0002-7863},
  doi = {10.1021/jacs.8b13155},
  urldate = {2022-12-16},
  abstract = {The ability to prepare physical qubits in specific initial quantum states is a critical requirement for their use in quantum information science (QIS). Subnanosecond photoinduced electron transfer in a structurally well-defined donor--acceptor system can be used to produce an entangled spin qubit (radical) pair in a pure initial singlet state fulfilling this criterion. Synthetic DNA is a promising platform on which to build spin qubit arrays with fixed spatial relationships; therefore, we have prepared a series of DNA hairpins in which naphthalenediimide (NDI) is the chromophore/acceptor hairpin linker, variable-length diblock A- and G-tracts are intermediate donors, and a stilbenediether (Sd) is the terminal donor. Photoexcitation of NDI in these DNA hairpins generates high-yield, long-lived, entangled spin qubit pairs at 85 K, and time-resolved and pulse electron paramagnetic resonance (EPR) spectroscopies are used to probe their spin dynamics. Specifically, measurements of the distance-dependent dipolar coupling between the two spins are used to obtain the average spin qubit pair distance in the absence of the terminal Sd donor and reveal that one of the spins is fully delocalized across up to five adjacent guanines in a G-tract on the EPR time scale. We have recently shown that extensive spin hopping between degenerate sites accessible to one spin of the pair may result in spin decoherence. However, we observe a strong out-of-phase electron spin echo envelope modulation (OOP-ESEEM) signal from the NDI{$\bullet$}----Sd{$\bullet$}+ spin qubit pair in DNA hairpins showing that spin coherence is maintained across a 2 adenine A-tract followed by a 2--4 guanine G-tract as a result of rapid spin transport to Sd. These results demonstrate that pulse-EPR can manipulate coherent spin states in DNA hairpins, which is essential for quantum gate operations relevant to QIS applications.},
  file = {/home/vatai/Sync/zotero-data/pdfs/olshansky2019photogenerated_spin-entangled_qubit_(radical)_pairs_in_dna_hairpins.pdf}
}

@article{park2013predictive,
  title = {Predictive {{Modeling}} in a {{Polyhedral Optimization Space}}},
  author = {Park, Eunjung and Cavazos, John and Pouchet, Louis-No{\"e}l and Bastoul, C{\'e}dric and Cohen, Albert and Sadayappan, P.},
  year = {2013},
  month = oct,
  journal = {Int J Parallel Prog},
  volume = {41},
  number = {5},
  pages = {704--750},
  issn = {1573-7640},
  doi = {10.1007/s10766-013-0241-1},
  urldate = {2022-09-28},
  abstract = {High-level program optimizations, such as loop transformations, are critical for high performance on multi-core targets. However, complex sequences of loop transformations are often required to expose parallelism (both coarse-grain and fine-grain) and improve data locality. The polyhedral compilation framework has proved to be very effective at representing these complex sequences and restructuring compute-intensive applications, seamlessly handling perfectly and imperfectly nested loops. It models arbitrarily complex sequences of loop transformations in a unified mathematical framework, dramatically increasing the expressiveness (and expected effectiveness) of the loop optimization stage. Nevertheless identifying the most effective loop transformations remains a major challenge: current state-of-the-art heuristics in polyhedral frameworks simply fail to expose good performance over a wide range of numerical applications. Their lack of effectiveness is mainly due to simplistic performance models that do not reflect the complexity today's processors (CPU, cache behavior, etc.). We address the problem of selecting the best polyhedral optimizations with dedicated machine learning models, trained specifically on the target machine. We show that these models can quickly select high-performance optimizations with very limited iterative search. We decouple the problem of selecting good complex sequences of optimizations in two stages: (1) we narrow the set of candidate optimizations using static cost models to select the loop transformations that implement specific high-level optimizations (e.g., tiling, parallelism, etc.); (2) we predict the performance of each high-level complex optimization sequence with trained models that take as input a performance-counter characterization of the original program. Our end-to-end framework is validated using numerous benchmarks on two modern multi-core platforms. We investigate a variety of different machine learning algorithms and hardware counters, and we obtain performance improvements over productions compilers ranging on average from \$\$3.2{\textbackslash}times \$\$to \$\$8.7{\textbackslash}times \$\$, by running not more than \$\$6\$\$program variants from a polyhedral optimization space.},
  langid = {english},
  keywords = {Iterative compilation,Loop transformation,Machine learning,Performance counters,Polyhedral optimization},
  note = {Old paper (sounds like they are doing what we want to do)},
  file = {/home/vatai/Sync/zotero-data/pdfs/park2013predictive_modeling_in_a_polyhedral_optimization_space.pdf}
}

@inproceedings{patabandi2023efficiently,
  title = {Efficiently {{Learning Locality Optimizations}} by {{Decomposing Transformation Domains}}},
  booktitle = {Proceedings of the 32nd {{ACM SIGPLAN International Conference}} on {{Compiler Construction}}},
  author = {Patabandi, Tharindu R. and Hall, Mary},
  year = {2023},
  month = feb,
  series = {{{CC}} 2023},
  pages = {37--49},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3578360.3580272},
  urldate = {2023-03-02},
  abstract = {Optimizing compilers for efficient machine learning are more important than ever due to the rising ubiquity of the application domain in numerous facets of life. Predictive model-guided compiler optimization is sometimes used to derive sequences of loop transformations that optimize the performance of the machine learning computations. However, training-data generation for these models often requires the traversal of prohibitively expensive schedule spaces and executing code variants corresponding to different schedule options. The size of these search spaces can quickly explode when predicting the combined effects of multiple loop transformations. This paper characterizes a learning strategy for deriving transformation sequences called Composed Singular Prediction (CSP). Instead of a monolithic cost model that predicts the profitability of a given transformation sequence, CSP exploits a collection of cost models, each trained on a particular loop transformation domain. In a case study, a domain-specific compiler deploys the learned models to predict loop tiling and loop permutation schedules to perform data locality optimization of Conv2d kernels. The system achieves performance improvements up to 4.0x against Intel oneDNN while saving {\textasciitilde} 105.3x in training data collection time compared to exhaustive exploration of the design space.},
  isbn = {9798400700880},
  keywords = {Compilers,Convolution,Optimization},
  file = {/home/vatai/Sync/zotero-data/pdfs/patabandi_hall2023efficiently_learning_locality_optimizations_by_decomposing_transformation.pdf}
}

@inproceedings{pati2022demystifying,
  title = {Demystifying {{BERT}}: {{System Design Implications}}},
  shorttitle = {Demystifying {{BERT}}},
  booktitle = {2022 {{IEEE International Symposium}} on {{Workload Characterization}} ({{IISWC}})},
  author = {Pati, Suchita and Aga, Shaizeen and Jayasena, Nuwan and Sinclair, Matthew D.},
  year = {2022},
  month = nov,
  pages = {296--309},
  doi = {10.1109/IISWC55918.2022.00033},
  urldate = {2024-11-10},
  abstract = {Transfer learning in natural language processing (NLP) uses increasingly large models that tackle challenging problems. Consequently, these applications are driving the requirements of future systems. To this end, we study the computationally and time-intensive training phase of NLP models and identify how its algorithmic behavior can guide future accelerator design. We focus on BERT (Bi-directional Encoder Representations from Transformer), one of the most popular Transformer-based NLP models, and identify key operations which are worthy of attention in accelerator design. In particular, we focus on the manifestation, size, and arithmetic behavior of these operations which remain constant irrespective of hardware choice. Our results show that although computations which manifest as matrix multiplications dominate BERT's execution, they have considerable heterogeneity. Furthermore, we characterize memory-intensive computations which also feature prominently in BERT but have received less attention. To capture future Transformer trends, we also show and discuss implications of these behaviors as networks get larger. Moreover, we study the impact of key training techniques like distributed training, check-pointing, and mixed-precision training. Finally, our analysis identifies holistic solutions to optimize systems for BERT-like models and we further demonstrate how enhancing compute-intensive accelerators with near-memory compute can help accelerate Transformer networks.},
  keywords = {Accelerator design,Bit error rate,Characterization,Computational modeling,Deep Learning,Near memory Computing,Propulsion,Technological innovation,Training,Transfer learning,Transformers},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/NLP/Pati et al. - 2022 - Demystifying BERT System Design Implications.pdf;/home/vatai/Sync/zotero-data/storage/SDNHFPF5/9975394.html}
}

@inproceedings{peng2018siena,
  title = {Siena: {{Exploring}} the {{Design Space}} of {{Heterogeneous Memory Systems}}},
  shorttitle = {Siena},
  booktitle = {{{SC18}}: {{International Conference}} for {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Peng, Ivy B. and Vetter, Jeffrey S.},
  year = {2018},
  month = nov,
  pages = {427--440},
  publisher = {IEEE},
  address = {Dallas, TX, USA},
  doi = {10.1109/SC.2018.00036},
  urldate = {2022-10-17},
  abstract = {Memory systems are crucial to the performance, power, and cost of high-performance computing systems. Recently, multiple factors are driving the need for more complex, deep memory hierarchies. However, architects and customers are struggling to design memory systems that effectively balance multiple, often competing, factors in this large, multidimensional, and fast-moving design space. In this paper, we systematically explore the organization of heterogeneous memory systems on a framework called Siena. Siena facilitates quick exploration of memory architectures with flexible configurations of memory systems and realistic memory workloads. We perform a design space exploration on 22 proposed memory systems using eight relevant workloads. Our results show that horizontal organizations of memories can achieve higher performance than vertical organizations when the distribution of memory traffic balances the performance gap between memories. However, the coupling effects through shared resources and application behaviors could negate the advantage of high-performance memory in horizontal organizations.},
  isbn = {978-1-5386-8384-2},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/peng_vetter2018siena.pdf}
}

@misc{peters2019sparse,
  title = {Sparse {{Sequence-to-Sequence Models}}},
  author = {Peters, Ben and Niculae, Vlad and Martins, Andr{\'e} F. T.},
  year = {2019},
  month = jun,
  number = {arXiv:1905.05702},
  eprint = {1905.05702},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1905.05702},
  urldate = {2022-09-21},
  abstract = {Sequence-to-sequence models are a powerful workhorse of NLP. Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. In this paper, we propose sparse sequence-to-sequence models, rooted in a new family of \${\textbackslash}alpha\$-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any \${\textbackslash}alpha {$>$} 1\$. We provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. Experiments on morphological inflection and machine translation reveal consistent gains over dense models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: ACL 2019 Camera Ready},
  file = {/home/vatai/Sync/zotero-data/pdfs/peters2019sparse_sequence-to-sequence_models2.pdf;/home/vatai/Sync/zotero-data/storage/Q77B4RFI/1905.html}
}

@misc{phan2019self,
  title = {Self {{Learning}} from {{Large Scale Code Corpus}} to {{Infer Structure}} of {{Method Invocations}}},
  author = {Phan, Hung},
  year = {2019},
  month = sep,
  journal = {arXiv.org},
  urldate = {2024-09-07},
  abstract = {Automatically generating code from a textual description of method invocation confronts challenges. There were two current research directions for this problem. One direction focuses on considering a textual description of method invocations as a separate Natural Language query and do not consider the surrounding context of the code. Another direction takes advantage of a practical large scale code corpus for providing a Machine Translation model to generate code. However, this direction got very low accuracy. In this work, we tried to improve these drawbacks by proposing MethodInfoToCode, an approach that embeds context information and optimizes the ability of learning of original Phrase-based Statistical Machine Translation (PBMT) in NLP to infer implementation of method invocation given method name and other context information. We conduct an expression prediction models learned from 2.86 million method invocations from the practical data of high qualities corpus on Github that used 6 popular libraries: JDK, Android, GWT, Joda-Time, Hibernate, and Xstream. By the evaluation, we show that if the developers only write the method name of a method invocation in a body of a method, MethodInfoToCode can predict the generated expression correctly at 73\% in F1 score.},
  howpublished = {https://arxiv.org/abs/1909.03147v1},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/Codegen/needed/phan2019self_learning_from_large_scale_code_corpus_to_infer_structure_of_method_invocations.pdf}
}

@article{pollard1994solution,
  title = {Solution of the {{Redfield}} Equation for the Dissipative Quantum Dynamics of Multilevel Systems},
  author = {Pollard, W. Thomas and Friesner, Richard A.},
  year = {1994},
  month = apr,
  journal = {The Journal of Chemical Physics},
  volume = {100},
  number = {7},
  pages = {5054--5065},
  issn = {0021-9606},
  doi = {10.1063/1.467222},
  urldate = {2023-08-06},
  abstract = {We present a new method for solving the Redfield equation, which describes the evolution of the reduced density matrix of a multilevel quantum-mechanical system interacting with a thermal bath. The method is based on a new decomposition of the Redfield relaxation tensor that makes possible its direct application to the density matrix without explicit construction of the full tensor. In the resulting expressions, only ordinary matrices are involved and so any quantum system whose Hamiltonian can be diagonalized can be treated with the full Redfield theory. To efficiently solve the equation of motion for the density matrix, we introduce a generalization of the short-iterative-Lanczos propagator. Together, these contributions allow the complete Redfield theory to be applied to significantly larger systems than was previously possible. Several model calculations are presented to illustrate the methodology, including one example with 172 quantum states.},
  file = {/home/vatai/Sync/zotero-data/pdfs/pollard1994solution_of_the_redfield_equation_for_the_dissipative_quantum_dynamics_of.pdf;/home/vatai/Sync/zotero-data/storage/MEG457KI/Solution-of-the-Redfield-equation-for-the.html}
}

@misc{qi2017pointnet,
  title = {{{PointNet}}: {{Deep Learning}} on {{Point Sets}} for {{3D Classification}} and {{Segmentation}}},
  shorttitle = {{{PointNet}}},
  author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
  year = {2017},
  month = apr,
  number = {arXiv:1612.00593},
  eprint = {1612.00593},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1612.00593},
  urldate = {2024-06-04},
  abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: CVPR 2017},
  file = {/home/vatai/Sync/zotero-data/pdfs/qi2017pointnet.pdf;/home/vatai/Sync/zotero-data/storage/ZASKZHGU/1612.html}
}

@article{ragan-kelley2013halide,
  title = {Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines},
  author = {{Ragan-Kelley}, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain},
  year = {2013},
  pages = {12},
  abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/ragan-kelley2013halide.pdf}
}

@misc{rajbhandari2020zero,
  title = {{{ZeRO}}: {{Memory Optimizations Toward Training Trillion Parameter Models}}},
  shorttitle = {{{ZeRO}}},
  author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  year = {2020},
  month = may,
  number = {arXiv:1910.02054},
  eprint = {1910.02054},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-02-06},
  abstract = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/vatai/Sync/zotero-data/pdfs/rajbhandari2020zero.pdf}
}

@misc{rajbhandari2021zeroinfinity,
  title = {{{ZeRO-Infinity}}: {{Breaking}} the {{GPU Memory Wall}} for {{Extreme Scale Deep Learning}}},
  shorttitle = {{{ZeRO-Infinity}}},
  author = {Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  year = {2021},
  month = apr,
  number = {arXiv:2104.07857},
  eprint = {2104.07857},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.07857},
  urldate = {2023-04-07},
  abstract = {In the last three years, the largest dense deep learning models have grown over 1000x to reach hundreds of billions of parameters, while the GPU memory has only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has been supported primarily though system innovations that allow large models to fit in the aggregate GPU memory of multiple GPUs. However, we are getting close to the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion parameter model for training, and such clusters are simply out of reach for most data scientists. In addition, training models at that scale requires complex combinations of parallelism techniques that puts a big burden on the data scientists to refactor their model. In this paper we present ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code refactoring. At the same time it achieves excellent training throughput and scalability, unencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models with tens and even hundreds of trillions of parameters for training on current generation GPU clusters. It can be used to fine-tune trillion parameter models on a single NVIDIA DGX-2 node, making large models more accessible. In terms of training throughput and scalability, it sustains over 25 petaflops on 512 NVIDIA V100 GPUs(40\% of peak), while also demonstrating super linear scalability. An open source implementation of ZeRO-Infinity is available through DeepSpeed, a deep learning optimization library that makes distributed training easy, efficient, and effective.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Computer Science - Performance},
  file = {/home/vatai/Sync/zotero-data/pdfs/rajbhandari2021zero-infinity.pdf;/home/vatai/Sync/zotero-data/storage/FGNK5V87/2104.html}
}

@article{ramsay2022radical,
  title = {Radical Triads, Not Pairs, May Explain Effects of Hypomagnetic Fields on Neurogenesis},
  author = {Ramsay, Jess and Kattnig, Daniel R.},
  year = {2022},
  month = sep,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {9},
  pages = {e1010519},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010519},
  urldate = {2022-09-25},
  abstract = {Adult hippocampal neurogenesis and hippocampus-dependent cognition in mice have been found to be adversely affected by hypomagnetic field exposure. The effect concurred with a reduction of reactive oxygen species in the absence of the geomagnetic field. A recent theoretical study suggests a mechanistic interpretation of this phenomenon in the framework of the Radical Pair Mechanism. According to this model, a flavin-superoxide radical pair, born in the singlet spin configuration, undergoes magnetic field-dependent spin dynamics such that the pair's recombination is enhanced as the applied magnetic field is reduced. This model has two ostensible weaknesses: a) the assumption of a singlet initial state is irreconcilable with known reaction pathways generating such radical pairs, and b) the model neglects the swift spin relaxation of free superoxide, which abolishes any magnetic sensitivity in geomagnetic/hypomagnetic fields. We here suggest that a model based on a radical triad and the assumption of a secondary radical scavenging reaction can, in principle, explain the phenomenon without unnatural assumptions, thus providing a coherent explanation of hypomagnetic field effects in biology.},
  langid = {english},
  keywords = {Flavin,Magnetic fields,Oxygen,Particle spin,Recombination reactions,Relaxation time,Superoxides,Vitamin C},
  file = {/home/vatai/Sync/zotero-data/pdfs/ramsay_kattnig2022radical_triads,_not_pairs,_may_explain_effects_of_hypomagnetic_fields_on.pdf;/home/vatai/Sync/zotero-data/storage/A78AW9VI/article.html}
}

@misc{ramzi2020denoising,
  title = {Denoising {{Score-Matching}} for {{Uncertainty Quantification}} in {{Inverse Problems}}},
  author = {Ramzi, Zaccharie and Remy, Benjamin and Lanusse, Francois and Starck, Jean-Luc and Ciuciu, Philippe},
  year = {2020},
  month = nov,
  number = {arXiv:2011.08698},
  eprint = {2011.08698},
  primaryclass = {physics, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.08698},
  urldate = {2024-05-27},
  abstract = {Deep neural networks have proven extremely efficient at solving a wide rangeof inverse problems, but most often the uncertainty on the solution they provideis hard to quantify. In this work, we propose a generic Bayesian framework forsolving inverse problems, in which we limit the use of deep neural networks tolearning a prior distribution on the signals to recover. We adopt recent denoisingscore matching techniques to learn this prior from data, and subsequently use it aspart of an annealed Hamiltonian Monte-Carlo scheme to sample the full posteriorof image inverse problems. We apply this framework to Magnetic ResonanceImage (MRI) reconstruction and illustrate how this approach not only yields highquality reconstructions but can also be used to assess the uncertainty on particularfeatures of a reconstructed image.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Physics - Medical Physics,Statistics - Machine Learning},
  file = {/home/vatai/Sync/zotero-data/pdfs/ramzi2020denoising_score-matching_for_uncertainty_quantification_in_inverse_problems.pdf;/home/vatai/Sync/zotero-data/storage/BUGBLKZ3/2011.html}
}

@inproceedings{rawat2019optimizing,
  title = {On {{Optimizing Complex Stencils}} on {{GPUs}}},
  booktitle = {2019 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Rawat, Prashant Singh and Vaidya, Miheer and {Sukumaran-Rajam}, Aravind and Rountev, Atanas and Pouchet, Louis-No{\"e}l and Sadayappan, P.},
  year = {2019},
  month = may,
  pages = {641--652},
  issn = {1530-2075},
  doi = {10.1109/IPDPS.2019.00073},
  abstract = {Stencil computations are often the compute-intensive kernel in many scientific applications. With the increasing demand for computational accuracy, and the emergence of massively data-parallel high-bandwidth architectures like GPUs, stencils have steadily become more complex in terms of the stencil order, data accesses, and reuse patterns. Many prior efforts have focused on optimizing simpler stencil computations on various platforms. However, existing stencil code generators face challenges in optimizing such complex multi-statement stencil DAGs. This paper addresses the challenges in optimizing high-order stencil DAGs on GPUs by focusing on two key considerations: (1) enabling the domain expert to guide the code optimization, which may otherwise be extremely challenging for complex stencils; and (2) using bottleneck analysis via runtime profiling to guide the application of optimizations, and the tuning of various code generation parameters. We implement these abstractions in a prototype code generation framework termed ARTEMIS, and evaluate its efficacy over multiple stencil kernels with varying complexity and operational intensity on an NVIDIA P100 GPU.},
  keywords = {Code optimization,DSL,Generators,GPGPU,Graphics processing units,Instruction sets,Kernel,Optimization,Registers,Stencil Computations},
  file = {/home/vatai/Sync/zotero-data/pdfs/rawat2019on_optimizing_complex_stencils_on_gpus.pdf;/home/vatai/Sync/zotero-data/storage/H3GN24HP/8820786.html}
}

@misc{real2020automlzero,
  title = {{{AutoML-Zero}}: {{Evolving Machine Learning Algorithms From Scratch}}},
  shorttitle = {{{AutoML-Zero}}},
  author = {Real, Esteban and Liang, Chen and So, David R. and Le, Quoc V.},
  year = {2020},
  month = jun,
  number = {arXiv:2003.03384},
  eprint = {2003.03384},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2003.03384},
  urldate = {2022-10-19},
  abstract = {Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.2,I.2.6,Statistics - Machine Learning},
  note = {Comment: Accepted for publication at the 37th International Conference on Machine Learning (ICML 2020). Near camera-ready version},
  file = {/home/vatai/Sync/zotero-data/pdfs/real2020automl-zero.pdf;/home/vatai/Sync/zotero-data/storage/3WXMB5QS/2003.html}
}

@misc{reisinger2024matthiasjreisinger,
  title = {{{MatthiasJReisinger}}/{{PolyBenchC-4}}.2.1},
  author = {Reisinger, Matthias},
  year = {2024},
  month = aug,
  urldate = {2024-09-09},
  abstract = {PolyBench/C benchmark suite (version 4.2.1 beta) from  http://web.cse.ohio-state.edu/{\textasciitilde}pouchet/software/polybench/}
}

@misc{remy2020probabilistic,
  title = {Probabilistic {{Mapping}} of {{Dark Matter}} by {{Neural Score Matching}}},
  author = {Remy, Benjamin and Lanusse, Francois and Ramzi, Zaccharie and Liu, Jia and Jeffrey, Niall and Starck, Jean-Luc},
  year = {2020},
  month = nov,
  number = {arXiv:2011.08271},
  eprint = {2011.08271},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.08271},
  urldate = {2024-05-27},
  abstract = {The Dark Matter present in the Large-Scale Structure of the Universe is invisible, but its presence can be inferred through the small gravitational lensing effect it has on the images of far away galaxies. By measuring this lensing effect on a large number of galaxies it is possible to reconstruct maps of the Dark Matter distribution on the sky. This, however, represents an extremely challenging inverse problem due to missing data and noise dominated measurements. In this work, we present a novel methodology for addressing such inverse problems by combining elements of Bayesian statistics, analytic physical theory, and a recent class of Deep Generative Models based on Neural Score Matching. This approach allows to do the following: (1) make full use of analytic cosmological theory to constrain the 2pt statistics of the solution, (2) learn from cosmological simulations any differences between this analytic prior and full simulations, and (3) obtain samples from the full Bayesian posterior of the problem for robust Uncertainty Quantification. We present an application of this methodology on the first deep-learning-assisted Dark Matter map reconstruction of the Hubble Space Telescope COSMOS field.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics},
  note = {Comment: 6 pages, accepted submission to the NeurIPS 2019 Machine Learning and the Physical Sciences Workshop},
  file = {/home/vatai/Sync/zotero-data/pdfs/remy2020probabilistic_mapping_of_dark_matter_by_neural_score_matching.pdf;/home/vatai/Sync/zotero-data/storage/PENXTXGD/2011.html}
}

@article{remy2023probabilistic,
  title = {Probabilistic {{Mass Mapping}} with {{Neural Score Estimation}}},
  author = {Remy, Benjamin and Lanusse, Francois and Jeffrey, Niall and Liu, Jia and Starck, Jean-Luc and Osato, Ken and Schrabback, Tim},
  year = {2023},
  month = apr,
  journal = {A\&A},
  volume = {672},
  eprint = {2201.05561},
  primaryclass = {astro-ph},
  pages = {A51},
  issn = {0004-6361, 1432-0746},
  doi = {10.1051/0004-6361/202243054},
  urldate = {2024-05-27},
  abstract = {Weak lensing mass-mapping is a useful tool to access the full distribution of dark matter on the sky, but because of intrinsic galaxy ellipticies and finite fields/missing data, the recovery of dark matter maps constitutes a challenging ill-posed inverse problem. We introduce a novel methodology allowing for efficient sampling of the high-dimensional Bayesian posterior of the weak lensing mass-mapping problem, and relying on simulations for defining a fully non-Gaussian prior. We aim to demonstrate the accuracy of the method on simulations, and then proceed to applying it to the mass reconstruction of the HST/ACS COSMOS field. The proposed methodology combines elements of Bayesian statistics, analytic theory, and a recent class of Deep Generative Models based on Neural Score Matching. This approach allows us to do the following: 1) Make full use of analytic cosmological theory to constrain the 2pt statistics of the solution. 2) Learn from cosmological simulations any differences between this analytic prior and full simulations. 3) Obtain samples from the full Bayesian posterior of the problem for robust Uncertainty Quantification. We demonstrate the method on the \${\textbackslash}kappa\$TNG simulations and find that the posterior mean significantly outperfoms previous methods (Kaiser-Squires, Wiener filter, Sparsity priors) both on root-mean-square error and in terms of the Pearson correlation. We further illustrate the interpretability of the recovered posterior by establishing a close correlation between posterior convergence values and SNR of clusters artificially introduced into a field. Finally, we apply the method to the reconstruction of the HST/ACS COSMOS field and yield the highest quality convergence map of this field to date.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning},
  note = {Comment: Submitted to A\&A, 20 pages, 15 figures, comments are welcome},
  file = {/home/vatai/Sync/zotero-data/pdfs/remy2023probabilistic_mass_mapping_with_neural_score_estimation.pdf;/home/vatai/Sync/zotero-data/storage/MSH998WT/2201.html}
}

@article{rodgers2009chemical,
  title = {Chemical Magnetoreception in Birds: {{The}} Radical Pair Mechanism},
  shorttitle = {Chemical Magnetoreception in Birds},
  author = {Rodgers, Christopher T. and Hore, P. J.},
  year = {2009},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {106},
  number = {2},
  pages = {353--360},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.0711968106},
  urldate = {2022-07-30},
  file = {/home/vatai/Sync/zotero-data/pdfs/rodgers_hore2009chemical_magnetoreception_in_birds2.pdf}
}

@article{rodgers2009chemicala,
  title = {Chemical Magnetoreception in Birds: {{The}} Radical Pair Mechanism},
  shorttitle = {Chemical Magnetoreception in Birds},
  author = {Rodgers, Christopher T. and Hore, P. J.},
  year = {2009},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {106},
  number = {2},
  pages = {353--360},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.0711968106},
  urldate = {2022-10-08},
  file = {/home/vatai/Sync/zotero-data/pdfs/rodgers_hore2009chemical_magnetoreception_in_birds.pdf}
}

@article{rodgers2009magnetic,
  title = {Magnetic Field Effects in Chemical Systems},
  author = {Rodgers, Christopher T.},
  year = {2009},
  month = jan,
  journal = {Pure and Applied Chemistry},
  volume = {81},
  number = {1},
  pages = {19--43},
  issn = {1365-3075, 0033-4545},
  doi = {10.1351/PAC-CON-08-10-18},
  urldate = {2022-07-23},
  abstract = {Abstract             Chemical reactions that involve radical intermediates can be influenced by magnetic fields, which act to alter their rate, yield, or product distribution. These effects have been studied extensively in liquids, solids, and constrained media such as micelles. They may be interpreted using the radical pair mechanism (RPM). Such effects are central to the field of spin chemistry of which there have been several detailed and extensive reviews. This review instead presents an introductory account of the field of spin chemistry, suitable for use by graduate students or researchers who are new to the area. It proceeds by giving a brief historical overview of the development of spin chemistry, before introducing the essential theory. This is then illustrated by application to a series of recent developments in solution-phase magnetic field effects (MFEs). The closing pages of this review describe the role played by spin chemistry in the remarkable magnetic compass sense of birds and other animals.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/rodgers2009magnetic_field_effects_in_chemical_systems.pdf}
}

@inproceedings{rombach2022highresolution,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bjorn},
  year = {2022},
  month = jun,
  pages = {10674--10685},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.01042},
  urldate = {2022-12-05},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.},
  isbn = {978-1-66546-946-3},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/rombach2022high-resolution_image_synthesis_with_latent_diffusion_models.pdf}
}

@misc{ruiz2016generalized,
  title = {The {{Generalized Reparameterization Gradient}}},
  author = {Ruiz, Francisco J. R. and Titsias, Michalis K. and Blei, David M.},
  year = {2016},
  month = oct,
  number = {arXiv:1610.02287},
  eprint = {1610.02287},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1610.02287},
  urldate = {2023-03-22},
  abstract = {The reparameterization gradient has become a widely used method to obtain Monte Carlo gradients to optimize the variational objective. However, this technique does not easily apply to commonly used distributions such as beta or gamma without further approximations, and most practical applications of the reparameterization gradient fit Gaussian distributions. In this paper, we introduce the generalized reparameterization gradient, a method that extends the reparameterization gradient to a wider class of variational distributions. Generalized reparameterizations use invertible transformations of the latent variables which lead to transformed distributions that weakly depend on the variational parameters. This results in new Monte Carlo gradients that combine reparameterization gradients and score function gradients. We demonstrate our approach on variational inference for two complex probabilistic models. The generalized reparameterization is effective: even a single sample from the variational distribution is enough to obtain a low-variance gradient.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  note = {Comment: 16 pages, 15 figures, NIPS version},
  file = {/home/vatai/Sync/zotero-data/pdfs/ruiz2016the_generalized_reparameterization_gradient.pdf;/home/vatai/Sync/zotero-data/storage/2RZ8YUNQ/1610.html}
}

@article{sasongko2021reusetracker,
  title = {{{ReuseTracker}}: {{Fast Yet Accurate Multicore Reuse Distance Analyzer}}},
  shorttitle = {{{ReuseTracker}}},
  author = {Sasongko, Muhammad Aditya and Chabbi, Milind and Marzijarani, Mandana Bagheri and Unat, Didem},
  year = {2021},
  month = dec,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {19},
  number = {1},
  pages = {3:1--3:25},
  issn = {1544-3566},
  doi = {10.1145/3484199},
  urldate = {2022-12-16},
  abstract = {One widely used metric that measures data locality is reuse distance---the number of unique memory locations that are accessed between two consecutive accesses to a particular memory location. State-of-the-art techniques that measure reuse distance in parallel applications rely on simulators or binary instrumentation tools that incur large performance and memory overheads. Moreover, the existing sampling-based tools are limited to measuring reuse distances of a single thread and discard interactions among threads in multi-threaded programs. In this work, we propose ReuseTracker---a fast and accurate reuse distance analyzer that leverages existing hardware features in commodity CPUs. ReuseTracker is designed for multi-threaded programs and takes cache-coherence effects into account. By utilizing hardware features like performance monitoring units and debug registers, ReuseTracker can accurately profile reuse distance in parallel applications with much lower overheads than existing tools. It introduces only 2.9{\texttimes} runtime and 2.8{\texttimes} memory overheads. Our tool achieves 92\% accuracy when verified against a newly developed configurable benchmark that can generate a variety of different reuse distance patterns. We demonstrate the tool's functionality with two use-case scenarios using PARSEC, Rodinia, and Synchrobench benchmark suites where ReuseTracker guides code refactoring in these benchmarks by detecting spatial reuses in shared caches that are also false sharing and successfully predicts whether some benchmarks in these suites can benefit from adjacent cache line prefetch optimization.},
  keywords = {address sampling,debug registers,hardware performance counters,Reuse distance},
  file = {/home/vatai/Sync/zotero-data/pdfs/sasongko2021reusetracker.pdf}
}

@misc{schaad2023fuzzyflow,
  title = {{{FuzzyFlow}}: {{Leveraging Dataflow To Find}} and {{Squash Program Optimization Bugs}}},
  shorttitle = {{{FuzzyFlow}}},
  author = {Schaad, Philipp and Schneider, Timo and {Ben-Nun}, Tal and Calotoiu, Alexandru and Ziogas, Alexandros Nikolaos and Hoefler, Torsten},
  year = {2023},
  month = jun,
  number = {arXiv:2306.16178},
  eprint = {2306.16178},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.16178},
  urldate = {2023-08-20},
  abstract = {The current hardware landscape and application scale is driving performance engineers towards writing bespoke optimizations. Verifying such optimizations, and generating minimal failing cases, is important for robustness in the face of changing program conditions, such as inputs and sizes. However, isolation of minimal test-cases from existing applications and generating new configurations are often difficult due to side effects on the system state, mostly related to dataflow. This paper introduces FuzzyFlow: a fault localization and test case extraction framework designed to test program optimizations. We leverage dataflow program representations to capture a fully reproducible system state and area-of-effect for optimizations to enable fast checking for semantic equivalence. To reduce testing time, we design an algorithm for minimizing test inputs, trading off memory for recomputation. We demonstrate FuzzyFlow on example use cases in real-world applications where the approach provides up to 528 times faster optimization testing and debugging compared to traditional approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {/home/vatai/Sync/zotero-data/pdfs/schaad2023fuzzyflow.pdf;/home/vatai/Sync/zotero-data/storage/JESZWEZQ/2306.html}
}

@article{schulten1978semiclassical,
  title = {Semiclassical Description of Electron Spin Motion in Radicals Including the Effect of Electron Hopping},
  author = {Schulten, Klaus and Wolynes, Peter G.},
  year = {1978},
  month = apr,
  journal = {The Journal of Chemical Physics},
  volume = {68},
  number = {7},
  pages = {3292--3297},
  issn = {0021-9606},
  doi = {10.1063/1.436135},
  urldate = {2024-02-11},
  abstract = {The coherent electron spin motion in radicals induced by the hyperfine coupling to nuclear spins is described semiclassically. The nuclear spins are treated as constant classical vectors around which the electron spin precesses. The ensemble average over all nuclear spin configurations is taken yielding the electron spin correlation tensor \&lt;S(0)S(t) {$\greaterequivlnt$}. Borrowing from the theory of rotational diffusion the effect of electron hopping between molecules on the spin correlation tensor is described. The treatment is applied to the time evolution of the electron spin state of a radical pair initially prepared in a singlet state.},
  file = {/home/vatai/Sync/zotero-data/pdfs/schulten1978semiclassical_description_of_electron_spin_motion_in_radicals_including_the.pdf;/home/vatai/Sync/zotero-data/storage/AJ4C6N4A/Semiclassical-description-of-electron-spin-motion.html}
}

@article{serrano2020interpretability,
  title = {Interpretability for Current {{NLP}}},
  author = {Serrano, Sofia},
  year = {2020},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/serranointerpretability_for_current_nlp.pdf}
}

@article{shaw2020myths,
  title = {Myths and Mythconceptions: What Does It Mean to Be a Programming Language, Anyhow?},
  shorttitle = {Myths and Mythconceptions},
  author = {Shaw, Mary},
  year = {2020},
  month = jun,
  journal = {Proc. ACM Program. Lang.},
  volume = {4},
  number = {HOPL},
  pages = {1--44},
  issn = {2475-1421},
  doi = {10.1145/3480947},
  urldate = {2022-08-02},
  abstract = {Modern software does not stand alone; it is embedded in complex physical and sociotechnical systems. It relies on computational support from interdependent subsystems as well as non-code resources such as data, communications, sensors, and interactions with humans. Both general-purpose programming languages and mainstream programming language research focus on symbolic notations with well-defined abstractions that are intended for use by professionals to write programs that solve precisely specified problems. There is a strong emphasis on correctness of the resulting programs, preferably by formal reasoning. However, these languages, despite their careful design and formal foundations, address only a modest portion of modern software and only a minority of software developers.             Several persistent myths reinforce this focus. These myths express an idealized model of software and software development. They provide a lens for examining modern software and software development practice: highly trained professionals are outnumbered by vernacular developers. Writing new code is dominated by composition of ill-specified software and non-software components. General-purpose languages may be less appropriate for a task than domain-specific languages, and functional correctness is often a less appropriate goal than overall fitness for task. Support for programming to meet a specification is of little help to people who are programming in order to understand their problems. Reasoning about software is challenged by uncertainty and nondeterminism in the execution environment and by the increasingly dominant role of data, especially with the advent of systems that rely on machine learning. The lens of our persistent myths illuminates the dissonance between our idealized view of software development and common practice, which enables us to identify emerging opportunities and challenges for programming language research.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/shaw2020myths_and_mythconceptions.pdf}
}

@article{shen2020genomic,
  title = {Genomic {{Diversity}} of {{Severe Acute Respiratory Syndrome}}--{{Coronavirus}} 2 in {{Patients With Coronavirus Disease}} 2019},
  author = {Shen, Zijie and Xiao, Yan and Kang, Lu and Ma, Wentai and Shi, Leisheng and Zhang, Li and Zhou, Zhuo and Yang, Jing and Zhong, Jiaxin and Yang, Donghong and Guo, Li and Zhang, Guoliang and Li, Hongru and Xu, Yu and Chen, Mingwei and Gao, Zhancheng and Wang, Jianwei and Ren, Lili and Li, Mingkun},
  year = {2020},
  month = jul,
  journal = {Clinical Infectious Diseases},
  volume = {71},
  number = {15},
  pages = {713--720},
  issn = {1058-4838},
  doi = {10.1093/cid/ciaa203},
  urldate = {2023-02-28},
  abstract = {A novel coronavirus (CoV), severe acute respiratory syndrome (SARS)--CoV-2, has infected \&gt;75\>000 individuals and spread to \&gt;20 countries. It is still unclear how fast the virus evolved and how it interacts with other microorganisms in the lung.We have conducted metatranscriptome sequencing for bronchoalveolar lavage fluid samples from 8 patients with SARS--CoV-2, and also analyzed data from 25 patients with community-acquired pneumonia (CAP), and 20 healthy controls for comparison.The median number of intrahost variants was 1--4 in SARS--CoV-2--infected patients, ranged from 0 to 51 in different samples. The distribution of variants on genes was similar to those observed in the population data. However, very few intrahost variants were observed in the population as polymorphisms, implying either a bottleneck or purifying selection involved in the transmission of the virus, or a consequence of the limited diversity represented in the current polymorphism data. Although current evidence did not support the transmission of intrahost variants in a possible person-to-person spread, the risk should not be overlooked. Microbiotas in SARS--CoV-2--infected patients were similar to those in CAP, either dominated by the pathogens or with elevated levels of oral and upper respiratory commensal bacteria.SARS--CoV-2 evolves in vivo after infection, which may affect its virulence, infectivity, and transmissibility. Although how the intrahost variant spreads in the population is still elusive, it is necessary to strengthen the surveillance of the viral evolution in the population and associated clinical changes.},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/SparkLeBlast/Shen et al. - 2020 - Genomic Diversity of Severe Acute Respiratory Syndrome–Coronavirus 2 in Patients With Coronavirus Di.pdf;/home/vatai/Sync/zotero-data/storage/5MTSCQ2E/5780800.html}
}

@article{shewchuk1994introduction,
  title = {An {{Introduction}} to the {{Conjugate Gradient Method Without}} the {{Agonizing Pain}}},
  author = {Shewchuk, J.},
  year = {1994},
  journal = {undefined},
  urldate = {2022-06-10},
  abstract = {The Conjugate Gradient Method is the most prominent iterative method for solving sparse systems of linear equations. Unfortunately, many textbook treatments of the topic are written so that even their own authors would be mystified, if they bothered to read their own writing. For this reason, an understanding of the method has been reserved for the elite brilliant few who have painstakingly decoded the mumblings of their forebears. Nevertheless, the Conjugate Gradient Method is a composite of simple, elegant ideas that almost anyone can understand. Of course, a reader as intelligent as yourself will learn them almost effortlessly. The idea of quadratic forms is introduced and used to derive the methods of Steepest Descent, Conjugate Directions, and Conjugate Gradients. Eigenvectors are explained and used to examine the convergence of the Jacobi Method, Steepest Descent, and Conjugate Gradients. Other topics include preconditioning and the nonlinear Conjugate Gradient Method. I have taken pains to make this article easy to read. Sixty-two illustrations are provided. Dense prose is avoided. Concepts are explained in several different ways. Most equations are coupled with an intuitive interpretation.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/shewchuk1994an_introduction_to_the_conjugate_gradient_method_without_the_agonizing_pain.pdf;/home/vatai/Sync/zotero-data/storage/6JXBXAK4/70000f7791ab8519429ce939bc897738a05939c3.html}
}

@inproceedings{shido2019automatic,
  title = {Automatic {{Source Code Summarization}} with {{Extended Tree-LSTM}}},
  booktitle = {2019 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Shido, Yusuke and Kobayashi, Yasuaki and Yamamoto, Akihiro and Miyamoto, Atsushi and Matsumura, Tadayuki},
  year = {2019},
  month = jul,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2019.8851751},
  urldate = {2024-08-25},
  abstract = {Neural machine translation models are used to automatically generate a document from given source code since this can be regarded as a machine translation task. Source code summarization is one of the components for automatic document generation, which generates a summary in natural language from given source code. This suggests that techniques used in neural machine translation, such as Long Short-Term Memory (LSTM), can be used for source code summarization. However, there is a considerable difference between source code and natural language: Source code is essentially structured, having loops and conditional branching, etc. Therefore, there is some obstacle to apply known machine translation models to source code.Abstract syntax trees (ASTs) capture these structural properties and play an important role in recent machine learning studies on source code. Tree-LSTM is proposed as a generalization of LSTMs for tree-structured data. However, there is a critical issue when applying it to ASTs: It cannot handle a tree that contains nodes having an arbitrary number of children and their order simultaneously, which ASTs generally have such nodes. To address this issue, we propose an extension of Tree-LSTM, which we call Multi-way Tree-LSTM and apply it for source code summarization. As a result of computational experiments, our proposal achieved better results when compared with several state-of-the-art techniques.},
  keywords = {Computational modeling,Logic gates,Natural languages,Recurrent neural networks,Standards,Task analysis},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/Codegen/shido2019automatic_source_code_summarization_with_extended_tree-lstm.pdf}
}

@article{shin2021survey,
  title = {A {{Survey}} of {{Automatic Code Generation}} from {{Natural Language}}},
  author = {Shin, Jiho and Nam, Jaechang},
  year = {2021},
  journal = {Journal of Information Processing Systems},
  volume = {17},
  number = {3},
  pages = {537--555},
  publisher = {Korea Information Processing Society},
  issn = {1976-913X},
  doi = {10.3745/JIPS.04.0216},
  urldate = {2022-09-29},
  abstract = {Many researchers have carried out studies related to programming languages since the beginning of computer science. Besides programming with traditional programming languages (i.e., procedural, object-oriented, functional programming language, etc.), a new paradigm of programming is being carried out. It is programming with natural language. By programming with natural language, we expect that it will free our expressiveness in contrast to programming languages which have strong constraints in syntax. This paper surveys the approaches that generate source code automatically from a natural language description. We also categorize the approaches by their forms of input and output. Finally, we analyze the current trend of approaches and suggest the future direction of this research domain to improve automatic code generation with natural language. From the analysis, we state that researchers should work on customizing language models in the domain of source code and explore better representations of source code such as embedding techniques and pre-trained models which have been proved to work well on natural language processing tasks.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/shin2021a_survey_of_automatic_code_generation_from_natural_language.pdf;/home/vatai/Sync/zotero-data/storage/RNS85YGV/JAKO202120461938898.html}
}

@misc{shoeybi2020megatronlm,
  title = {Megatron-{{LM}}: {{Training Multi-Billion Parameter Language Models Using Model Parallelism}}},
  shorttitle = {Megatron-{{LM}}},
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  year = {2020},
  month = mar,
  number = {arXiv:1909.08053},
  eprint = {1909.08053},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-08-26},
  abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76\% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30\% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5\% compared to SOTA accuracy of 63.2\%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9\% compared to SOTA accuracy of 89.4\%).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/vatai/Sync/zotero-data/pdfs/shoeybi2020megatron-lm.pdf}
}

@article{silver2016mastering,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature16961},
  urldate = {2024-08-30},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  copyright = {2016 Springer Nature Limited},
  langid = {english},
  keywords = {Computational science,Computer science,Reward}
}

@article{sosnovsky2019magnetic,
  title = {Magnetic Field and Orientation Dependence of Solid-State {{CIDNP}}},
  author = {Sosnovsky, Denis V. and Lukzen, Nikita N. and Vieth, Hans-Martin and Jeschke, Gunnar and Gr{\"a}sing, Daniel and Bielytskyi, Pavlo and Matysik, J{\"o}rg and Ivanov, Konstantin L.},
  year = {2019},
  month = mar,
  journal = {J. Chem. Phys.},
  volume = {150},
  number = {9},
  pages = {094105},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.5077078},
  urldate = {2023-02-01},
  file = {/home/vatai/Sync/zotero-data/pdfs/sosnovsky2019magnetic_field_and_orientation_dependence_of_solid-state_cidnp2.pdf}
}

@inproceedings{stein2020exploring,
  title = {Exploring {{Paraphrasing Techniques}} on {{Formal Language}} for {{Generating Semantics Preserving Source Code Transformations}}},
  booktitle = {2020 {{IEEE}} 14th {{International Conference}} on {{Semantic Computing}} ({{ICSC}})},
  author = {Stein, Aviel J. and Kapllani, Levi and Mancoridis, Spiros and Greenstadt, Rachel},
  year = {2020},
  month = feb,
  pages = {242--248},
  issn = {2325-6516},
  doi = {10.1109/ICSC.2020.00051},
  urldate = {2024-08-25},
  abstract = {Automatically identifying and generating equivalent semantic content to a word, phrase, or sentence is an important part of natural language processing (NLP). The research done so far in paraphrases in NLP has been focused exclusively on textual data, but has significant potential if it is applied to formal languages like source code. In this paper, we present a novel technique for generating source code transformations via the use of paraphrases. We explore how to extract and validate source code paraphrases. The transformations can be used for stylometry tasks and processes like refactoring. A machine learning method of identifying valid transformations has the advantage of avoiding the generation of transformations by hand and is more likely to have more valid transformations. Our dataset is comprised by 27,300 C++ source code files, consisting of 273topics each with 10 parallel files. This generates approximately152,000 paraphrases. Of these paraphrases, 11\% yield valid code transformations. We then train a random forest classifier that can identify valid transformations with 83\% accuracy. In this paper we also discuss some of the observed relationships betweenlinked paraphrase transformations. We depict the relationshipsthat emerge between alternative equivalent code transformationsin a graph formalism.},
  keywords = {Code Transformations,Feature extraction,Formal languages,Machine learning,Natural language processing,Natural Language Processing,Semantic Computing,Semantics,Source Code Paraphrasing,Syntactics,Task analysis},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/Codegen/stein2020exploring_paraphrasing_techniques_on_formal_language_for_generating_semantics_preserving_source_code.pdf;/home/vatai/Sync/zotero-data/storage/LR39XWC8/9031503.html}
}

@article{steiner1989magnetic,
  title = {Magnetic Field Effects in Chemical Kinetics and Related Phenomena},
  author = {Steiner, Ulrich E. and Ulrich, Thomas},
  year = {1989},
  month = jan,
  journal = {Chem. Rev.},
  volume = {89},
  number = {1},
  pages = {51--147},
  publisher = {American Chemical Society},
  issn = {0009-2665},
  doi = {10.1021/cr00091a003},
  urldate = {2022-08-15},
  file = {/home/vatai/Sync/zotero-data/pdfs/steiner1989magnetic_field_effects_in_chemical_kinetics_and_related_phenomena.pdf;/home/vatai/Sync/zotero-data/storage/W27IPJHR/cr00091a003.html}
}

@article{steiner1992electron,
  title = {Electron Spin Relaxation of Photobiochemically Generated Radical Pairs Diffusing in Micellar Supercages},
  author = {Steiner, U. E. and Wu, J. Q.},
  year = {1992},
  month = may,
  journal = {Chemical Physics},
  volume = {162},
  number = {1},
  pages = {53--67},
  issn = {0301-0104},
  doi = {10.1016/0301-0104(92)80220-P},
  urldate = {2022-10-18},
  abstract = {A kinetic analysis of previous experimental data [T. Ulrich and U.E. Steiner, Chem. Phys. Letters 112 (1984) 365] on the magnetic-field dependent recombination kinetics of radical pairs photochemically produced with triplet spin correlation in water-in-oil microemulsion droplets of variable size (rM) provided the rM- and field-dependence of the rate constant kr of the T{\textpm} ⇋ S, T0 spin relaxation process. It was found independent of rM at low fields, but strongly decreasing with increasing rM at fields {$\greaterequivlnt$} 100 G. The latter behaviour was attributed to the mechanism of electron-spin dipolar interaction. Quantitative treatments of this relaxation mechanism are provided (i) by adapting Torrey's analytical result for homogeneous solution to the situation of one radical pair in a micellar supercage and (ii) by Monte Carlo calculation of the autocorrelation function of the pertinent perturbation matrix element, considering diffusion in the volume or on the surface of the supercage. The electron-spin dipolar relaxation mechanism can quantitatively account for the experimental data on kr at fields {$\greaterequivlnt$} 100 G.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/steiner_wu1992electron_spin_relaxation_of_photobiochemically_generated_radical_pairs.pdf;/home/vatai/Sync/zotero-data/storage/QZV727ZW/030101049280220P.html}
}

@article{stephens2017arm,
  title = {The {{ARM Scalable Vector Extension}}},
  author = {Stephens, Nigel and Biles, Stuart and Boettcher, Matthias and Eapen, Jacob and Eyole, Mbou and Gabrielli, Giacomo and Horsnell, Matt and Magklis, Grigorios and Martinez, Alejandro and Premillieu, Nathanael and Reid, Alastair and Rico, Alejandro and Walker, Paul},
  year = {2017},
  month = mar,
  journal = {IEEE Micro},
  volume = {37},
  number = {2},
  eprint = {1803.06185},
  primaryclass = {cs},
  pages = {26--39},
  issn = {0272-1732},
  doi = {10.1109/MM.2017.35},
  urldate = {2023-04-14},
  abstract = {This article describes the ARM Scalable Vector Extension (SVE). Several goals guided the design of the architecture. First was the need to extend the vector processing capability associated with the ARM AArch64 execution state to better address the computational requirements in domains such as high-performance computing, data analytics, computer vision, and machine learning. Second was the desire to introduce an extension that can scale across multiple implementations, both now and into the future, allowing CPU designers to choose the vector length most suitable for their power, performance, and area targets. Finally, the architecture should avoid imposing a software development cost as the vector length changes and where possible reduce it by improving the reach of compiler auto-vectorization technologies. SVE achieves these goals. It allows implementations to choose a vector register length between 128 and 2,048 bits. It supports a vector-length agnostic programming model that lets code run and scale automatically across all vector lengths without recompilation. Finally, it introduces several innovative features that begin to overcome some of the traditional barriers to autovectorization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Hardware Architecture,Computer Science - Performance},
  note = {Comment: 8 pages, 8 figures, IEEE Micro paper},
  file = {/home/vatai/Sync/zotero-data/pdfs/stephens2017the_arm_scalable_vector_extension.pdf}
}

@article{steuwer2015generating,
  title = {Generating Performance Portable Code Using Rewrite Rules: From High-Level Functional Expressions to High-Performance {{OpenCL}} Code},
  shorttitle = {Generating Performance Portable Code Using Rewrite Rules},
  author = {Steuwer, Michel and Fensch, Christian and Lindley, Sam and Dubach, Christophe},
  year = {2015},
  month = aug,
  journal = {SIGPLAN Not.},
  volume = {50},
  number = {9},
  pages = {205--217},
  issn = {0362-1340},
  doi = {10.1145/2858949.2784754},
  urldate = {2022-10-18},
  abstract = {Computers have become increasingly complex with the emergence of heterogeneous hardware combining multicore CPUs and GPUs. These parallel systems exhibit tremendous computational power at the cost of increased programming effort resulting in a tension between performance and code portability. Typically, code is either tuned in a low-level imperative language using hardware-specific optimizations to achieve maximum performance or is written in a high-level, possibly functional, language to achieve portability at the expense of performance. We propose a novel approach aiming to combine high-level programming, code portability, and high-performance. Starting from a high-level functional expression we apply a simple set of rewrite rules to transform it into a low-level functional representation, close to the OpenCL programming model, from which OpenCL code is generated. Our rewrite rules define a space of possible implementations which we automatically explore to generate hardware-specific OpenCL implementations. We formalize our system with a core dependently-typed lambda-calculus along with a denotational semantics which we use to prove the correctness of the rewrite rules. We test our design in practice by implementing a compiler which generates high performance imperative OpenCL code. Our experiments show that we can automatically derive hardware-specific implementations from simple functional high-level algorithmic expressions offering performance on a par with highly tuned code for multicore CPUs and GPUs written by experts.},
  keywords = {Algorithmic patterns,code generation,GPU,OpenCL,performance portability,rewrite rules},
  file = {/home/vatai/Sync/zotero-data/pdfs/steuwer2015generating_performance_portable_code_using_rewrite_rules.pdf}
}

@inproceedings{stock2014framework,
  title = {A Framework for Enhancing Data Reuse via Associative Reordering},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Stock, Kevin and Kong, Martin and Grosser, Tobias and Pouchet, Louis-No{\"e}l and Rastello, Fabrice and Ramanujam, J. and Sadayappan, P.},
  year = {2014},
  month = jun,
  series = {{{PLDI}} '14},
  pages = {65--76},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2594291.2594342},
  urldate = {2022-11-10},
  abstract = {The freedom to reorder computations involving associative operators has been widely recognized and exploited in designing parallel algorithms and to a more limited extent in optimizing compilers. In this paper, we develop a novel framework utilizing the associativity and commutativity of operations in regular loop computations to enhance register reuse. Stencils represent a particular class of important computations where the optimization framework can be applied to enhance performance. We show how stencil operations can be implemented to better exploit register reuse and reduce load/stores. We develop a multi-dimensional retiming formalism to characterize the space of valid implementations in conjunction with other program transformations. Experimental results demonstrate the effectiveness of the framework on a collection of high-order stencils.},
  isbn = {978-1-4503-2784-8},
  file = {/home/vatai/Sync/zotero-data/pdfs/stock2014a_framework_for_enhancing_data_reuse_via_associative_reordering.pdf}
}

@misc{stodghill2001lecture,
  title = {Lecture: {{Transforming Imperfectly Nested Loops}}},
  author = {Stodghill, Paul},
  year = {2001},
  howpublished = {https://www.cs.cornell.edu/courses/cs612/2001SP/lectures/lecture09.pdf}
}

@article{stoll2006easyspin,
  title = {{{EasySpin}}, a Comprehensive Software Package for Spectral Simulation and Analysis in {{EPR}}},
  author = {Stoll, Stefan and Schweiger, Arthur},
  year = {2006},
  month = jan,
  journal = {Journal of Magnetic Resonance},
  volume = {178},
  number = {1},
  pages = {42--55},
  issn = {1090-7807},
  doi = {10.1016/j.jmr.2005.08.013},
  urldate = {2022-06-20},
  abstract = {EasySpin, a computational package for spectral simulation and analysis in EPR, is described. It is based on Matlab, a commercial technical computation software. EasySpin provides extensive EPR-related functionality, ranging from elementary spin physics to data analysis. In addition, it provides routines for the simulation of liquid- and solid-state EPR and ENDOR spectra. These simulation functions are built on a series of novel algorithms that enhance scope, speed and accuracy of spectral simulations. Spin systems with an arbitrary number of electron and nuclear spins are supported. The structure of the toolbox as well as the theoretical background underlying its simulation functionality are presented, and some illustrative examples are given.},
  langid = {english},
  keywords = {Breit-Rabi,ENDOR,EPR},
  file = {/home/vatai/Sync/zotero-data/pdfs/stoll2006easyspin,_a_comprehensive_software_package_for_spectral_simulation_and_analysis.pdf;/home/vatai/Sync/zotero-data/storage/AMYU8I9C/S1090780705002892.html}
}

@article{stoltzfus2019tiling,
  title = {Tiling {{Optimizations}} for {{Stencil Computations Using Rewrite Rules}} in {{Lift}}},
  author = {Stoltzfus, Larisa and Hagedorn, Bastian and Steuwer, Michel and Gorlatch, Sergei and Dubach, Christophe},
  year = {2019},
  month = dec,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {16},
  number = {4},
  pages = {52:1--52:25},
  issn = {1544-3566},
  doi = {10.1145/3368858},
  urldate = {2022-10-16},
  abstract = {Stencil computations are a widely used type of algorithm, found in applications from physical simulations to machine learning. Stencils are embarrassingly parallel, therefore fit on modern hardware such as Graphic Processing Units perfectly. Although stencil computations have been extensively studied, optimizing them for increasingly diverse hardware remains challenging. Domain-specific Languages (DSLs) have raised the programming abstraction and offer good performance; however, this method places the burden on DSL implementers to write almost full-fledged parallelizing compilers and optimizers. Lift has recently emerged as a promising approach to achieve performance portability by using a small set of reusable parallel primitives that DSL or library writers utilize. Lift's key novelty is in its encoding of optimizations as a system of extensible rewrite rules which are used to explore the optimization space. This article demonstrates how complex multi-dimensional stencil code and optimizations are expressed using compositions of simple 1D Lift primitives and rewrite rules. We introduce two optimizations that provide high performance for stencils in particular: classical overlapped tiling for multi-dimensional stencils and 2.5D tiling specifically for 3D stencils. We provide an in-depth analysis on how the tiling optimizations affects stencils of different shapes and sizes across different applications. Our experimental results show that our approach outperforms existing compiler approaches and hand-tuned codes.},
  keywords = {Code generation,GPU computing,lift,performance portability,read,stencil},
  file = {/home/vatai/Sync/zotero-data/pdfs/stoltzfus2019tiling_optimizations_for_stencil_computations_using_rewrite_rules_in_lift.pdf}
}

@inproceedings{sun2022stencilmart,
  title = {{{StencilMART}}: {{Predicting Optimization Selection}} for {{Stencil Computations}} across {{GPUs}}},
  shorttitle = {{{StencilMART}}},
  booktitle = {2022 {{IEEE International Parallel}} and {{Distributed Processing Symposium}} ({{IPDPS}})},
  author = {Sun, Qingxiao and Liu, Yi and Yang, Hailong and Jiang, Zhonghui and Luan, Zhongzhi and Qian, Depei},
  year = {2022},
  month = may,
  pages = {875--885},
  issn = {1530-2075},
  doi = {10.1109/IPDPS53621.2022.00090},
  abstract = {Stencil computations are widely used in high performance computing (HPC) applications. Many HPC platforms utilize the high computation capability of GPUs to accelerate stencil computations. In recent years, stencils have become more diverse in terms of stencil order, memory accesses and computation patterns. To adapt diverse stencils to GPUs, a variety of optimization techniques have been proposed such as streaming and retiming. However, due to the diversity of stencil patterns and GPU architectures, no single optimization technique fits all stencils. Besides, it is challenging to choose the most cost-efficient GPU for accelerating target stencils. To address the above problems, we propose StencilMART, an automatic optimization selection framework that predicts the best optimization combination and execution time under a certain parameter setting for stencils on GPUs. Specifically, the StencilMART represents the stencil patterns as binary tensors and neighboring features through tensor assignment and feature extraction. In addition, the StencilMART implements various machine learning methods such as classification and regression that utilize stencil representation and hardware characteristics for execution time prediction. The experiment results show that the StencilMART can achieve accurate optimization selection and performance prediction for various stencils across GPUs.},
  keywords = {Computer architecture,Distributed processing,Feature extraction,GPU,Graphics processing units,High performance computing,Machine learning,Machine Learning,Optimization Strategies,Performance Prediction,read,Stencil Computation,Tensors},
  file = {/home/vatai/Sync/zotero-data/pdfs/sun2022stencilmart.pdf;/home/vatai/Sync/zotero-data/storage/P3Z3V8VU/9820650.html}
}

@article{szymanski1986liouville,
  title = {A Liouville Space Formulation of Wangsness-Bloch-Redfield Theory of Nuclear Spin Relaxation Suitable for Machine Computation. {{I}}. Fundamental Aspects},
  author = {Szymanski, Slawomir and {Gryff-Keller}, Adam M and Binsch, Gerhard},
  year = {1986},
  month = jul,
  journal = {Journal of Magnetic Resonance (1969)},
  volume = {68},
  number = {3},
  pages = {399--432},
  issn = {0022-2364},
  doi = {10.1016/0022-2364(86)90334-3},
  urldate = {2023-03-01},
  abstract = {This is the first of a projected series of papers whose ultimate aim is to construct a general computational framework for the analysis of relaxation effects in multispin systems. In the present paper we address ourselves to the fundamental aspects of the theory of nuclear spin relaxation developed by Wangsness, Bloch, and Redfield. It is shown that by starting from a consistent quantum-mechanical treatment of the combined system of spins and bath, thereby avoiding the ad hoc assumptions of the semiclassical approach, and by making systematic use of the Liouville representation of quantum mechanics, it is possible to arrive at a formulation which is simultaneously compact, coherent, and well adapted to computer implementation. The formalism presented covers the most general case, but is at the same time capable of accommodating the simplifications arising from the special situations commonly encountered in practice. The latter are subdivided into intrinsic symmetry properties inherent in the basic structure of the theory itself or attributable to special features of the spin system, such as nuclear permutation symmetry or the weak coupling approximation, and into extrinsic symmetry properties ensuing from special physical situations or associated with specific types of experiments. Each of these symmetries is subjected to a separate analysis by means of general operator techniques and group theory and it is demonstrated how the resulting invariance properties can be exploited in practice. In the final section we tie all this together and are thus led to the global symmetry groups that apply under particular sets of conditions.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/szymanski1986a_liouville_space_formulation_of_wangsness-bloch-redfield_theory_of_nuclear.pdf;/home/vatai/Sync/zotero-data/storage/7Q9SX7B5/0022236486903343.html}
}

@article{talaashrafi2022pipeline,
  title = {A {{Pipeline Pattern Detection Technique}} in {{Polly}}},
  author = {Talaashrafi, Delaram and Doerfert, Johannes and Maza, Marc Moreno},
  year = {2022},
  pages = {12},
  abstract = {The polyhedral model has repeatedly shown how it facilitates various loop transformations, including loop parallelization, loop tiling, and software pipelining. However, parallelism is almost exclusively exploited on a per-loop basis without much work on detecting cross-loop parallelization opportunities. While many problems can be scheduled such that loop dimensions are dependence-free, the resulting loop parallelism does not necessarily maximize concurrent execution, especially not for unbalanced problems.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/talaashrafi2022a_pipeline_pattern_detection_technique_in_polly.pdf}
}

@article{tehranijamsaz2022learning,
  title = {Learning {{Intermediate Representations}} Using {{Graph Neural Networks}} for {{NUMA}} and {{Prefetchers Optimization}}},
  author = {TehraniJamsaz, Ali and Popov, Mihail and Dutta, Akash and Saillard, Emmanuelle and Jannesari, Ali},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.00611 [cs]},
  eprint = {2203.00611},
  primaryclass = {cs},
  urldate = {2022-03-07},
  abstract = {There is a large space of NUMA and hardware prefetcher configurations that can significantly impact the performance of an application. Previous studies have demonstrated how a model can automatically select configurations based on the dynamic properties of the code to achieve speedups. This paper demonstrates how the static Intermediate Representation (IR) of the code can guide NUMA/prefetcher optimizations without the prohibitive cost of performance profiling. We propose a method to create a comprehensive dataset that includes a diverse set of intermediate representations along with optimum configurations. We then apply a graph neural network model in order to validate this dataset. We show that our static intermediate representation based model achieves 80\% of the performance gains provided by expensive dynamic performance profiling based strategies. We further develop a hybrid model that uses both static and dynamic information. Our hybrid model achieves the same gains as the dynamic models but at a reduced cost by only profiling 30\% of the programs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
  file = {/home/vatai/Sync/zotero-data/pdfs/tehranijamsaz2022learning_intermediate_representations_using_graph_neural_networks_for_numa_and.pdf}
}

@inproceedings{thorsen2007parallel,
  title = {Parallel Genomic Sequence-Search on a Massively Parallel System},
  booktitle = {Proceedings of the 4th International Conference on {{Computing}} Frontiers},
  author = {Thorsen, Oystein and Smith, Brian and Sosa, Carlos P. and Jiang, Karl and Lin, Heshan and Peters, Amanda and Feng, Wu-chun},
  year = {2007},
  month = may,
  pages = {59--68},
  publisher = {ACM},
  address = {Ischia Italy},
  doi = {10.1145/1242531.1242542},
  urldate = {2023-02-28},
  abstract = {In the life sciences, genomic databases for sequence search have been growing exponentially in size. As a result, faster sequencesearch algorithms to search these databases continue to evolve to cope with algorithmic time complexity. The ubiquitous tool for such search is the Basic Local Alignment Search Tool (BLAST) [1] from the National Center for Biotechnology Information (NCBI). Despite continued algorithmic improvements in BLAST, it cannot keep up with the rate at which the database is exponentially increasing in size. Therefore, parallel implementations such as mpiBLAST have emerged to address this problem. The performance of such implementations depends on a myriad of factors including algorithmic, architectural, and mapping of the algorithm to the architecture. This paper describes modifications and extensions to a parallel and distributed-memory version of BLAST called mpiBLAST-PIO and how it maps to a massively parallel system, specifically IBM Blue Gene/L (BG/L). The extensions include a virtual file manager, a "multiple master" runtime model, efficient fragment distribution, and intelligent load balancing. In this study, we have shown that our optimized mpiBLAST-PIO on BG/L using a query with 28014 sequences and the NR and NT databases scales to 8192 nodes (two cores per node). The cases tested here are well suited for a massively parallel system.},
  isbn = {978-1-59593-683-7},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/thorsen2007parallel_genomic_sequence-search_on_a_massively_parallel_system.pdf}
}

@article{trofin2021mlgo,
  title = {{{MLGO}}: A {{Machine Learning Guided Compiler Optimizations Framework}}},
  shorttitle = {{{MLGO}}},
  author = {Trofin, M. and Qian, Yundi and Brevdo, E. and Lin, Zinan and Choromanski, K. and Li, D.},
  year = {2021},
  journal = {ArXiv},
  abstract = {This work is the first full integration of ML in a complex compiler pass in a real-world setting, and uses two different ML algorithms: Policy Gradient and Evolution Strategies, to train the inlining-for-size model, and achieves up to 7\% size reduction when compared to state of the art LLVM -Oz. Leveraging machine-learning (ML) techniques for compiler optimizations has been widely studied and explored in academia. However, the adoption of ML in general-purpose, industry strength compilers has yet to happen. We propose MLGO, a framework for integrating ML techniques systematically in an industrial compiler -- LLVM. As a case study, we present the details and results of replacing the heuristics-based inlining-for-size optimization in LLVM with machine learned models. To the best of our knowledge, this work is the first full integration of ML in a complex compiler pass in a real-world setting. It is available in the main LLVM repository. We use two different ML algorithms: Policy Gradient and Evolution Strategies, to train the inlining-for-size model, and achieve up to 7{\textbackslash}\% size reduction, when compared to state of the art LLVM -Oz. The same model, trained on one corpus, generalizes well to a diversity of real-world targets, as well as to the same set of targets after months of active development. This property of the trained models is beneficial to deploy ML techniques in real-world settings.},
  file = {/home/vatai/Sync/zotero-data/pdfs/trofin2021mlgo.pdf}
}

@article{trott2022kokkos,
  title = {Kokkos 3: {{Programming Model Extensions}} for the {{Exascale Era}}},
  shorttitle = {Kokkos 3},
  author = {Trott, Christian R. and {Lebrun-Grandi{\'e}}, Damien and Arndt, Daniel and Ciesko, Jan and Dang, Vinh and Ellingwood, Nathan and Gayatri, Rahulkumar and Harvey, Evan and Hollman, Daisy S. and Ibanez, Dan and Liber, Nevin and Madsen, Jonathan and Miles, Jeff and Poliakoff, David and Powell, Amy and Rajamanickam, Sivasankaran and Simberg, Mikael and Sunderland, Dan and Turcksin, Bruno and Wilke, Jeremiah},
  year = {2022},
  month = apr,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {33},
  number = {4},
  pages = {805--817},
  issn = {1558-2183},
  doi = {10.1109/TPDS.2021.3097283},
  abstract = {As the push towards exascale hardware has increased the diversity of system architectures, performance portability has become a critical aspect for scientific software. We describe the Kokkos Performance Portable Programming Model that allows developers to write single source applications for diverse high-performance computing architectures. Kokkos provides key abstractions for both the compute and memory hierarchy of modern hardware. We describe the novel abstractions that have been added to Kokkos version 3 such as hierarchical parallelism, containers, task graphs, and arbitrary-sized atomic operations to prepare for exascale era architectures. We demonstrate the performance of these new features with reproducible benchmarks on CPUs and GPUs.},
  keywords = {Benchmark testing,exascale,Graphics processing units,Hardware,heterogeneous computing,high-performance computing,Kernel,Laboratories,Layout,Performance portability,Programming,programming models},
  file = {/home/vatai/Sync/zotero-data/pdfs/trott2022kokkos_3.pdf;/home/vatai/Sync/zotero-data/storage/GVHSID9Q/9485033.html}
}

@inproceedings{trumper2023performance,
  title = {Performance {{Embeddings}}: {{A Similarity-Based Transfer Tuning Approach}} to {{Performance Optimization}}},
  shorttitle = {Performance {{Embeddings}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Supercomputing}}},
  author = {Tr{\"u}mper, Lukas and {Ben-Nun}, Tal and Schaad, Philipp and Calotoiu, Alexandru and Hoefler, Torsten},
  year = {2023},
  month = jun,
  series = {{{ICS}} '23},
  pages = {50--62},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3577193.3593714},
  urldate = {2023-07-26},
  abstract = {Performance optimization is an increasingly challenging but often repetitive task. While each platform has its quirks, the underlying code transformations rely on data movement and computational characteristics that recur across applications. This paper proposes to leverage those similarities by constructing an embedding space for subprograms. The continuous space captures both static and dynamic properties of loop nests via symbolic code analysis and performance profiling, respectively. Performance embeddings enable direct knowledge transfer of performance tuning between applications, which can result from autotuning or tailored improvements. We demonstrate this transfer tuning approach on case studies in deep neural networks, dense and sparse linear algebra compositions, and numerical weather prediction stencils. Transfer tuning reduces the search complexity by up to four orders of magnitude and outperforms the MKL library in sparse-dense matrix multiplication. The results exhibit clear correspondences between program characteristics and optimizations, outperforming prior specialized state-of-the-art approaches and generalizing beyond their capabilities.},
  isbn = {9798400700569},
  keywords = {autotuning,compilers,embeddings,peephole optimization,performance optimization,transfer tuning},
  file = {/home/vatai/Sync/zotero-data/pdfs/trümper2023performance_embeddings3.pdf}
}

@article{tsvetkova2017even,
  title = {Even Good Bots Fight: {{The}} Case of {{Wikipedia}}},
  shorttitle = {Even Good Bots Fight},
  author = {Tsvetkova, Milena and {Garc{\'i}a-Gavilanes}, Ruth and Floridi, Luciano and Yasseri, Taha},
  year = {2017},
  month = feb,
  journal = {PLOS ONE},
  volume = {12},
  number = {2},
  pages = {e0171774},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0171774},
  urldate = {2022-09-06},
  abstract = {In recent years, there has been a huge increase in the number of bots online, varying from Web crawlers for search engines, to chatbots for online customer service, spambots on social media, and content-editing bots in online collaboration communities. The online world has turned into an ecosystem of bots. However, our knowledge of how these automated agents are interacting with each other is rather poor. Bots are predictable automatons that do not have the capacity for emotions, meaning-making, creativity, and sociality and it is hence natural to expect interactions between bots to be relatively predictable and uneventful. In this article, we analyze the interactions between bots that edit articles on Wikipedia. We track the extent to which bots undid each other's edits over the period 2001--2010, model how pairs of bots interact over time, and identify different types of interaction trajectories. We find that, although Wikipedia bots are intended to support the encyclopedia, they often undo each other's edits and these sterile ``fights'' may sometimes continue for years. Unlike humans on Wikipedia, bots' interactions tend to occur over longer periods of time and to be more reciprocated. Yet, just like humans, bots in different cultural environments may behave differently. Our research suggests that even relatively ``dumb'' bots may give rise to complex interactions, and this carries important implications for Artificial Intelligence research. Understanding what affects bot-bot interactions is crucial for managing social media well, providing adequate cyber-security, and designing well functioning autonomous vehicles.},
  langid = {english},
  keywords = {Artificial intelligence,Ecosystems,Encyclopedias,Internet,Language,Network reciprocity,Online encyclopedias,Twitter},
  file = {/home/vatai/Sync/zotero-data/pdfs/tsvetkova2017even_good_bots_fight.pdf;/home/vatai/Sync/zotero-data/storage/LRSPVPAP/article.html}
}

@article{vasilache2012joint,
  title = {Joint {{Scheduling}} and {{Layout Optimization}} to {{Enable Multi-Level Vectorization}}},
  author = {Vasilache, Nicolas and Meister, Benoit and Baskaran, Muthu and Lethin, Richard},
  year = {2012},
  journal = {IMPACT},
  pages = {9},
  abstract = {We describe a novel loop nest scheduling strategy implemented in the R-Stream compiler1 : the first scheduling formulation to jointly optimize a trade-off between parallelism, locality, contiguity of array accesses and data layout permutations in a single complete formulation. Our search space contains the maximal amount of vectorization in the program and automatically finds opportunities for automatic multi-level vectorization and simd-ization. Using our model of memory layout, we demonstrate that the amount of contiguous accesses, vectorization and simd-ization can be increased modulo data layout permutations automatically exposed by our technique. This additional degree of freedom opens new opportunities for the scheduler that were previously out of reach. But perhaps the most significant aspect of this work is to encompass an ever increasing number of traditional optimization phases into a single pass. Our approach offers a good solution to the fundamental problem of phase ordering of high-level loop transformations.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/vasilache2012joint_scheduling_and_layout_optimization_to_enable_multi-level_vectorization.pdf}
}

@article{vasilache2022composable,
  title = {Composable and {{Modular Code Generation}} in {{MLIR}}: {{A Structured}} and {{Retargetable Approach}} to {{Tensor Compiler Construction}}},
  shorttitle = {Composable and {{Modular Code Generation}} in {{MLIR}}},
  author = {Vasilache, Nicolas and Zinenko, Oleksandr and Bik, Aart J. C. and Ravishankar, Mahesh and Raoux, Thomas and Belyaev, Alexander and Springer, Matthias and Gysi, Tobias and Caballero, Diego and Herhut, Stephan and Laurenzo, Stella and Cohen, Albert},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.03293 [cs]},
  eprint = {2202.03293},
  primaryclass = {cs},
  urldate = {2022-02-15},
  abstract = {Despite significant investment in software infrastructure, machine learning systems, runtimes and compilers do not compose properly. We propose a new design aiming at providing unprecedented degrees of modularity, composability and genericity. This paper discusses a structured approach to the construction of domain-specific code generators for tensor compilers, with the stated goal of improving the productivity of both compiler engineers and end-users. The approach leverages the natural structure of tensor algebra. It has been the main driver for the design of progressive lowering paths in {\textbackslash}MLIR. The proposed abstractions and transformations span data structures and control flow with both functional (SSA form) and imperative (side-effecting) semantics. We discuss the implications of this infrastructure on compiler construction and present preliminary experimental results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Programming Languages},
  file = {/home/vatai/Sync/zotero-data/pdfs/vasilache2022composable_and_modular_code_generation_in_mlir.pdf}
}

@misc{vaswani2023attention,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2024-03-07},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: 15 pages, 5 figures},
  file = {/home/vatai/Sync/zotero-data/pdfs/vaswani2023attention_is_all_you_need.pdf;/home/vatai/Sync/zotero-data/storage/7HPI7D3F/1706.html}
}

@article{venkatakeerthy2020ir2vec,
  title = {{{IR2VEC}}: {{LLVM IR Based Scalable Program Embeddings}}},
  shorttitle = {{{IR2VEC}}},
  author = {VenkataKeerthy, S. and Aggarwal, Rohit and Jain, Shalini and Desarkar, Maunendra Sankar and Upadrasta, Ramakrishna and Srikant, Y. N.},
  year = {2020},
  month = dec,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {17},
  number = {4},
  pages = {32:1--32:27},
  issn = {1544-3566},
  doi = {10.1145/3418463},
  urldate = {2022-11-13},
  abstract = {We propose IR2VEC, a Concise and Scalable encoding infrastructure to represent programs as a distributed embedding in continuous space. This distributed embedding is obtained by combining representation learning methods with flow information to capture the syntax as well as the semantics of the input programs. As our infrastructure is based on the Intermediate Representation (IR) of the source code, obtained embeddings are both language and machine independent. The entities of the IR are modeled as relationships, and their representations are learned to form a seed embedding vocabulary. Using this infrastructure, we propose two incremental encodings: Symbolic and Flow-Aware. Symbolic encodings are obtained from the seed embedding vocabulary, and Flow-Aware encodings are obtained by augmenting the Symbolic encodings with the flow information. We show the effectiveness of our methodology on two optimization tasks (Heterogeneous device mapping and Thread coarsening). Our way of representing the programs enables us to use non-sequential models resulting in orders of magnitude of faster training time. Both the encodings generated by IR2VEC outperform the existing methods in both the tasks, even while using simple machine learning models. In particular, our results improve or match the state-of-the-art speedup in 11/14 benchmark-suites in the device mapping task across two platforms and 53/68 benchmarks in the thread coarsening task across four different platforms. When compared to the other methods, our embeddings are more scalable, is non-data-hungry, and has better Out-Of-Vocabulary (OOV) characteristics.},
  keywords = {compiler optimizations,heterogeneous systems,intermediate representations,LLVM,representation learning},
  file = {/home/vatai/Sync/zotero-data/pdfs/venkatakeerthy2020ir2vec.pdf}
}

@inproceedings{verdoolaege2010isl,
  title = {Isl: {{An Integer Set Library}} for the {{Polyhedral Model}}},
  shorttitle = {Isl},
  booktitle = {Mathematical {{Software}} -- {{ICMS}} 2010},
  author = {Verdoolaege, Sven},
  editor = {Fukuda, Komei and van der Hoeven, Joris and Joswig, Michael and Takayama, Nobuki},
  year = {2010},
  pages = {299--302},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-15582-6_49},
  abstract = {In compiler research, polytopes and related mathematical objects have been successfully used for several decades to represent and manipulate computer programs in an approach that has become known as the polyhedral model. The key insight is that the kernels of many compute-intensive applications are composed of loops with bounds that are affine combinations of symbolic constants and outer loop iterators. The iterations of a loop nest can then be represented as the integer points in a (parametric) polytope and manipulated as a whole, rather than as individual iterations. A similar reasoning holds for the elements of an array and for mappings between loop iterations and array elements.},
  isbn = {978-3-642-15582-6},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/current/Codegen/verdoolaege2010isl_an_integer_set_library_for_the_polyhedral_model.pdf}
}

@article{verdoolaege2012polyhedral,
  title = {Polyhedral {{Extraction Tool}}},
  author = {Verdoolaege, Sven and Grosser, Tobias},
  year = {2012},
  pages = {8},
  abstract = {We present a new library for extracting a polyhedral model from C source. The library is based on clang, the LLVM C frontend, and isl, a library for manipulating quasi-affine sets and relations. The use of clang for parsing the C code brings advanced diagnostics and full support for C99. The use of isl allows for an easy construction and a powerful and compact representation of the polyhedral model. Besides allowing arbitrary piecewise quasi-affine index expressions and conditions, the library also supports some data dependent constructs and has special treatment for unsigned integers. The library has been successfully used to obtain polyhedral models for use in an equivalence checker, a tool for constructing polyhedral process networks, a parallelizer targeting GPUs and an interactive polyhedral environment.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/verdoolaege2012polyhedral_extraction_tool.pdf}
}

@article{verdoolaege2013polyhedral,
  title = {Polyhedral Parallel Code Generation for {{CUDA}}},
  author = {Verdoolaege, Sven and Carlos Juega, Juan and Cohen, Albert and Ignacio G{\'o}mez, Jos{\'e} and Tenllado, Christian and Catthoor, Francky},
  year = {2013},
  month = jan,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {9},
  number = {4},
  pages = {54:1--54:23},
  issn = {1544-3566},
  doi = {10.1145/2400682.2400713},
  urldate = {2022-09-29},
  abstract = {This article addresses the compilation of a sequential program for parallel execution on a modern GPU. To this end, we present a novel source-to-source compiler called PPCG. PPCG singles out for its ability to accelerate computations from any static control loop nest, generating multiple CUDA kernels when necessary. We introduce a multilevel tiling strategy and a code generation scheme for the parallelization and locality optimization of imperfectly nested loops, managing memory and exposing concurrency according to the constraints of modern GPUs. We evaluate our algorithms and tool on the entire PolyBench suite.},
  keywords = {C-to-CUDA,code generation,compilers,CUDA,GPU,loop transformations,Par4All,Polyhedral model,PPCG.},
  file = {/home/vatai/Sync/zotero-data/pdfs/verdoolaege2013polyhedral_parallel_code_generation_for_cuda.pdf}
}

@inproceedings{verdoolaege2014schedule,
  title = {Schedule Trees},
  booktitle = {International {{Workshop}} on {{Polyhedral Compilation Techniques}}},
  author = {Verdoolaege, Sven and Guelton, Serge and Grosser, Tobias and Cohen, Albert},
  year = {2014},
  month = jan,
  address = {Vienna, Austria},
  urldate = {2022-02-15},
  abstract = {Schedules in the polyhedral model, both those that represent the original execution order and those produced by scheduling algorithms, naturally have the form of a tree. Generic schedule representations proposed in the literature encode this tree structure such that it is only implicitly available. Following the internal representation of isl, we propose to represent schedules as explicit trees and further extend the concept by introducing different kinds of nodes. We compare our schedule trees to other representations in detail and illustrate how they have been successfully used to simplify the implementation of a non-trivial polyhedral compiler.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/verdoolaege2014schedule_trees.pdf}
}

@misc{verdoolaege2015polyhedral,
  title = {Polyhedral Compilation without Polyhedra},
  author = {Verdoolaege, Sven},
  year = {2015},
  urldate = {2023-06-20},
  abstract = {Polyhedral compilation is widely used in high-level synthesis tools and in production compilers such as gcc, LLVM and IBM/XL and is still being actively developed by several research groups across the globe, resulting in highly attended IMPACT workshops and a recent polyhedral school. It is based on the polyhedral model, a powerful abstraction for analyzing and transforming (parts of) programs that are "sufficiently regular". The key feature of this model is that it is instance based, allowing for a representation and treatment of individual dynamic executions of a statement inside a loop nest and/or individual array elements. The name of the model derives from the mathematical objects called polyhedra that are used internally to describe these instances. These polyhedra are often represented using matrices, giving the impression to outsiders that the polyhedral model is difficult to use. In this tutorial, we follow the tradition of the Omega Project and use a slightly higher-level representation based on integer tuples bounded by quasi-affine constraints, which we call named Presburger sets. In particular, we will address the following topics:     How to model various aspects of a piece of code using Presburger sets and relations.     Which basic operations are available on such sets and relations, without going into details on how these operations are implemented.     How to use these operations to mainly analyze but also transform programs.     Which tools are available for polyhedral compilation.     A small selection of some of the results that may be achieved through polyhedral compilation.  An earlier version of part of this tutorial was presented as the first half of a lecture at the polyhedral school, but a lot of extra material was added, mainly about dependence and dataflow analysis.},
  howpublished = {https://kuleuven.limo.libis.be/discovery/fulldisplay/lirias1656500/32KUL\_KUL:Lirias},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/verdoolaegepolyhedral_compilation_without_polyhedra.pdf;/home/vatai/Sync/zotero-data/storage/B4CFNTNI/fulldisplay.html}
}

@article{verdoolaege2021presburger,
  title = {Presburger {{Formulas}} and {{Polyhedral Compilation}}},
  author = {Verdoolaege, Sven and Labs, Polly and Leuven, {\relax KU}},
  year = {2021},
  pages = {174},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/verdoolaegepresburger_formulas_and_polyhedral_compilation.pdf}
}

@inproceedings{wang2021codet5,
  title = {{{CodeT5}}: {{Identifier-aware Unified Pre-trained Encoder-Decoder Models}} for {{Code Understanding}} and {{Generation}}},
  shorttitle = {{{CodeT5}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven C.H.},
  year = {2021},
  month = nov,
  pages = {8696--8708},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.685},
  urldate = {2022-08-04},
  abstract = {Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.},
  file = {/home/vatai/Sync/zotero-data/pdfs/wang2021codet5.pdf}
}

@misc{wei2016network,
  title = {Network {{Morphism}}},
  author = {Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen},
  year = {2016},
  month = mar,
  number = {arXiv:1603.01670},
  eprint = {1603.01670},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1603.01670},
  urldate = {2023-04-11},
  abstract = {We present in this paper a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as {\textbackslash}emph\{network morphism\} in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement for this network morphism is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: Under review for ICML 2016},
  file = {/home/vatai/Sync/zotero-data/pdfs/wei2016network_morphism.pdf;/home/vatai/Sync/zotero-data/storage/Y3WWCZZN/1603.html}
}

@inproceedings{weinberg2005quantifying,
  title = {Quantifying {{Locality In The Memory Access Patterns}} of {{HPC Applications}}},
  booktitle = {{{ACM}}/{{IEEE SC}} 2005 {{Conference}} ({{SC}}'05)},
  author = {Weinberg, J. and McCracken, M.O. and Strohmaier, E. and Snavely, A.},
  year = {2005},
  pages = {50--50},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/SC.2005.59},
  urldate = {2022-10-17},
  abstract = {Several benchmarks for measuring the memory performance of HPC systems along dimensions of spatial and temporal memory locality have recently been proposed. However, little is understood about the relationships of these benchmarks to real applications and to each other. We propose a methodology for producing architecture-neutral characterizations of the spatial and temporal locality exhibited by the memory access patterns of applications. We demonstrate that the results track intuitive notions of locality on several synthetic and application benchmarks. We employ the methodology to analyze the memory performance components of the HPC Challenge Benchmarks, the Apex-MAP benchmark, and their relationships to each other and other benchmarks and applications. We show that this analysis can be used to both increase understanding of the benchmarks and enhance their usefulness by mapping them, along with applications, to a 2-D space along axes of spatial and temporal locality.},
  isbn = {978-1-59593-061-3},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/weinberg2005quantifying_locality_in_the_memory_access_patterns_of_hpc_applications.pdf}
}

@article{wittenberg2010introduction,
  title = {An {{Introduction}} to {{Maximum Entropy}} and {{Minimum Cross-entropy Estimation Using Stata}}},
  author = {Wittenberg, Martin},
  year = {2010},
  month = sep,
  journal = {The Stata Journal},
  volume = {10},
  number = {3},
  pages = {315--330},
  issn = {1536-867X, 1536-8734},
  doi = {10.1177/1536867X1001000301},
  urldate = {2024-01-18},
  abstract = {Maximum entropy and minimum cross-entropy estimation are applicable when faced with ill-posed estimation problems. I introduce a Stata command that estimates a probability distribution using a maximum entropy or minimum cross-entropy criterion. I show how this command can be used to calibrate survey data to various population totals.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/storage/57VLFBBA/Wittenberg - 2010 - An Introduction to Maximum Entropy and Minimum Cro.pdf}
}

@article{wu2018covalent,
  title = {Covalent {{Radical Pairs}} as {{Spin Qubits}}: {{Influence}} of {{Rapid Electron Motion}} between {{Two Equivalent Sites}} on {{Spin Coherence}}},
  shorttitle = {Covalent {{Radical Pairs}} as {{Spin Qubits}}},
  author = {Wu, Yilei and Zhou, Jiawang and Nelson, Jordan N. and Young, Ryan M. and Krzyaniak, Matthew D. and Wasielewski, Michael R.},
  year = {2018},
  month = oct,
  journal = {J. Am. Chem. Soc.},
  volume = {140},
  number = {40},
  pages = {13011--13021},
  publisher = {American Chemical Society},
  issn = {0002-7863},
  doi = {10.1021/jacs.8b08105},
  urldate = {2022-12-16},
  abstract = {Ultrafast photodriven electron transfer reactions starting from an excited singlet state in an organic donor--acceptor molecule generate a radical pair (RP) in which the two spins are initially entangled and, in principle, can serve as coupled spin qubits in quantum information science (QIS) applications, provided that spin coherence lifetimes in these RPs are long. Here we investigate the effects of electron transfer between two equivalent sites comprising the reduced acceptor of the RP. A covalent electron donor--acceptor molecule (D--C--A24+) including a p-methoxyaniline donor (D), a 4-aminonaphthalene-1,8-imide chromophoric primary acceptor (C), and a m-xylene bridged cyclophane having two equivalent phenyl-extended viologens (A24+) as a secondary acceptor was synthesized along with the analogous molecule having one phenyl-extended viologen acceptor and a second, more difficult to reduce 2,5-dimethoxyphenyl-extended viologen in a very similar cyclophane structure (D--C--A4+). Photoexcitation of C within each molecule results in subnanosecond formation of D+{$\bullet$}--C--A23+{$\bullet$} and D+{$\bullet$}--C--A3+{$\bullet$}. The spin dynamics of these RPs were characterized by time-resolved EPR spectroscopy and magnetic field effects on the RP yield in both CH3CN and CD3CN. The data show that rapid electron hopping within A23+{$\bullet$} promotes spin decoherence in D+{$\bullet$}--C--A23+{$\bullet$} relative to D+{$\bullet$}--C--A3+{$\bullet$} having a monomeric acceptor, while the interaction of the RP electron spins with the nuclear spins of the solvent have little or no effect on the spin dynamics. These observations provide important information for designing and understanding novel molecular assemblies of spin qubits with long coherence times for QIS applications.},
  file = {/home/vatai/Sync/zotero-data/pdfs/wu2018covalent_radical_pairs_as_spin_qubits.pdf}
}

@article{wu2022autotuning,
  title = {Autotuning {{PolyBench}} Benchmarks with {{LLVM Clang}}/{{Polly}} Loop Optimization Pragmas Using {{Bayesian}} Optimization},
  author = {Wu, Xingfu and Kruse, Michael and Balaprakash, Prasanna and Finkel, Hal and Hovland, Paul and Taylor, Valerie and Hall, Mary},
  year = {2022},
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {34},
  number = {20},
  pages = {e6683},
  issn = {1532-0634},
  doi = {10.1002/cpe.6683},
  urldate = {2022-10-21},
  abstract = {We develop a ytopt autotuning framework that leverages Bayesian optimization to explore the parameter space search and compare four different supervised learning methods within Bayesian optimization and evaluate their effectiveness. We select six of the most complex PolyBench benchmarks and apply the newly developed LLVM Clang/Polly loop optimization pragmas to the benchmarks to optimize them. We then use the autotuning framework to optimize the pragma parameters to improve their performance. The experimental results show that our autotuning approach outperforms the other compiling methods to provide the smallest execution time for the benchmarks syr2k, 3mm, heat-3d, lu, and covariance with two large datasets in 200 code evaluations for effectively searching the parameter spaces with up to 170,368 different configurations. We find that the Floyd--Warshall benchmark did not benefit from autotuning. To cope with this issue, we provide some compiler option solutions to improve the performance. Then we present loop autotuning without a user's knowledge using a simple mctree autotuning framework to further improve the performance of the Floyd--Warshall benchmark. We also extend the ytopt autotuning framework to tune a deep learning application.},
  langid = {english},
  keywords = {autotuning,Clang,loop transformation,machine learning,optimization,Polly,PolyBench benchmarks},
  file = {/home/vatai/Sync/zotero-data/pdfs/wu2022autotuning_polybench_benchmarks_with_llvm_clang-polly_loop_optimization_pragmas.pdf;/home/vatai/Sync/zotero-data/storage/2YWAZUG7/cpe.html}
}

@article{wu2022autotuninga,
  title = {Autotuning {{PolyBench Benchmarks}} with {{LLVM Clang}}/{{Polly Loop Optimization Pragmas Using Bayesian Optimization}} (Extended Version)},
  author = {Wu, Xingfu and Kruse, Michael and Balaprakash, Prasanna and Finkel, Hal and Hovland, Paul and Hall, Mary},
  year = {2022},
  pages = {19},
  abstract = {In this paper, we develop a ytopt autotuning framework that leverages Bayesian optimization to explore the parameter space search and compare four different supervised learning methods within Bayesian optimization and evaluate their effectiveness. We select six of the most complex PolyBench benchmarks and apply the newly developed LLVM Clang/Polly loop optimization pragmas to the benchmarks to optimize them. We then use the autotuning framework to optimize the pragma parameters to improve their performance. The experimental results show that our autotuning approach outperforms the other compiling methods to provide the smallest execution time for the benchmarks syr2k, 3mm, heat-3d, lu, and covariance with two large datasets in 200 code evaluations for effectively searching the parameter spaces with up to 170,368 different configurations. We find that the Floyd-Warshall benchmark did not benefit from autotuning because Polly uses heuristics to optimize the benchmark to make it run much slower. To cope with this issue, we provide some compiler option solutions to improve the performance. Then we present loop autotuning without a user's knowledge using a simple mctree autotuning framework to further improve the performance of the Floyd-Warshall benchmark. We also extend the ytopt autotuning framework to tune a deep learning application.},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/wuautotuning_polybench_benchmarks_with_llvm_clang-polly_loop_optimization_pragmas.pdf}
}

@inproceedings{yamazaki2020lowsynchronization,
  title = {Low-Synchronization Orthogonalization Schemes for s-Step and Pipelined {{Krylov}} Solvers in {{Trilinos}}},
  booktitle = {Proceedings of the 2020 {{SIAM Conference}} on {{Parallel Processing}} for {{Scientific Computing}}},
  author = {Yamazaki, Ichitaro and Thomas, Stephen and Hoemmen, Mark and Boman, Erik G. and {\'S}wirydowicz, Katarzyna and Elliott, James J.},
  year = {2020},
  pages = {118--128},
  publisher = {SIAM},
  file = {/home/vatai/Sync/zotero-data/pdfs/yamazaki2020low-synchronization_orthogonalization_schemes_for_s-step_and_pipelined_krylov.pdf}
}

@inproceedings{youssef2020sparkleblast,
  title = {{{SparkLeBLAST}}: {{Scalable Parallelization}} of {{BLAST Sequence Alignment Using Spark}}},
  shorttitle = {{{SparkLeBLAST}}},
  booktitle = {2020 20th {{IEEE}}/{{ACM International Symposium}} on {{Cluster}}, {{Cloud}} and {{Internet Computing}} ({{CCGRID}})},
  author = {Youssef, Karim and Feng, Wu-chun},
  year = {2020},
  month = may,
  pages = {539--548},
  publisher = {IEEE},
  address = {Melbourne, Australia},
  doi = {10.1109/CCGrid49817.2020.00-39},
  urldate = {2023-02-28},
  abstract = {The exponential growth of genomic data presents challenges in analyzing and computing on such biological data at scale. While NCBI's BLAST is a widely used pairwise sequence alignment tool, it does not scale to large datasets that are hundreds of gigabytes (GB) in size. To address this scalability problem, mpiBLAST emerged and became widely used, enabling scaling to 65,536 processes. However, mpiBLAST suffers from being tightly coupled with a specific implementation of BLAST, rendering it difficult to upgrade with the ever-evolving NCBI BLAST code. To address this shortcoming, recent parallel BLAST tools, such as SparkBLAST, consist of wrappers that are decoupled from the BLAST code but suffer from poor scalability with large sequence databases. Thus, there does not exist any parallel BLAST tool that can simultaneously address the issues of performance, scalability, programmability, and upgradability. To address this void, we propose SparkLeBLAST, a parallel BLAST tool that leverages our performance modeling and the Spark framework to deliver the performance and scalability of mpiBLAST and the ease of programming and upgradability of SparkBLAST, respectively. Ultimately, SparkLeBLAST delivers a 10x speedup relative to the state-of-the-art SparkBLAST and nearly a 2x speedup relative to the latest version of mpiBLAST.},
  isbn = {978-1-72816-095-5},
  langid = {english},
  file = {/home/vatai/Sync/zotero-data/pdfs/youssef_feng2020sparkleblast.pdf}
}

@article{zarea2015spin,
  title = {Spin Polarization Transfer by the Radical Pair Mechanism},
  author = {Zarea, Mehdi and Ratner, Mark A. and Wasielewski, Michael R.},
  year = {2015},
  month = aug,
  journal = {J. Chem. Phys.},
  volume = {143},
  number = {5},
  pages = {054101},
  publisher = {American Institute of Physics},
  issn = {0021-9606},
  doi = {10.1063/1.4927589},
  urldate = {2023-03-01},
  file = {/home/vatai/Sync/zotero-data/pdfs/zarea2015spin_polarization_transfer_by_the_radical_pair_mechanism.pdf}
}

@article{zhang2021survey,
  title = {A {{Survey}} on {{Neural Network Interpretability}}},
  author = {Zhang, Yu and Ti{\v n}o, Peter and Leonardis, Ale{\v s} and Tang, Ke},
  year = {2021},
  month = oct,
  journal = {IEEE Trans. Emerg. Top. Comput. Intell.},
  volume = {5},
  number = {5},
  eprint = {2012.14261},
  primaryclass = {cs},
  pages = {726--742},
  issn = {2471-285X},
  doi = {10.1109/TETCI.2021.3100641},
  urldate = {2023-02-24},
  abstract = {Along with the great success of deep neural networks, there is also growing concern about their black-box nature. The interpretability issue affects people's trust on deep learning systems. It is also related to many ethical problems, e.g., algorithmic discrimination. Moreover, interpretability is a desired property for deep networks to become powerful tools in other research fields, e.g., drug discovery and genomics. In this survey, we conduct a comprehensive review of the neural network interpretability research. We first clarify the definition of interpretability as it has been used in many different contexts. Then we elaborate on the importance of interpretability and propose a novel taxonomy organized along three dimensions: type of engagement (passive vs. active interpretation approaches), the type of explanation, and the focus (from local to global interpretability). This taxonomy provides a meaningful 3D view of distribution of papers from the relevant literature as two of the dimensions are not simply categorical but allow ordinal subcategories. Finally, we summarize the existing interpretability evaluation methods and suggest possible research directions inspired by our new taxonomy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: This work has been accepted by IEEE-TETCI},
  file = {/home/vatai/Sync/zotero-data/storage/IYKG7HIP/Zhang et al. - 2021 - A Survey on Neural Network Interpretability.pdf}
}

@article{zhao2019flextended,
  title = {Flextended {{Tiles}}: {{A Flexible Extension}} of {{Overlapped Tiles}} for {{Polyhedral Compilation}}},
  shorttitle = {Flextended {{Tiles}}},
  author = {Zhao, Jie and Cohen, Albert},
  year = {2019},
  month = dec,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {16},
  number = {4},
  pages = {47:1--47:25},
  issn = {1544-3566},
  doi = {10.1145/3369382},
  urldate = {2022-11-11},
  abstract = {Loop tiling to exploit data locality and parallelism plays an essential role in a variety of general-purpose and domain-specific compilers. Affine transformations in polyhedral frameworks implement classical forms of rectangular and parallelogram tiling, but these lead to pipelined start with rather inefficient wavefront parallelism. Multiple extensions to polyhedral compilers evaluated sophisticated shapes such as trapezoid or diamond tiles, enabling concurrent start along the axes of the iteration space; yet these resort to custom schedulers and code generators insufficiently integrated within the general framework. One of these modified shapes referred to as overlapped tiling also lacks a unifying framework to reason about its composition with affine transformations; this prevents its application in general-purpose loop-nest optimizers and the fair comparison with other techniques. We revisit overlapped tiling, recasting it as an affine transformation on schedule trees composable with any affine scheduling algorithm. We demonstrate how to derive tighter tile shapes with less redundant computations. Our method models the traditional ``scalene trapezoid'' shapes and novel ``right-rectangle'' variants. It goes beyond the state of the art by avoiding the restriction to a domain-specific language or introducing post-pass rescheduling and custom code generation. We conduct experiments on the PolyMage benchmarks and iterated stencils, validating the effectiveness and applicability of our technique on both general-purpose multicores and GPU accelerators.},
  keywords = {automatic parallelization,loop tiling,Polyhedral compilation,stencil computations},
  file = {/home/vatai/Sync/zotero-data/pdfs/zhao_cohen2019flextended_tiles.pdf}
}

@article{ZhenSi2019kebianbekutoruchangbainariwoyongitaarm,
  title = {{可変ベクトル長バイナリを用いたArm SVEプロセッサA64FXの評価}},
  author = {真司, 住元 and 由江, 稲田 and 英樹, 三輪 and 郁夫, 三吉},
  year = {2019},
  month = dec,
  journal = {研究報告ハイパフォーマンスコンピューティング（HPC）},
  volume = {2019-HPC-172},
  number = {6},
  pages = {1--6},
  issn = {2188-8841},
  urldate = {2023-03-31},
  abstract = {情報学広場 情報処理学会電子図書館},
  langid = {japanese},
  file = {/home/vatai/Sync/zotero-data/pdfs/真司2019可変ベクトル長バイナリを用いたarm_sveプロセッサa64fxの評価.pdf}
}

@article{zhou2022automated,
  title = {An {{Automated Tool}} for {{Analysis}} and {{Tuning}} of {{GPU-Accelerated Code}} in {{HPC Applications}}},
  author = {Zhou, Keren and Meng, Xiaozhu and Sai, Ryuichi and Grubisic, Dejan and {Mellor-Crummey}, John},
  year = {2022},
  month = apr,
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {33},
  number = {4},
  pages = {854--865},
  issn = {1558-2183},
  doi = {10.1109/TPDS.2021.3094169},
  abstract = {The US Department of Energy's fastest supercomputers and forthcoming exascale systems employ Graphics Processing Units (GPUs) to increase the computational performance of compute nodes. However, the complexity of GPU architectures makes tailoring sophisticated applications to achieve high performance on GPU-accelerated systems a major challenge. At best, prior performance tools for GPU code only provide coarse-grained tuning advice at the kernel level. In this article, we describe GPA, a performance advisor that suggests potential code optimizations at a hierarchy of levels, including individual lines, loops, and functions. To gather the fine-grained measurements needed to produce such insights, GPA uses instruction sampling and binary instrumentation to monitor execution of GPU code. At the time of this writing, GPU instruction sampling is only available on NVIDIA GPUs. To understand performance losses, GPA uses data flow analysis to approximately attribute measured instruction stalls back to their causes. GPA then analyzes patterns of stalls using information about a program's structure and the GPU architecture to identify optimization strategies that address inefficiencies observed. GPA then employs detailed performance models to estimate the potential speedup that each optimization might provide. Experiments with benchmarks and applications show that GPA provides useful advice for tuning GPU code. We applied GPA to analyze and tune a collection of codes on NVIDIA V100 and A100 GPUs. GPA suggested optimizations that it estimates will accelerate performance across the set of codes by a geometric mean of 1.21{\texttimes}. Applying these optimizations suggested by GPA accelerated these codes by a geometric mean of 1.19{\texttimes}.},
  keywords = {Graphics processing units,High performance computing,Instruments,Measurement,Optimization,parallel architectures,parallel programming,performance analysis,Registers,Tools,Tuning},
  file = {/home/vatai/Sync/zotero-data/pdfs/zhou2022an_automated_tool_for_analysis_and_tuning_of_gpu-accelerated_code_in_hpc.pdf;/home/vatai/Sync/zotero-data/storage/X4AQRLC9/9470950.html}
}

@article{zivanovic2017main,
  title = {Main {{Memory}} in {{HPC}}: {{Do We Need More}} or {{Could We Live}} with {{Less}}?},
  shorttitle = {Main {{Memory}} in {{HPC}}},
  author = {Zivanovic, Darko and Pavlovic, Milan and Radulovic, Milan and Shin, Hyunsung and Son, Jongpil and Mckee, Sally A. and Carpenter, Paul M. and Radojkovi{\'c}, Petar and Ayguad{\'e}, Eduard},
  year = {2017},
  month = mar,
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {14},
  number = {1},
  pages = {3:1--3:26},
  issn = {1544-3566},
  doi = {10.1145/3023362},
  urldate = {2022-11-10},
  abstract = {An important aspect of High-Performance Computing (HPC) system design is the choice of main memory capacity. This choice becomes increasingly important now that 3D-stacked memories are entering the market. Compared with conventional Dual In-line Memory Modules (DIMMs), 3D memory chiplets provide better performance and energy efficiency but lower memory capacities. Therefore, the adoption of 3D-stacked memories in the HPC domain depends on whether we can find use cases that require much less memory than is available now. This study analyzes the memory capacity requirements of important HPC benchmarks and applications. We find that the High-Performance Conjugate Gradients (HPCG) benchmark could be an important success story for 3D-stacked memories in HPC, but High-Performance Linpack (HPL) is likely to be constrained by 3D memory capacity. The study also emphasizes that the analysis of memory footprints of production HPC applications is complex and that it requires an understanding of application scalability and target category, i.e., whether the users target capability or capacity computing. The results show that most of the HPC applications under study have per-core memory footprints in the range of hundreds of megabytes, but we also detect applications and use cases that require gigabytes per core. Overall, the study identifies the HPC applications and use cases with memory footprints that could be provided by 3D-stacked memory chiplets, making a first step toward adoption of this novel technology in the HPC domain.},
  keywords = {high-performance computing,HPCG,HPL,Memory capacity requirements,production HPC applications},
  file = {/home/vatai/Sync/zotero-data/pdfs/zivanovic2017main_memory_in_hpc.pdf}
}
